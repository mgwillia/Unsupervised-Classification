vulcan02.umiacs.umd.edu
[31m{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-448', 'val_db_name': 'pascal-pretrained-448', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 32, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 448, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 448, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-val-neighbors.npy'}[0m
[34mRetrieve model[0m
{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-448', 'val_db_name': 'pascal-pretrained-448', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 32, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 448, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 448, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-val-neighbors.npy'}
loading pretrained
Model is ContrastiveModel
Model parameters: 30.02M
ContrastiveModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (contrastive_head): Sequential(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU()
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[34mSet CuDNN benchmark[0m
[34mRetrieve dataset[0m
Train transforms: Compose(
    RandomResizedCrop(size=(448, 448), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)
    RandomHorizontalFlip(p=0.5)
    RandomApply(
    p=0.8
    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[-0.1, 0.1])
)
    RandomGrayscale(p=0.2)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Validation transforms: Compose(
    CenterCrop(size=(448, 448))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Dataset contains 15774/15774 train/val samples
[34mBuild MemoryBank[0m
[34mRetrieve criterion[0m
Criterion is SimCLRLoss
[34mRetrieve optimizer[0m
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.001
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
[34mRestart from checkpoint /cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/checkpoint.pth.tar[0m
[34mStarting main loop[0m
[33mEpoch 42/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00006
Train ...
Epoch: [42][  0/492]	Loss 7.5706e-02 (7.5706e-02)
Epoch: [42][ 25/492]	Loss 9.5456e-02 (7.3942e-02)
Epoch: [42][ 50/492]	Loss 8.9012e-02 (8.1214e-02)
Epoch: [42][ 75/492]	Loss 8.1727e-02 (8.4720e-02)
Epoch: [42][100/492]	Loss 1.1665e-01 (8.0417e-02)
Epoch: [42][125/492]	Loss 8.7127e-02 (7.9896e-02)
Epoch: [42][150/492]	Loss 1.1058e-01 (8.0576e-02)
Epoch: [42][175/492]	Loss 1.1495e-01 (8.2055e-02)
Epoch: [42][200/492]	Loss 1.5114e-01 (8.2280e-02)
Epoch: [42][225/492]	Loss 8.9199e-02 (8.2451e-02)
Epoch: [42][250/492]	Loss 3.8147e-02 (8.2091e-02)
Epoch: [42][275/492]	Loss 1.5830e-01 (8.3518e-02)
Epoch: [42][300/492]	Loss 5.5747e-02 (8.3036e-02)
Epoch: [42][325/492]	Loss 1.4729e-01 (8.2717e-02)
Epoch: [42][350/492]	Loss 1.0502e-01 (8.3285e-02)
Epoch: [42][375/492]	Loss 6.7615e-02 (8.3382e-02)
Epoch: [42][400/492]	Loss 1.1686e-01 (8.3658e-02)
Epoch: [42][425/492]	Loss 1.1690e-01 (8.2862e-02)
Epoch: [42][450/492]	Loss 5.1941e-02 (8.2254e-02)
Epoch: [42][475/492]	Loss 1.0239e-01 (8.2466e-02)
Checkpoint ...
[33mEpoch 43/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00005
Train ...
Epoch: [43][  0/492]	Loss 1.1691e-01 (1.1691e-01)
Epoch: [43][ 25/492]	Loss 8.3125e-02 (8.5177e-02)
Epoch: [43][ 50/492]	Loss 1.5098e-01 (8.5505e-02)
Epoch: [43][ 75/492]	Loss 1.4631e-01 (8.6922e-02)
Epoch: [43][100/492]	Loss 1.0043e-01 (8.7365e-02)
Epoch: [43][125/492]	Loss 1.0864e-01 (8.7965e-02)
Epoch: [43][150/492]	Loss 3.4678e-02 (8.8958e-02)
Epoch: [43][175/492]	Loss 8.5936e-02 (8.8533e-02)
Epoch: [43][200/492]	Loss 5.4484e-02 (8.7926e-02)
Epoch: [43][225/492]	Loss 7.8741e-02 (8.8348e-02)
Epoch: [43][250/492]	Loss 8.0220e-02 (8.7107e-02)
Epoch: [43][275/492]	Loss 6.5879e-02 (8.6165e-02)
Epoch: [43][300/492]	Loss 7.6341e-02 (8.6274e-02)
Epoch: [43][325/492]	Loss 4.1545e-02 (8.5905e-02)
Epoch: [43][350/492]	Loss 7.0812e-02 (8.6698e-02)
Epoch: [43][375/492]	Loss 7.1801e-02 (8.6114e-02)
Epoch: [43][400/492]	Loss 7.7985e-02 (8.5447e-02)
Epoch: [43][425/492]	Loss 9.1111e-02 (8.5797e-02)
Epoch: [43][450/492]	Loss 8.6048e-02 (8.5265e-02)
Epoch: [43][475/492]	Loss 9.1094e-02 (8.5550e-02)
Checkpoint ...
[33mEpoch 44/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00004
Train ...
Epoch: [44][  0/492]	Loss 6.0309e-02 (6.0309e-02)
Epoch: [44][ 25/492]	Loss 7.9454e-02 (8.2365e-02)
Epoch: [44][ 50/492]	Loss 1.0785e-01 (8.1498e-02)
Epoch: [44][ 75/492]	Loss 1.1053e-01 (7.9662e-02)
Epoch: [44][100/492]	Loss 6.2942e-02 (7.9973e-02)
Epoch: [44][125/492]	Loss 1.4351e-01 (8.2335e-02)
Epoch: [44][150/492]	Loss 1.3289e-01 (8.3780e-02)
Epoch: [44][175/492]	Loss 5.7728e-02 (8.3997e-02)
Epoch: [44][200/492]	Loss 4.6194e-02 (8.4588e-02)
Epoch: [44][225/492]	Loss 6.2937e-02 (8.5114e-02)
Epoch: [44][250/492]	Loss 2.8157e-02 (8.4825e-02)
Epoch: [44][275/492]	Loss 1.4572e-01 (8.4487e-02)
Epoch: [44][300/492]	Loss 8.0593e-02 (8.3779e-02)
Epoch: [44][325/492]	Loss 1.2281e-01 (8.3152e-02)
Epoch: [44][350/492]	Loss 9.4266e-02 (8.2851e-02)
Epoch: [44][375/492]	Loss 5.3276e-02 (8.2710e-02)
Epoch: [44][400/492]	Loss 7.2324e-02 (8.3556e-02)
Epoch: [44][425/492]	Loss 1.1235e-01 (8.3717e-02)
Epoch: [44][450/492]	Loss 5.9213e-02 (8.3734e-02)
Epoch: [44][475/492]	Loss 5.7811e-02 (8.3765e-02)
Checkpoint ...
[33mEpoch 45/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00003
Train ...
Epoch: [45][  0/492]	Loss 7.1870e-02 (7.1870e-02)
Epoch: [45][ 25/492]	Loss 1.0870e-01 (7.6300e-02)
Epoch: [45][ 50/492]	Loss 3.8724e-02 (8.2249e-02)
Epoch: [45][ 75/492]	Loss 6.1171e-02 (7.9460e-02)
Epoch: [45][100/492]	Loss 5.6476e-02 (8.0422e-02)
Epoch: [45][125/492]	Loss 1.0704e-01 (7.9988e-02)
Epoch: [45][150/492]	Loss 7.5623e-02 (8.4821e-02)
Epoch: [45][175/492]	Loss 6.4766e-02 (8.4514e-02)
Epoch: [45][200/492]	Loss 5.3989e-02 (8.4726e-02)
Epoch: [45][225/492]	Loss 8.9010e-02 (8.4740e-02)
Epoch: [45][250/492]	Loss 1.2962e-01 (8.4767e-02)
Epoch: [45][275/492]	Loss 9.1387e-02 (8.4532e-02)
Epoch: [45][300/492]	Loss 7.8131e-02 (8.4239e-02)
Epoch: [45][325/492]	Loss 6.2282e-02 (8.3663e-02)
Epoch: [45][350/492]	Loss 4.4126e-02 (8.4024e-02)
Epoch: [45][375/492]	Loss 1.2405e-01 (8.4864e-02)
Epoch: [45][400/492]	Loss 1.0277e-01 (8.4965e-02)
Epoch: [45][425/492]	Loss 8.3666e-02 (8.5346e-02)
Epoch: [45][450/492]	Loss 7.6283e-02 (8.5849e-02)
Epoch: [45][475/492]	Loss 9.8235e-02 (8.5550e-02)
Checkpoint ...
[33mEpoch 46/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00002
Train ...
Epoch: [46][  0/492]	Loss 5.3534e-02 (5.3534e-02)
Epoch: [46][ 25/492]	Loss 8.2780e-02 (7.9392e-02)
Epoch: [46][ 50/492]	Loss 1.1924e-01 (8.4491e-02)
Epoch: [46][ 75/492]	Loss 1.2613e-01 (8.9652e-02)
Epoch: [46][100/492]	Loss 1.1933e-01 (9.0369e-02)
Epoch: [46][125/492]	Loss 8.1699e-02 (8.9193e-02)
Epoch: [46][150/492]	Loss 1.4249e-01 (8.9960e-02)
Epoch: [46][175/492]	Loss 8.2482e-02 (9.0017e-02)
Epoch: [46][200/492]	Loss 7.0748e-02 (8.8019e-02)
Epoch: [46][225/492]	Loss 5.0432e-02 (8.7464e-02)
Epoch: [46][250/492]	Loss 1.2206e-01 (8.8061e-02)
Epoch: [46][275/492]	Loss 1.3622e-01 (8.7051e-02)
Epoch: [46][300/492]	Loss 5.1355e-02 (8.7353e-02)
Epoch: [46][325/492]	Loss 1.4317e-01 (8.6279e-02)
Epoch: [46][350/492]	Loss 1.4816e-01 (8.6449e-02)
Epoch: [46][375/492]	Loss 1.0770e-01 (8.6163e-02)
Epoch: [46][400/492]	Loss 9.5071e-02 (8.6340e-02)
Epoch: [46][425/492]	Loss 6.1368e-02 (8.6334e-02)
Epoch: [46][450/492]	Loss 7.2135e-02 (8.6181e-02)
Epoch: [46][475/492]	Loss 3.2702e-02 (8.5530e-02)
Checkpoint ...
[33mEpoch 47/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00001
Train ...
Epoch: [47][  0/492]	Loss 1.4685e-01 (1.4685e-01)
Epoch: [47][ 25/492]	Loss 1.0890e-01 (9.0949e-02)
Epoch: [47][ 50/492]	Loss 5.3109e-02 (8.3395e-02)
Epoch: [47][ 75/492]	Loss 8.6031e-02 (8.5690e-02)
Epoch: [47][100/492]	Loss 4.6707e-02 (8.7260e-02)
Epoch: [47][125/492]	Loss 8.8159e-02 (8.5133e-02)
Epoch: [47][150/492]	Loss 1.1690e-01 (8.5207e-02)
Epoch: [47][175/492]	Loss 6.5273e-02 (8.3278e-02)
Epoch: [47][200/492]	Loss 6.0106e-02 (8.3530e-02)
Epoch: [47][225/492]	Loss 1.1204e-01 (8.3293e-02)
Epoch: [47][250/492]	Loss 5.9327e-02 (8.3579e-02)
Epoch: [47][275/492]	Loss 9.5738e-02 (8.3826e-02)
Epoch: [47][300/492]	Loss 8.7884e-02 (8.3168e-02)
Epoch: [47][325/492]	Loss 7.1493e-02 (8.3028e-02)
Epoch: [47][350/492]	Loss 1.1588e-01 (8.3769e-02)
Epoch: [47][375/492]	Loss 6.8786e-02 (8.4139e-02)
Epoch: [47][400/492]	Loss 5.7855e-02 (8.3539e-02)
Epoch: [47][425/492]	Loss 1.0406e-01 (8.3619e-02)
Epoch: [47][450/492]	Loss 9.1876e-02 (8.4209e-02)
Epoch: [47][475/492]	Loss 3.4549e-02 (8.4423e-02)
Checkpoint ...
[33mEpoch 48/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00000
Train ...
Epoch: [48][  0/492]	Loss 1.2454e-01 (1.2454e-01)
Epoch: [48][ 25/492]	Loss 1.1163e-01 (9.4742e-02)
Epoch: [48][ 50/492]	Loss 5.9731e-02 (8.5820e-02)
Epoch: [48][ 75/492]	Loss 6.4657e-02 (8.3553e-02)
Epoch: [48][100/492]	Loss 6.3843e-02 (8.3709e-02)
Epoch: [48][125/492]	Loss 6.3403e-02 (8.5148e-02)
Epoch: [48][150/492]	Loss 1.7741e-01 (8.6063e-02)
Epoch: [48][175/492]	Loss 6.2707e-02 (8.4076e-02)
Epoch: [48][200/492]	Loss 1.6314e-01 (8.5573e-02)
Epoch: [48][225/492]	Loss 3.5892e-02 (8.6377e-02)
Epoch: [48][250/492]	Loss 8.5209e-02 (8.6273e-02)
Epoch: [48][275/492]	Loss 1.0477e-01 (8.6436e-02)
Epoch: [48][300/492]	Loss 5.6973e-02 (8.5235e-02)
Epoch: [48][325/492]	Loss 9.6691e-02 (8.4363e-02)
Epoch: [48][350/492]	Loss 7.1264e-02 (8.4638e-02)
Epoch: [48][375/492]	Loss 4.6148e-02 (8.5234e-02)
Epoch: [48][400/492]	Loss 4.1887e-02 (8.5732e-02)
Epoch: [48][425/492]	Loss 7.3410e-02 (8.5794e-02)
Epoch: [48][450/492]	Loss 6.2620e-02 (8.5141e-02)
Epoch: [48][475/492]	Loss 1.1054e-01 (8.5068e-02)
Checkpoint ...
[33mEpoch 49/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00000
Train ...
Epoch: [49][  0/492]	Loss 8.3817e-02 (8.3817e-02)
Epoch: [49][ 25/492]	Loss 4.9812e-02 (8.9592e-02)
Epoch: [49][ 50/492]	Loss 7.0578e-02 (8.5348e-02)
Epoch: [49][ 75/492]	Loss 1.3135e-01 (9.2800e-02)
Epoch: [49][100/492]	Loss 8.2621e-02 (9.2365e-02)
Epoch: [49][125/492]	Loss 1.6381e-01 (9.1307e-02)
Epoch: [49][150/492]	Loss 4.1509e-02 (9.0252e-02)
Epoch: [49][175/492]	Loss 6.2047e-02 (8.9838e-02)
Epoch: [49][200/492]	Loss 5.1091e-02 (8.8099e-02)
Epoch: [49][225/492]	Loss 1.1828e-01 (8.6637e-02)
Epoch: [49][250/492]	Loss 1.3040e-01 (8.7300e-02)
Epoch: [49][275/492]	Loss 7.9225e-02 (8.6838e-02)
Epoch: [49][300/492]	Loss 1.0865e-01 (8.5514e-02)
Epoch: [49][325/492]	Loss 9.3227e-02 (8.4930e-02)
Epoch: [49][350/492]	Loss 7.9193e-02 (8.4573e-02)
Epoch: [49][375/492]	Loss 1.1302e-01 (8.4767e-02)
Epoch: [49][400/492]	Loss 8.7295e-02 (8.4262e-02)
Epoch: [49][425/492]	Loss 6.5611e-02 (8.5362e-02)
Epoch: [49][450/492]	Loss 1.2789e-01 (8.5773e-02)
Epoch: [49][475/492]	Loss 1.4887e-01 (8.6136e-02)
Fill memory bank for kNN...
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Evaluate ...
Result of kNN evaluation is 98.08
Checkpoint ...
[34mFill memory bank for mining the nearest neighbors (train) ...[0m
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Mine the nearest neighbors (Top-20)
Accuracy of top-20 nearest neighbors on train set is 61.39
[34mFill memory bank for mining the nearest neighbors (val) ...[0m
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Mine the nearest neighbors (Top-5)
Accuracy of top-5 nearest neighbors on val set is 67.17
