vulcan02.umiacs.umd.edu
[31m{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-448', 'val_db_name': 'pascal-pretrained-448', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 32, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 448, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 448, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-val-neighbors.npy'}[0m
[34mRetrieve model[0m
{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-448', 'val_db_name': 'pascal-pretrained-448', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 32, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 448, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 448, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-val-neighbors.npy'}
loading pretrained
Model is ContrastiveModel
Model parameters: 30.02M
ContrastiveModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (contrastive_head): Sequential(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU()
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[34mSet CuDNN benchmark[0m
[34mRetrieve dataset[0m
Train transforms: Compose(
    RandomResizedCrop(size=(448, 448), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)
    RandomHorizontalFlip(p=0.5)
    RandomApply(
    p=0.8
    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[-0.1, 0.1])
)
    RandomGrayscale(p=0.2)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Validation transforms: Compose(
    CenterCrop(size=(448, 448))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Dataset contains 15774/15774 train/val samples
[34mBuild MemoryBank[0m
[34mRetrieve criterion[0m
Criterion is SimCLRLoss
[34mRetrieve optimizer[0m
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.001
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
[34mNo checkpoint file at /cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/checkpoint.pth.tar[0m
[34mStarting main loop[0m
[33mEpoch 0/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [0][  0/492]	Loss 2.3921e+00 (2.3921e+00)
Epoch: [0][ 25/492]	Loss 7.3939e-01 (1.1041e+00)
Epoch: [0][ 50/492]	Loss 1.0324e+00 (8.9620e-01)
Epoch: [0][ 75/492]	Loss 4.7719e-01 (7.7744e-01)
Epoch: [0][100/492]	Loss 6.6304e-01 (6.9846e-01)
Epoch: [0][125/492]	Loss 4.1895e-01 (6.4633e-01)
Epoch: [0][150/492]	Loss 3.2572e-01 (6.0359e-01)
Epoch: [0][175/492]	Loss 5.0419e-01 (5.6868e-01)
Epoch: [0][200/492]	Loss 3.0507e-01 (5.4406e-01)
Epoch: [0][225/492]	Loss 4.7328e-01 (5.2243e-01)
Epoch: [0][250/492]	Loss 3.7548e-01 (5.0861e-01)
Epoch: [0][275/492]	Loss 2.6342e-01 (4.9636e-01)
Epoch: [0][300/492]	Loss 2.9063e-01 (4.8441e-01)
Epoch: [0][325/492]	Loss 5.6540e-01 (4.7566e-01)
Epoch: [0][350/492]	Loss 6.1925e-01 (4.6589e-01)
Epoch: [0][375/492]	Loss 2.7681e-01 (4.5738e-01)
Epoch: [0][400/492]	Loss 2.4010e-01 (4.4827e-01)
Epoch: [0][425/492]	Loss 2.7339e-01 (4.4028e-01)
Epoch: [0][450/492]	Loss 2.3608e-01 (4.3230e-01)
Epoch: [0][475/492]	Loss 4.0561e-01 (4.2528e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Evaluate ...
Result of kNN evaluation is 91.05
Checkpoint ...
[33mEpoch 1/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [1][  0/492]	Loss 3.1055e-01 (3.1055e-01)
Epoch: [1][ 25/492]	Loss 2.3905e-01 (2.8587e-01)
Epoch: [1][ 50/492]	Loss 1.8344e-01 (2.8823e-01)
Epoch: [1][ 75/492]	Loss 3.0257e-01 (2.7928e-01)
Epoch: [1][100/492]	Loss 2.7362e-01 (2.7885e-01)
Epoch: [1][125/492]	Loss 2.3202e-01 (2.7325e-01)
Epoch: [1][150/492]	Loss 2.9219e-01 (2.7936e-01)
Epoch: [1][175/492]	Loss 2.6599e-01 (2.7457e-01)
Epoch: [1][200/492]	Loss 2.5401e-01 (2.7864e-01)
Epoch: [1][225/492]	Loss 2.5127e-01 (2.7517e-01)
Epoch: [1][250/492]	Loss 1.5933e-01 (2.7757e-01)
Epoch: [1][275/492]	Loss 3.4314e-01 (2.7207e-01)
Epoch: [1][300/492]	Loss 4.2877e-01 (2.7208e-01)
Epoch: [1][325/492]	Loss 2.4215e-01 (2.7146e-01)
Epoch: [1][350/492]	Loss 1.7445e-01 (2.6933e-01)
Epoch: [1][375/492]	Loss 2.1161e-01 (2.6659e-01)
Epoch: [1][400/492]	Loss 1.4000e-01 (2.6568e-01)
Epoch: [1][425/492]	Loss 2.2036e-01 (2.6557e-01)
Epoch: [1][450/492]	Loss 2.1702e-01 (2.6449e-01)
Epoch: [1][475/492]	Loss 2.2481e-01 (2.6429e-01)
Checkpoint ...
[33mEpoch 2/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [2][  0/492]	Loss 1.5098e-01 (1.5098e-01)
Epoch: [2][ 25/492]	Loss 1.7967e-01 (1.9324e-01)
Epoch: [2][ 50/492]	Loss 1.9954e-01 (2.1254e-01)
Epoch: [2][ 75/492]	Loss 1.7562e-01 (2.2014e-01)
Epoch: [2][100/492]	Loss 1.0100e-01 (2.1729e-01)
Epoch: [2][125/492]	Loss 2.9749e-01 (2.1807e-01)
Epoch: [2][150/492]	Loss 1.5516e-01 (2.1586e-01)
Epoch: [2][175/492]	Loss 1.7957e-01 (2.2098e-01)
Epoch: [2][200/492]	Loss 1.2040e-01 (2.2079e-01)
Epoch: [2][225/492]	Loss 4.5084e-01 (2.2117e-01)
Epoch: [2][250/492]	Loss 3.1003e-01 (2.1834e-01)
Epoch: [2][275/492]	Loss 1.4071e-01 (2.2141e-01)
Epoch: [2][300/492]	Loss 3.0890e-01 (2.2133e-01)
Epoch: [2][325/492]	Loss 1.6192e-01 (2.1981e-01)
Epoch: [2][350/492]	Loss 3.0068e-01 (2.2057e-01)
Epoch: [2][375/492]	Loss 1.1747e-01 (2.2013e-01)
Epoch: [2][400/492]	Loss 1.6484e-01 (2.1939e-01)
Epoch: [2][425/492]	Loss 1.2292e-01 (2.1796e-01)
Epoch: [2][450/492]	Loss 2.2703e-01 (2.1620e-01)
Epoch: [2][475/492]	Loss 2.4480e-01 (2.1623e-01)
Checkpoint ...
[33mEpoch 3/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00099
Train ...
Epoch: [3][  0/492]	Loss 1.0799e-01 (1.0799e-01)
Epoch: [3][ 25/492]	Loss 1.1300e-01 (1.8360e-01)
Epoch: [3][ 50/492]	Loss 2.2523e-01 (1.8556e-01)
Epoch: [3][ 75/492]	Loss 2.1917e-01 (1.8607e-01)
Epoch: [3][100/492]	Loss 2.0126e-01 (1.9132e-01)
Epoch: [3][125/492]	Loss 2.3727e-01 (1.9292e-01)
Epoch: [3][150/492]	Loss 1.7361e-01 (1.9569e-01)
Epoch: [3][175/492]	Loss 2.0867e-01 (1.9805e-01)
Epoch: [3][200/492]	Loss 1.9369e-01 (1.9822e-01)
Epoch: [3][225/492]	Loss 8.3558e-02 (1.9865e-01)
Epoch: [3][250/492]	Loss 1.2413e-01 (1.9637e-01)
Epoch: [3][275/492]	Loss 2.0793e-01 (1.9355e-01)
Epoch: [3][300/492]	Loss 6.8432e-02 (1.9203e-01)
Epoch: [3][325/492]	Loss 2.1671e-01 (1.9202e-01)
Epoch: [3][350/492]	Loss 2.0906e-01 (1.9155e-01)
Epoch: [3][375/492]	Loss 1.9128e-01 (1.9214e-01)
Epoch: [3][400/492]	Loss 2.1361e-01 (1.9115e-01)
Epoch: [3][425/492]	Loss 9.9782e-02 (1.9048e-01)
Epoch: [3][450/492]	Loss 1.5540e-01 (1.9000e-01)
Epoch: [3][475/492]	Loss 2.1771e-01 (1.8846e-01)
Checkpoint ...
[33mEpoch 4/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [4][  0/492]	Loss 2.8445e-01 (2.8445e-01)
Epoch: [4][ 25/492]	Loss 1.5657e-01 (1.7690e-01)
Epoch: [4][ 50/492]	Loss 1.4633e-01 (1.7528e-01)
Epoch: [4][ 75/492]	Loss 1.3645e-01 (1.6923e-01)
Epoch: [4][100/492]	Loss 1.5565e-01 (1.6998e-01)
Epoch: [4][125/492]	Loss 2.6651e-01 (1.6834e-01)
Epoch: [4][150/492]	Loss 1.3780e-01 (1.7026e-01)
Epoch: [4][175/492]	Loss 9.3800e-02 (1.7102e-01)
Epoch: [4][200/492]	Loss 7.4871e-02 (1.7183e-01)
Epoch: [4][225/492]	Loss 1.5316e-01 (1.7124e-01)
Epoch: [4][250/492]	Loss 1.0399e-01 (1.7202e-01)
Epoch: [4][275/492]	Loss 2.7722e-01 (1.7461e-01)
Epoch: [4][300/492]	Loss 2.0145e-01 (1.7472e-01)
Epoch: [4][325/492]	Loss 9.4630e-02 (1.7353e-01)
Epoch: [4][350/492]	Loss 2.7531e-01 (1.7485e-01)
Epoch: [4][375/492]	Loss 1.6725e-01 (1.7432e-01)
Epoch: [4][400/492]	Loss 2.5726e-01 (1.7440e-01)
Epoch: [4][425/492]	Loss 1.0503e-01 (1.7509e-01)
Epoch: [4][450/492]	Loss 1.8409e-01 (1.7479e-01)
Epoch: [4][475/492]	Loss 1.9774e-01 (1.7349e-01)
Checkpoint ...
[33mEpoch 5/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [5][  0/492]	Loss 1.6811e-01 (1.6811e-01)
Epoch: [5][ 25/492]	Loss 3.0508e-01 (1.4875e-01)
Epoch: [5][ 50/492]	Loss 1.6629e-01 (1.5022e-01)
Epoch: [5][ 75/492]	Loss 1.2166e-01 (1.5422e-01)
Epoch: [5][100/492]	Loss 2.2969e-01 (1.5883e-01)
Epoch: [5][125/492]	Loss 1.9406e-01 (1.6436e-01)
Epoch: [5][150/492]	Loss 1.2095e-01 (1.6816e-01)
Epoch: [5][175/492]	Loss 1.3679e-01 (1.7326e-01)
Epoch: [5][200/492]	Loss 1.6388e-01 (1.7164e-01)
Epoch: [5][225/492]	Loss 1.9474e-01 (1.7090e-01)
Epoch: [5][250/492]	Loss 2.7082e-01 (1.7042e-01)
Epoch: [5][275/492]	Loss 2.3521e-01 (1.7041e-01)
Epoch: [5][300/492]	Loss 1.6175e-01 (1.6910e-01)
Epoch: [5][325/492]	Loss 1.4542e-01 (1.6971e-01)
Epoch: [5][350/492]	Loss 9.9509e-02 (1.6884e-01)
Epoch: [5][375/492]	Loss 5.1813e-02 (1.6864e-01)
Epoch: [5][400/492]	Loss 1.5399e-01 (1.6653e-01)
Epoch: [5][425/492]	Loss 2.9140e-01 (1.6740e-01)
Epoch: [5][450/492]	Loss 1.0684e-01 (1.6681e-01)
Epoch: [5][475/492]	Loss 1.1461e-01 (1.6545e-01)
Checkpoint ...
[33mEpoch 6/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00096
Train ...
Epoch: [6][  0/492]	Loss 9.1519e-02 (9.1519e-02)
Epoch: [6][ 25/492]	Loss 1.3556e-01 (1.4318e-01)
Epoch: [6][ 50/492]	Loss 1.6480e-01 (1.5101e-01)
Epoch: [6][ 75/492]	Loss 3.3355e-01 (1.5400e-01)
Epoch: [6][100/492]	Loss 1.4055e-01 (1.5226e-01)
Epoch: [6][125/492]	Loss 1.3121e-01 (1.5478e-01)
Epoch: [6][150/492]	Loss 1.5723e-01 (1.5436e-01)
Epoch: [6][175/492]	Loss 1.4715e-01 (1.5380e-01)
Epoch: [6][200/492]	Loss 1.4570e-01 (1.5379e-01)
Epoch: [6][225/492]	Loss 1.0642e-01 (1.5137e-01)
Epoch: [6][250/492]	Loss 1.4232e-01 (1.4944e-01)
Epoch: [6][275/492]	Loss 8.8036e-02 (1.4671e-01)
Epoch: [6][300/492]	Loss 1.8421e-01 (1.4635e-01)
Epoch: [6][325/492]	Loss 7.9830e-02 (1.4695e-01)
Epoch: [6][350/492]	Loss 1.9832e-01 (1.4677e-01)
Epoch: [6][375/492]	Loss 1.5186e-01 (1.4607e-01)
Epoch: [6][400/492]	Loss 1.1993e-01 (1.4587e-01)
Epoch: [6][425/492]	Loss 1.9178e-01 (1.4491e-01)
Epoch: [6][450/492]	Loss 1.2264e-01 (1.4568e-01)
Epoch: [6][475/492]	Loss 1.3210e-01 (1.4520e-01)
Checkpoint ...
[33mEpoch 7/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00095
Train ...
Epoch: [7][  0/492]	Loss 2.3482e-01 (2.3482e-01)
Epoch: [7][ 25/492]	Loss 2.3475e-01 (1.6127e-01)
Epoch: [7][ 50/492]	Loss 1.2386e-01 (1.5963e-01)
Epoch: [7][ 75/492]	Loss 5.7388e-02 (1.6530e-01)
Epoch: [7][100/492]	Loss 2.5472e-01 (1.6085e-01)
Epoch: [7][125/492]	Loss 1.6688e-01 (1.6085e-01)
Epoch: [7][150/492]	Loss 2.3585e-01 (1.6046e-01)
Epoch: [7][175/492]	Loss 1.1632e-01 (1.5871e-01)
Epoch: [7][200/492]	Loss 1.0835e-01 (1.5578e-01)
Epoch: [7][225/492]	Loss 2.1281e-01 (1.5321e-01)
Epoch: [7][250/492]	Loss 1.1143e-01 (1.5000e-01)
Epoch: [7][275/492]	Loss 1.5184e-01 (1.5013e-01)
Epoch: [7][300/492]	Loss 1.7414e-01 (1.4854e-01)
Epoch: [7][325/492]	Loss 1.0059e-01 (1.4805e-01)
Epoch: [7][350/492]	Loss 2.1741e-01 (1.4691e-01)
Epoch: [7][375/492]	Loss 9.8580e-02 (1.4730e-01)
Epoch: [7][400/492]	Loss 7.6365e-02 (1.4724e-01)
Epoch: [7][425/492]	Loss 1.8869e-01 (1.4680e-01)
Epoch: [7][450/492]	Loss 8.4253e-02 (1.4605e-01)
Epoch: [7][475/492]	Loss 9.1154e-02 (1.4547e-01)
Checkpoint ...
[33mEpoch 8/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00094
Train ...
Epoch: [8][  0/492]	Loss 1.3790e-01 (1.3790e-01)
Epoch: [8][ 25/492]	Loss 1.2239e-01 (1.3889e-01)
Epoch: [8][ 50/492]	Loss 7.1919e-02 (1.3229e-01)
Epoch: [8][ 75/492]	Loss 1.9100e-01 (1.4025e-01)
Epoch: [8][100/492]	Loss 1.1758e-01 (1.3785e-01)
Epoch: [8][125/492]	Loss 1.1169e-01 (1.3697e-01)
Epoch: [8][150/492]	Loss 1.2210e-01 (1.3657e-01)
Epoch: [8][175/492]	Loss 1.4079e-01 (1.3416e-01)
Epoch: [8][200/492]	Loss 1.0207e-01 (1.3432e-01)
Epoch: [8][225/492]	Loss 1.7175e-01 (1.3632e-01)
Epoch: [8][250/492]	Loss 1.5108e-01 (1.3726e-01)
Epoch: [8][275/492]	Loss 9.6039e-02 (1.3716e-01)
Epoch: [8][300/492]	Loss 2.1951e-01 (1.3715e-01)
Epoch: [8][325/492]	Loss 9.8297e-02 (1.3700e-01)
Epoch: [8][350/492]	Loss 9.1994e-02 (1.3805e-01)
Epoch: [8][375/492]	Loss 2.2278e-01 (1.3744e-01)
Epoch: [8][400/492]	Loss 1.0375e-01 (1.3726e-01)
Epoch: [8][425/492]	Loss 7.8845e-02 (1.3646e-01)
Epoch: [8][450/492]	Loss 1.8053e-01 (1.3669e-01)
Epoch: [8][475/492]	Loss 9.5253e-02 (1.3693e-01)
Checkpoint ...
[33mEpoch 9/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00092
Train ...
Epoch: [9][  0/492]	Loss 1.9938e-01 (1.9938e-01)
Epoch: [9][ 25/492]	Loss 3.3346e-01 (1.3451e-01)
Epoch: [9][ 50/492]	Loss 8.8127e-02 (1.2956e-01)
Epoch: [9][ 75/492]	Loss 1.1996e-01 (1.3826e-01)
Epoch: [9][100/492]	Loss 7.7028e-02 (1.3821e-01)
Epoch: [9][125/492]	Loss 1.1752e-01 (1.3466e-01)
Epoch: [9][150/492]	Loss 8.7234e-02 (1.3608e-01)
Epoch: [9][175/492]	Loss 1.4375e-01 (1.3377e-01)
Epoch: [9][200/492]	Loss 1.2140e-01 (1.3194e-01)
Epoch: [9][225/492]	Loss 8.7272e-02 (1.3313e-01)
Epoch: [9][250/492]	Loss 1.7136e-01 (1.3219e-01)
Epoch: [9][275/492]	Loss 9.8802e-02 (1.3098e-01)
Epoch: [9][300/492]	Loss 1.3676e-01 (1.3040e-01)
Epoch: [9][325/492]	Loss 1.1169e-01 (1.3042e-01)
Epoch: [9][350/492]	Loss 6.9296e-02 (1.2969e-01)
Epoch: [9][375/492]	Loss 2.1083e-01 (1.2974e-01)
Epoch: [9][400/492]	Loss 9.9609e-02 (1.3055e-01)
Epoch: [9][425/492]	Loss 1.4089e-01 (1.3060e-01)
Epoch: [9][450/492]	Loss 5.8040e-02 (1.2985e-01)
Epoch: [9][475/492]	Loss 7.3572e-02 (1.2989e-01)
Checkpoint ...
[33mEpoch 10/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00090
Train ...
Epoch: [10][  0/492]	Loss 1.0665e-01 (1.0665e-01)
Epoch: [10][ 25/492]	Loss 1.2775e-01 (1.1637e-01)
Epoch: [10][ 50/492]	Loss 7.7837e-02 (1.1705e-01)
Epoch: [10][ 75/492]	Loss 8.2632e-02 (1.1759e-01)
Epoch: [10][100/492]	Loss 1.5911e-01 (1.1847e-01)
Epoch: [10][125/492]	Loss 4.4902e-02 (1.2150e-01)
Epoch: [10][150/492]	Loss 2.2550e-01 (1.2112e-01)
Epoch: [10][175/492]	Loss 1.6946e-01 (1.2136e-01)
Epoch: [10][200/492]	Loss 4.6441e-02 (1.2456e-01)
Epoch: [10][225/492]	Loss 1.4468e-01 (1.2434e-01)
Epoch: [10][250/492]	Loss 8.8103e-02 (1.2316e-01)
Epoch: [10][275/492]	Loss 1.2120e-01 (1.2328e-01)
Epoch: [10][300/492]	Loss 1.0029e-01 (1.2303e-01)
Epoch: [10][325/492]	Loss 8.6448e-02 (1.2498e-01)
Epoch: [10][350/492]	Loss 1.9632e-01 (1.2559e-01)
Epoch: [10][375/492]	Loss 2.3473e-01 (1.2603e-01)
Epoch: [10][400/492]	Loss 1.3450e-01 (1.2714e-01)
Epoch: [10][425/492]	Loss 8.9799e-02 (1.2801e-01)
Epoch: [10][450/492]	Loss 1.3137e-01 (1.2972e-01)
Epoch: [10][475/492]	Loss 1.3934e-01 (1.2993e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Evaluate ...
Result of kNN evaluation is 96.20
Checkpoint ...
[33mEpoch 11/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00089
Train ...
Epoch: [11][  0/492]	Loss 1.2495e-01 (1.2495e-01)
Epoch: [11][ 25/492]	Loss 1.0904e-01 (1.2255e-01)
Epoch: [11][ 50/492]	Loss 8.2095e-02 (1.1887e-01)
Epoch: [11][ 75/492]	Loss 1.7996e-01 (1.2451e-01)
Epoch: [11][100/492]	Loss 6.9641e-02 (1.2850e-01)
Epoch: [11][125/492]	Loss 1.2961e-01 (1.2892e-01)
Epoch: [11][150/492]	Loss 1.6052e-01 (1.2823e-01)
Epoch: [11][175/492]	Loss 1.1049e-01 (1.2892e-01)
Epoch: [11][200/492]	Loss 1.2732e-01 (1.2758e-01)
Epoch: [11][225/492]	Loss 2.1335e-01 (1.2673e-01)
Epoch: [11][250/492]	Loss 1.0617e-01 (1.2396e-01)
Epoch: [11][275/492]	Loss 1.7659e-01 (1.2475e-01)
Epoch: [11][300/492]	Loss 6.4535e-02 (1.2314e-01)
Epoch: [11][325/492]	Loss 9.2479e-02 (1.2398e-01)
Epoch: [11][350/492]	Loss 1.0361e-01 (1.2307e-01)
Epoch: [11][375/492]	Loss 8.7275e-02 (1.2259e-01)
Epoch: [11][400/492]	Loss 2.5404e-01 (1.2321e-01)
Epoch: [11][425/492]	Loss 1.6619e-01 (1.2362e-01)
Epoch: [11][450/492]	Loss 1.3449e-01 (1.2318e-01)
Epoch: [11][475/492]	Loss 1.1299e-01 (1.2255e-01)
Checkpoint ...
[33mEpoch 12/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00086
Train ...
Epoch: [12][  0/492]	Loss 1.1998e-01 (1.1998e-01)
Epoch: [12][ 25/492]	Loss 7.3879e-02 (1.2513e-01)
Epoch: [12][ 50/492]	Loss 8.7748e-02 (1.2474e-01)
Epoch: [12][ 75/492]	Loss 8.0363e-02 (1.2095e-01)
Epoch: [12][100/492]	Loss 1.1153e-01 (1.2096e-01)
Epoch: [12][125/492]	Loss 1.8199e-01 (1.2143e-01)
Epoch: [12][150/492]	Loss 1.1023e-01 (1.1869e-01)
Epoch: [12][175/492]	Loss 7.5365e-02 (1.1862e-01)
Epoch: [12][200/492]	Loss 1.6895e-01 (1.1739e-01)
Epoch: [12][225/492]	Loss 1.0719e-01 (1.1752e-01)
Epoch: [12][250/492]	Loss 1.3275e-01 (1.1921e-01)
Epoch: [12][275/492]	Loss 9.1034e-02 (1.1920e-01)
Epoch: [12][300/492]	Loss 1.4858e-01 (1.1947e-01)
Epoch: [12][325/492]	Loss 9.4642e-02 (1.1943e-01)
Epoch: [12][350/492]	Loss 1.2417e-01 (1.1926e-01)
Epoch: [12][375/492]	Loss 1.2782e-01 (1.2028e-01)
Epoch: [12][400/492]	Loss 9.7376e-02 (1.1988e-01)
Epoch: [12][425/492]	Loss 5.1458e-02 (1.1968e-01)
Epoch: [12][450/492]	Loss 1.5816e-01 (1.1866e-01)
Epoch: [12][475/492]	Loss 9.5531e-02 (1.1805e-01)
Checkpoint ...
[33mEpoch 13/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00084
Train ...
Epoch: [13][  0/492]	Loss 7.7974e-02 (7.7974e-02)
Epoch: [13][ 25/492]	Loss 1.7214e-01 (1.0405e-01)
Epoch: [13][ 50/492]	Loss 1.0396e-01 (1.1671e-01)
Epoch: [13][ 75/492]	Loss 5.8269e-02 (1.1391e-01)
Epoch: [13][100/492]	Loss 1.4429e-01 (1.1438e-01)
Epoch: [13][125/492]	Loss 1.4779e-01 (1.1662e-01)
Epoch: [13][150/492]	Loss 1.6056e-01 (1.1729e-01)
Epoch: [13][175/492]	Loss 8.7385e-02 (1.1703e-01)
Epoch: [13][200/492]	Loss 1.3290e-01 (1.1735e-01)
Epoch: [13][225/492]	Loss 6.2361e-02 (1.1730e-01)
Epoch: [13][250/492]	Loss 1.9170e-01 (1.1868e-01)
Epoch: [13][275/492]	Loss 1.2113e-01 (1.1887e-01)
Epoch: [13][300/492]	Loss 8.3897e-02 (1.1741e-01)
Epoch: [13][325/492]	Loss 1.8622e-01 (1.1735e-01)
Epoch: [13][350/492]	Loss 1.1071e-01 (1.1767e-01)
Epoch: [13][375/492]	Loss 6.6519e-02 (1.1825e-01)
Epoch: [13][400/492]	Loss 1.4070e-01 (1.1798e-01)
Epoch: [13][425/492]	Loss 9.3633e-02 (1.1796e-01)
Epoch: [13][450/492]	Loss 1.7956e-01 (1.1697e-01)
Epoch: [13][475/492]	Loss 4.8699e-02 (1.1612e-01)
Checkpoint ...
[33mEpoch 14/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00082
Train ...
Epoch: [14][  0/492]	Loss 2.0891e-01 (2.0891e-01)
Epoch: [14][ 25/492]	Loss 2.0342e-01 (1.2765e-01)
Epoch: [14][ 50/492]	Loss 7.1944e-02 (1.2518e-01)
Epoch: [14][ 75/492]	Loss 1.4558e-01 (1.2639e-01)
Epoch: [14][100/492]	Loss 1.5324e-01 (1.2299e-01)
Epoch: [14][125/492]	Loss 1.6821e-01 (1.1852e-01)
Epoch: [14][150/492]	Loss 1.3713e-01 (1.1689e-01)
Epoch: [14][175/492]	Loss 7.6424e-02 (1.1713e-01)
Epoch: [14][200/492]	Loss 5.6639e-02 (1.1572e-01)
Epoch: [14][225/492]	Loss 1.2495e-01 (1.1631e-01)
Epoch: [14][250/492]	Loss 5.1041e-02 (1.1685e-01)
Epoch: [14][275/492]	Loss 1.3027e-01 (1.1512e-01)
Epoch: [14][300/492]	Loss 8.7513e-02 (1.1517e-01)
Epoch: [14][325/492]	Loss 1.0370e-01 (1.1409e-01)
Epoch: [14][350/492]	Loss 5.7283e-02 (1.1301e-01)
Epoch: [14][375/492]	Loss 6.7147e-02 (1.1269e-01)
Epoch: [14][400/492]	Loss 5.8616e-02 (1.1313e-01)
Epoch: [14][425/492]	Loss 7.4123e-02 (1.1350e-01)
Epoch: [14][450/492]	Loss 6.0380e-02 (1.1391e-01)
Epoch: [14][475/492]	Loss 1.0064e-01 (1.1368e-01)
Checkpoint ...
[33mEpoch 15/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00079
Train ...
Epoch: [15][  0/492]	Loss 7.8645e-02 (7.8645e-02)
Epoch: [15][ 25/492]	Loss 1.2405e-01 (1.1855e-01)
Epoch: [15][ 50/492]	Loss 1.2242e-01 (1.1450e-01)
Epoch: [15][ 75/492]	Loss 7.4978e-02 (1.1437e-01)
Epoch: [15][100/492]	Loss 7.5178e-02 (1.1093e-01)
Epoch: [15][125/492]	Loss 8.7720e-02 (1.1273e-01)
Epoch: [15][150/492]	Loss 9.1909e-02 (1.1244e-01)
Epoch: [15][175/492]	Loss 1.2328e-01 (1.1152e-01)
Epoch: [15][200/492]	Loss 6.9647e-02 (1.0908e-01)
Epoch: [15][225/492]	Loss 8.7642e-02 (1.0818e-01)
Epoch: [15][250/492]	Loss 8.0706e-02 (1.0958e-01)
Epoch: [15][275/492]	Loss 1.6895e-01 (1.1010e-01)
Epoch: [15][300/492]	Loss 9.6069e-02 (1.1128e-01)
Epoch: [15][325/492]	Loss 1.4406e-01 (1.1157e-01)
Epoch: [15][350/492]	Loss 1.2943e-01 (1.1058e-01)
Epoch: [15][375/492]	Loss 1.7662e-01 (1.1062e-01)
Epoch: [15][400/492]	Loss 9.8815e-02 (1.1015e-01)
Epoch: [15][425/492]	Loss 6.6909e-02 (1.0993e-01)
Epoch: [15][450/492]	Loss 7.1644e-02 (1.0973e-01)
Epoch: [15][475/492]	Loss 1.7448e-01 (1.0936e-01)
Checkpoint ...
[33mEpoch 16/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00077
Train ...
Epoch: [16][  0/492]	Loss 8.6055e-02 (8.6055e-02)
Epoch: [16][ 25/492]	Loss 9.0917e-02 (1.0281e-01)
Epoch: [16][ 50/492]	Loss 1.3752e-01 (1.1091e-01)
Epoch: [16][ 75/492]	Loss 6.3785e-02 (1.1485e-01)
Epoch: [16][100/492]	Loss 1.3463e-01 (1.1723e-01)
Epoch: [16][125/492]	Loss 6.4603e-02 (1.1696e-01)
Epoch: [16][150/492]	Loss 9.4559e-02 (1.1547e-01)
Epoch: [16][175/492]	Loss 2.0047e-01 (1.1623e-01)
Epoch: [16][200/492]	Loss 1.5793e-01 (1.1575e-01)
Epoch: [16][225/492]	Loss 2.0458e-01 (1.1472e-01)
Epoch: [16][250/492]	Loss 7.0250e-02 (1.1365e-01)
Epoch: [16][275/492]	Loss 6.4477e-02 (1.1291e-01)
Epoch: [16][300/492]	Loss 7.8381e-02 (1.1282e-01)
Epoch: [16][325/492]	Loss 1.0622e-01 (1.1290e-01)
Epoch: [16][350/492]	Loss 5.2564e-02 (1.1229e-01)
Epoch: [16][375/492]	Loss 1.1665e-01 (1.1178e-01)
Epoch: [16][400/492]	Loss 3.8997e-02 (1.1194e-01)
Epoch: [16][425/492]	Loss 8.5881e-02 (1.1173e-01)
Epoch: [16][450/492]	Loss 1.3849e-01 (1.1161e-01)
Epoch: [16][475/492]	Loss 5.2365e-02 (1.1123e-01)
Checkpoint ...
[33mEpoch 17/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00074
Train ...
Epoch: [17][  0/492]	Loss 2.5920e-01 (2.5920e-01)
Epoch: [17][ 25/492]	Loss 1.3546e-01 (9.8712e-02)
Epoch: [17][ 50/492]	Loss 1.6440e-01 (1.0121e-01)
Epoch: [17][ 75/492]	Loss 1.2520e-01 (1.0509e-01)
Epoch: [17][100/492]	Loss 2.1233e-01 (1.0612e-01)
Epoch: [17][125/492]	Loss 8.7493e-02 (1.0556e-01)
Epoch: [17][150/492]	Loss 5.2027e-02 (1.0398e-01)
Epoch: [17][175/492]	Loss 7.6212e-02 (1.0487e-01)
Epoch: [17][200/492]	Loss 9.8539e-02 (1.0317e-01)
Epoch: [17][225/492]	Loss 1.4017e-01 (1.0266e-01)
Epoch: [17][250/492]	Loss 8.7574e-02 (1.0405e-01)
Epoch: [17][275/492]	Loss 9.4500e-02 (1.0465e-01)
Epoch: [17][300/492]	Loss 1.1382e-01 (1.0540e-01)
Epoch: [17][325/492]	Loss 5.1678e-02 (1.0520e-01)
Epoch: [17][350/492]	Loss 1.1685e-01 (1.0494e-01)
Epoch: [17][375/492]	Loss 1.0337e-01 (1.0485e-01)
Epoch: [17][400/492]	Loss 8.9022e-02 (1.0467e-01)
Epoch: [17][425/492]	Loss 6.3290e-02 (1.0443e-01)
Epoch: [17][450/492]	Loss 1.0137e-01 (1.0389e-01)
Epoch: [17][475/492]	Loss 7.4149e-02 (1.0364e-01)
Checkpoint ...
[33mEpoch 18/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00071
Train ...
Epoch: [18][  0/492]	Loss 1.7954e-01 (1.7954e-01)
Epoch: [18][ 25/492]	Loss 5.0294e-02 (1.2022e-01)
Epoch: [18][ 50/492]	Loss 1.1768e-01 (1.1456e-01)
Epoch: [18][ 75/492]	Loss 1.5120e-01 (1.0950e-01)
Epoch: [18][100/492]	Loss 1.3169e-01 (1.0954e-01)
Epoch: [18][125/492]	Loss 5.6611e-02 (1.0796e-01)
Epoch: [18][150/492]	Loss 8.2439e-02 (1.0679e-01)
Epoch: [18][175/492]	Loss 5.1287e-02 (1.0535e-01)
Epoch: [18][200/492]	Loss 5.9451e-02 (1.0478e-01)
Epoch: [18][225/492]	Loss 1.8274e-01 (1.0342e-01)
Epoch: [18][250/492]	Loss 5.0462e-02 (1.0284e-01)
Epoch: [18][275/492]	Loss 5.6785e-02 (1.0284e-01)
Epoch: [18][300/492]	Loss 1.0443e-01 (1.0246e-01)
Epoch: [18][325/492]	Loss 9.4250e-02 (1.0266e-01)
Epoch: [18][350/492]	Loss 1.1962e-01 (1.0451e-01)
Epoch: [18][375/492]	Loss 7.4575e-02 (1.0425e-01)
Epoch: [18][400/492]	Loss 1.7380e-01 (1.0360e-01)
Epoch: [18][425/492]	Loss 8.3311e-02 (1.0362e-01)
Epoch: [18][450/492]	Loss 7.0542e-02 (1.0332e-01)
Epoch: [18][475/492]	Loss 1.6155e-01 (1.0305e-01)
Checkpoint ...
[33mEpoch 19/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00068
Train ...
Epoch: [19][  0/492]	Loss 1.3995e-01 (1.3995e-01)
Epoch: [19][ 25/492]	Loss 1.4058e-01 (1.0206e-01)
Epoch: [19][ 50/492]	Loss 1.9487e-01 (1.0310e-01)
Epoch: [19][ 75/492]	Loss 1.4644e-01 (1.0034e-01)
Epoch: [19][100/492]	Loss 1.7337e-01 (1.0009e-01)
Epoch: [19][125/492]	Loss 1.1207e-01 (9.8910e-02)
Epoch: [19][150/492]	Loss 5.3409e-02 (1.0084e-01)
Epoch: [19][175/492]	Loss 7.7720e-02 (1.0180e-01)
Epoch: [19][200/492]	Loss 7.5555e-02 (1.0199e-01)
Epoch: [19][225/492]	Loss 5.3514e-02 (1.0297e-01)
Epoch: [19][250/492]	Loss 1.1806e-01 (1.0427e-01)
Epoch: [19][275/492]	Loss 4.7197e-02 (1.0319e-01)
Epoch: [19][300/492]	Loss 8.0916e-02 (1.0322e-01)
Epoch: [19][325/492]	Loss 1.0171e-01 (1.0351e-01)
Epoch: [19][350/492]	Loss 5.5119e-02 (1.0393e-01)
Epoch: [19][375/492]	Loss 9.7015e-02 (1.0482e-01)
Epoch: [19][400/492]	Loss 7.1196e-02 (1.0422e-01)
Epoch: [19][425/492]	Loss 8.1445e-02 (1.0370e-01)
Epoch: [19][450/492]	Loss 1.2569e-01 (1.0301e-01)
Epoch: [19][475/492]	Loss 7.0183e-02 (1.0210e-01)
Checkpoint ...
[33mEpoch 20/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00065
Train ...
Epoch: [20][  0/492]	Loss 7.9403e-02 (7.9403e-02)
Epoch: [20][ 25/492]	Loss 1.1061e-01 (1.0182e-01)
Epoch: [20][ 50/492]	Loss 6.5945e-02 (9.8240e-02)
Epoch: [20][ 75/492]	Loss 2.0705e-01 (1.0162e-01)
Epoch: [20][100/492]	Loss 9.0183e-02 (1.0140e-01)
Epoch: [20][125/492]	Loss 1.8194e-01 (1.0190e-01)
Epoch: [20][150/492]	Loss 6.8270e-02 (9.9842e-02)
Epoch: [20][175/492]	Loss 1.7525e-01 (1.0200e-01)
Epoch: [20][200/492]	Loss 4.3572e-02 (1.0140e-01)
Epoch: [20][225/492]	Loss 5.8587e-02 (1.0131e-01)
Epoch: [20][250/492]	Loss 5.8322e-02 (1.0095e-01)
Epoch: [20][275/492]	Loss 8.4996e-02 (9.9772e-02)
Epoch: [20][300/492]	Loss 5.3657e-02 (9.9783e-02)
Epoch: [20][325/492]	Loss 5.5628e-02 (9.9501e-02)
Epoch: [20][350/492]	Loss 9.5088e-02 (1.0034e-01)
Epoch: [20][375/492]	Loss 1.1449e-01 (1.0057e-01)
Epoch: [20][400/492]	Loss 1.2711e-01 (9.9860e-02)
Epoch: [20][425/492]	Loss 9.7511e-02 (9.9946e-02)
Epoch: [20][450/492]	Loss 9.5929e-02 (1.0000e-01)
Epoch: [20][475/492]	Loss 7.2027e-02 (1.0005e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Evaluate ...
Result of kNN evaluation is 97.71
Checkpoint ...
[33mEpoch 21/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00062
Train ...
Epoch: [21][  0/492]	Loss 1.1419e-01 (1.1419e-01)
Epoch: [21][ 25/492]	Loss 6.2899e-02 (1.1847e-01)
Epoch: [21][ 50/492]	Loss 1.6367e-01 (1.0767e-01)
Epoch: [21][ 75/492]	Loss 6.2586e-02 (1.0620e-01)
Epoch: [21][100/492]	Loss 7.4224e-02 (1.0282e-01)
Epoch: [21][125/492]	Loss 6.4280e-02 (1.0229e-01)
Epoch: [21][150/492]	Loss 1.0422e-01 (1.0260e-01)
Epoch: [21][175/492]	Loss 7.1759e-02 (1.0234e-01)
Epoch: [21][200/492]	Loss 5.7362e-02 (1.0049e-01)
Epoch: [21][225/492]	Loss 5.7197e-02 (9.9186e-02)
Epoch: [21][250/492]	Loss 6.6418e-02 (9.9397e-02)
Epoch: [21][275/492]	Loss 5.3652e-02 (9.9632e-02)
Epoch: [21][300/492]	Loss 1.4111e-01 (1.0015e-01)
Epoch: [21][325/492]	Loss 1.2504e-01 (1.0023e-01)
Epoch: [21][350/492]	Loss 3.7939e-02 (9.9111e-02)
Epoch: [21][375/492]	Loss 1.1872e-01 (9.9620e-02)
Epoch: [21][400/492]	Loss 7.1892e-02 (9.9620e-02)
Epoch: [21][425/492]	Loss 1.1799e-01 (9.9707e-02)
Epoch: [21][450/492]	Loss 6.4292e-02 (9.9509e-02)
Epoch: [21][475/492]	Loss 8.4700e-02 (9.9280e-02)
Checkpoint ...
[33mEpoch 22/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00059
Train ...
Epoch: [22][  0/492]	Loss 6.2435e-02 (6.2435e-02)
Epoch: [22][ 25/492]	Loss 7.3983e-02 (8.3131e-02)
Epoch: [22][ 50/492]	Loss 5.5675e-02 (8.8305e-02)
Epoch: [22][ 75/492]	Loss 1.1808e-01 (9.2769e-02)
Epoch: [22][100/492]	Loss 1.1348e-01 (9.3523e-02)
Epoch: [22][125/492]	Loss 6.3003e-02 (9.2694e-02)
Epoch: [22][150/492]	Loss 1.2674e-01 (9.6849e-02)
Epoch: [22][175/492]	Loss 7.7416e-02 (9.6811e-02)
Epoch: [22][200/492]	Loss 5.3376e-02 (9.6306e-02)
Epoch: [22][225/492]	Loss 5.7071e-02 (9.5427e-02)
Epoch: [22][250/492]	Loss 6.1985e-02 (9.5674e-02)
Epoch: [22][275/492]	Loss 8.9904e-02 (9.6071e-02)
Epoch: [22][300/492]	Loss 1.8935e-01 (9.6532e-02)
Epoch: [22][325/492]	Loss 5.1673e-02 (9.7054e-02)
Epoch: [22][350/492]	Loss 7.0280e-02 (9.7148e-02)
Epoch: [22][375/492]	Loss 1.4098e-01 (9.7196e-02)
Epoch: [22][400/492]	Loss 1.0259e-01 (9.8225e-02)
Epoch: [22][425/492]	Loss 4.4589e-02 (9.7897e-02)
Epoch: [22][450/492]	Loss 8.2493e-02 (9.7778e-02)
Epoch: [22][475/492]	Loss 4.5250e-02 (9.7663e-02)
Checkpoint ...
[33mEpoch 23/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00056
Train ...
Epoch: [23][  0/492]	Loss 1.9169e-01 (1.9169e-01)
Epoch: [23][ 25/492]	Loss 6.1173e-02 (9.3363e-02)
Epoch: [23][ 50/492]	Loss 1.2106e-01 (9.7760e-02)
Epoch: [23][ 75/492]	Loss 1.2415e-01 (9.3853e-02)
Epoch: [23][100/492]	Loss 8.4965e-02 (9.6392e-02)
Epoch: [23][125/492]	Loss 6.6636e-02 (9.3731e-02)
Epoch: [23][150/492]	Loss 7.6863e-02 (9.5449e-02)
Epoch: [23][175/492]	Loss 7.7873e-02 (9.5684e-02)
Epoch: [23][200/492]	Loss 1.0868e-01 (9.6233e-02)
Epoch: [23][225/492]	Loss 9.4900e-02 (9.7832e-02)
Epoch: [23][250/492]	Loss 1.2889e-01 (9.7700e-02)
Epoch: [23][275/492]	Loss 1.4480e-01 (9.7336e-02)
Epoch: [23][300/492]	Loss 1.9322e-01 (9.7293e-02)
Epoch: [23][325/492]	Loss 6.7372e-02 (9.7314e-02)
Epoch: [23][350/492]	Loss 1.0401e-01 (9.7887e-02)
Epoch: [23][375/492]	Loss 8.2979e-02 (9.8004e-02)
Epoch: [23][400/492]	Loss 7.0796e-02 (9.6716e-02)
Epoch: [23][425/492]	Loss 6.3964e-02 (9.6841e-02)
Epoch: [23][450/492]	Loss 1.2985e-01 (9.6634e-02)
Epoch: [23][475/492]	Loss 1.2462e-01 (9.6207e-02)
Checkpoint ...
[33mEpoch 24/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00053
Train ...
Epoch: [24][  0/492]	Loss 1.1928e-01 (1.1928e-01)
Epoch: [24][ 25/492]	Loss 7.9072e-02 (9.3587e-02)
Epoch: [24][ 50/492]	Loss 9.3028e-02 (9.1101e-02)
Epoch: [24][ 75/492]	Loss 9.8274e-02 (9.3418e-02)
Epoch: [24][100/492]	Loss 1.2009e-01 (9.2009e-02)
Epoch: [24][125/492]	Loss 5.1437e-02 (9.5105e-02)
Epoch: [24][150/492]	Loss 7.5254e-02 (9.3015e-02)
Epoch: [24][175/492]	Loss 1.0171e-01 (9.2634e-02)
Epoch: [24][200/492]	Loss 5.6125e-02 (9.2935e-02)
Epoch: [24][225/492]	Loss 6.3667e-02 (9.5632e-02)
Epoch: [24][250/492]	Loss 6.2618e-02 (9.5055e-02)
Epoch: [24][275/492]	Loss 1.6514e-01 (9.5281e-02)
Epoch: [24][300/492]	Loss 1.5614e-01 (9.5278e-02)
Epoch: [24][325/492]	Loss 5.7579e-02 (9.4345e-02)
Epoch: [24][350/492]	Loss 7.0893e-02 (9.4494e-02)
Epoch: [24][375/492]	Loss 7.0928e-02 (9.5351e-02)
Epoch: [24][400/492]	Loss 9.8125e-02 (9.4838e-02)
Epoch: [24][425/492]	Loss 1.2585e-01 (9.4762e-02)
Epoch: [24][450/492]	Loss 9.8188e-02 (9.5348e-02)
Epoch: [24][475/492]	Loss 1.1000e-01 (9.4972e-02)
Checkpoint ...
[33mEpoch 25/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00050
Train ...
Epoch: [25][  0/492]	Loss 9.4541e-02 (9.4541e-02)
Epoch: [25][ 25/492]	Loss 4.9810e-02 (8.9229e-02)
Epoch: [25][ 50/492]	Loss 1.3177e-01 (8.5356e-02)
Epoch: [25][ 75/492]	Loss 1.1794e-01 (8.7549e-02)
Epoch: [25][100/492]	Loss 1.1749e-01 (9.4275e-02)
Epoch: [25][125/492]	Loss 1.0066e-01 (9.5347e-02)
Epoch: [25][150/492]	Loss 1.1573e-01 (9.4805e-02)
Epoch: [25][175/492]	Loss 1.5538e-01 (9.6707e-02)
Epoch: [25][200/492]	Loss 1.4023e-01 (9.5342e-02)
Epoch: [25][225/492]	Loss 8.6430e-02 (9.4269e-02)
Epoch: [25][250/492]	Loss 9.1127e-02 (9.3895e-02)
Epoch: [25][275/492]	Loss 2.9523e-01 (9.3560e-02)
Epoch: [25][300/492]	Loss 5.3657e-02 (9.3670e-02)
Epoch: [25][325/492]	Loss 8.5242e-02 (9.3839e-02)
Epoch: [25][350/492]	Loss 5.4368e-02 (9.4213e-02)
Epoch: [25][375/492]	Loss 6.8407e-02 (9.5363e-02)
Epoch: [25][400/492]	Loss 1.2455e-01 (9.5076e-02)
Epoch: [25][425/492]	Loss 1.0867e-01 (9.4823e-02)
Epoch: [25][450/492]	Loss 7.1436e-02 (9.4723e-02)
Epoch: [25][475/492]	Loss 1.1291e-01 (9.4714e-02)
Checkpoint ...
[33mEpoch 26/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00047
Train ...
Epoch: [26][  0/492]	Loss 9.8417e-02 (9.8417e-02)
Epoch: [26][ 25/492]	Loss 9.0580e-02 (9.3256e-02)
Epoch: [26][ 50/492]	Loss 1.0160e-01 (9.6837e-02)
Epoch: [26][ 75/492]	Loss 6.8833e-02 (9.6298e-02)
Epoch: [26][100/492]	Loss 7.2344e-02 (9.4626e-02)
Epoch: [26][125/492]	Loss 1.1452e-01 (9.2770e-02)
Epoch: [26][150/492]	Loss 6.3662e-02 (9.3553e-02)
Epoch: [26][175/492]	Loss 1.1987e-01 (9.1573e-02)
Epoch: [26][200/492]	Loss 1.0980e-01 (9.3192e-02)
Epoch: [26][225/492]	Loss 6.0269e-02 (9.3461e-02)
Epoch: [26][250/492]	Loss 5.0564e-02 (9.3860e-02)
Epoch: [26][275/492]	Loss 6.4165e-02 (9.3123e-02)
Epoch: [26][300/492]	Loss 5.3689e-02 (9.2841e-02)
Epoch: [26][325/492]	Loss 1.0378e-01 (9.3471e-02)
Epoch: [26][350/492]	Loss 5.7535e-02 (9.3434e-02)
Epoch: [26][375/492]	Loss 6.4572e-02 (9.4398e-02)
Epoch: [26][400/492]	Loss 5.0215e-02 (9.3631e-02)
Epoch: [26][425/492]	Loss 4.6807e-02 (9.3689e-02)
Epoch: [26][450/492]	Loss 1.8004e-01 (9.3460e-02)
Epoch: [26][475/492]	Loss 1.3049e-01 (9.3302e-02)
Checkpoint ...
[33mEpoch 27/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00044
Train ...
Epoch: [27][  0/492]	Loss 1.0252e-01 (1.0252e-01)
Epoch: [27][ 25/492]	Loss 1.0548e-01 (7.7469e-02)
Epoch: [27][ 50/492]	Loss 8.7643e-02 (8.7511e-02)
Epoch: [27][ 75/492]	Loss 8.8994e-02 (8.7490e-02)
Epoch: [27][100/492]	Loss 5.4230e-02 (9.3461e-02)
Epoch: [27][125/492]	Loss 1.1956e-01 (9.1023e-02)
Epoch: [27][150/492]	Loss 2.2878e-01 (9.0963e-02)
Epoch: [27][175/492]	Loss 8.4371e-02 (9.1650e-02)
Epoch: [27][200/492]	Loss 1.0304e-01 (9.2819e-02)
Epoch: [27][225/492]	Loss 7.9525e-02 (9.3785e-02)
Epoch: [27][250/492]	Loss 1.8365e-01 (9.3170e-02)
Epoch: [27][275/492]	Loss 7.8724e-02 (9.2945e-02)
Epoch: [27][300/492]	Loss 1.5043e-01 (9.2770e-02)
Epoch: [27][325/492]	Loss 7.8993e-02 (9.3002e-02)
Epoch: [27][350/492]	Loss 6.7943e-02 (9.3195e-02)
Epoch: [27][375/492]	Loss 9.5759e-02 (9.3360e-02)
Epoch: [27][400/492]	Loss 7.8627e-02 (9.2969e-02)
Epoch: [27][425/492]	Loss 6.5936e-02 (9.3071e-02)
Epoch: [27][450/492]	Loss 1.0907e-01 (9.2984e-02)
Epoch: [27][475/492]	Loss 7.2371e-02 (9.2628e-02)
Checkpoint ...
[33mEpoch 28/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00041
Train ...
Epoch: [28][  0/492]	Loss 1.3181e-01 (1.3181e-01)
Epoch: [28][ 25/492]	Loss 1.6836e-01 (9.7837e-02)
Epoch: [28][ 50/492]	Loss 5.3444e-02 (9.5928e-02)
Epoch: [28][ 75/492]	Loss 7.7841e-02 (9.6245e-02)
Epoch: [28][100/492]	Loss 1.2421e-01 (1.0072e-01)
Epoch: [28][125/492]	Loss 7.2021e-02 (9.9031e-02)
Epoch: [28][150/492]	Loss 1.2634e-01 (9.8200e-02)
Epoch: [28][175/492]	Loss 6.6646e-02 (9.7736e-02)
Epoch: [28][200/492]	Loss 1.2160e-01 (9.6786e-02)
Epoch: [28][225/492]	Loss 8.2109e-02 (9.4742e-02)
Epoch: [28][250/492]	Loss 9.4822e-02 (9.3558e-02)
Epoch: [28][275/492]	Loss 1.1152e-01 (9.3647e-02)
Epoch: [28][300/492]	Loss 9.2645e-02 (9.3747e-02)
Epoch: [28][325/492]	Loss 1.7995e-01 (9.3896e-02)
Epoch: [28][350/492]	Loss 7.8848e-02 (9.4680e-02)
Epoch: [28][375/492]	Loss 1.6254e-01 (9.4808e-02)
Epoch: [28][400/492]	Loss 1.2145e-01 (9.4600e-02)
Epoch: [28][425/492]	Loss 6.6981e-02 (9.4554e-02)
Epoch: [28][450/492]	Loss 7.8611e-02 (9.4033e-02)
Epoch: [28][475/492]	Loss 1.1999e-01 (9.4358e-02)
Checkpoint ...
[33mEpoch 29/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00038
Train ...
Epoch: [29][  0/492]	Loss 6.8148e-02 (6.8148e-02)
Epoch: [29][ 25/492]	Loss 9.7396e-02 (8.4793e-02)
Epoch: [29][ 50/492]	Loss 7.4851e-02 (8.5523e-02)
Epoch: [29][ 75/492]	Loss 9.8293e-02 (8.3188e-02)
Epoch: [29][100/492]	Loss 1.3583e-01 (8.4454e-02)
Epoch: [29][125/492]	Loss 1.3034e-01 (8.8129e-02)
Epoch: [29][150/492]	Loss 1.0839e-01 (8.8352e-02)
Epoch: [29][175/492]	Loss 6.6173e-02 (8.9750e-02)
Epoch: [29][200/492]	Loss 1.2872e-01 (9.1448e-02)
Epoch: [29][225/492]	Loss 7.0321e-02 (9.1241e-02)
Epoch: [29][250/492]	Loss 8.8222e-02 (9.1143e-02)
Epoch: [29][275/492]	Loss 8.3445e-02 (9.0992e-02)
Epoch: [29][300/492]	Loss 4.4036e-02 (9.0013e-02)
Epoch: [29][325/492]	Loss 1.4038e-01 (8.9408e-02)
Epoch: [29][350/492]	Loss 4.9444e-02 (8.9161e-02)
Epoch: [29][375/492]	Loss 6.1284e-02 (8.8753e-02)
Epoch: [29][400/492]	Loss 6.9723e-02 (8.8649e-02)
Epoch: [29][425/492]	Loss 8.1472e-02 (8.8766e-02)
Epoch: [29][450/492]	Loss 9.9338e-02 (8.8981e-02)
Epoch: [29][475/492]	Loss 4.1547e-02 (8.8864e-02)
Checkpoint ...
[33mEpoch 30/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00035
Train ...
Epoch: [30][  0/492]	Loss 6.3851e-02 (6.3851e-02)
Epoch: [30][ 25/492]	Loss 9.2932e-02 (7.6700e-02)
Epoch: [30][ 50/492]	Loss 9.3030e-02 (8.2273e-02)
Epoch: [30][ 75/492]	Loss 7.2047e-02 (8.3572e-02)
Epoch: [30][100/492]	Loss 1.0107e-01 (8.3810e-02)
Epoch: [30][125/492]	Loss 4.0567e-02 (8.2869e-02)
Epoch: [30][150/492]	Loss 1.1035e-01 (8.2260e-02)
Epoch: [30][175/492]	Loss 1.4854e-01 (8.4268e-02)
Epoch: [30][200/492]	Loss 8.9067e-02 (8.6071e-02)
Epoch: [30][225/492]	Loss 4.9335e-02 (8.6176e-02)
Epoch: [30][250/492]	Loss 7.0950e-02 (8.5685e-02)
Epoch: [30][275/492]	Loss 2.9019e-01 (8.5860e-02)
Epoch: [30][300/492]	Loss 5.1473e-02 (8.6568e-02)
Epoch: [30][325/492]	Loss 1.3788e-01 (8.6208e-02)
Epoch: [30][350/492]	Loss 6.3227e-02 (8.6133e-02)
Epoch: [30][375/492]	Loss 8.7727e-02 (8.5991e-02)
Epoch: [30][400/492]	Loss 1.0433e-01 (8.6100e-02)
Epoch: [30][425/492]	Loss 4.3609e-02 (8.6226e-02)
Epoch: [30][450/492]	Loss 6.3814e-02 (8.6756e-02)
Epoch: [30][475/492]	Loss 7.3492e-02 (8.6251e-02)
Fill memory bank for kNN...
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Evaluate ...
Result of kNN evaluation is 97.78
Checkpoint ...
[33mEpoch 31/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00032
Train ...
Epoch: [31][  0/492]	Loss 6.1112e-02 (6.1112e-02)
Epoch: [31][ 25/492]	Loss 7.8508e-02 (9.5999e-02)
Epoch: [31][ 50/492]	Loss 1.4158e-01 (9.5235e-02)
Epoch: [31][ 75/492]	Loss 6.4530e-02 (9.3220e-02)
Epoch: [31][100/492]	Loss 1.1156e-01 (9.2645e-02)
Epoch: [31][125/492]	Loss 6.6484e-02 (9.2929e-02)
Epoch: [31][150/492]	Loss 6.5619e-02 (9.0959e-02)
Epoch: [31][175/492]	Loss 1.0567e-01 (9.0698e-02)
Epoch: [31][200/492]	Loss 1.1022e-01 (9.3066e-02)
Epoch: [31][225/492]	Loss 7.6757e-02 (9.0528e-02)
Epoch: [31][250/492]	Loss 8.8185e-02 (8.9648e-02)
Epoch: [31][275/492]	Loss 9.8189e-02 (8.9392e-02)
Epoch: [31][300/492]	Loss 6.8184e-02 (9.0280e-02)
Epoch: [31][325/492]	Loss 1.4156e-01 (9.1164e-02)
Epoch: [31][350/492]	Loss 9.2216e-02 (9.1379e-02)
Epoch: [31][375/492]	Loss 7.4064e-02 (9.0830e-02)
Epoch: [31][400/492]	Loss 1.1070e-01 (9.0522e-02)
Epoch: [31][425/492]	Loss 4.4157e-02 (9.1162e-02)
Epoch: [31][450/492]	Loss 1.0380e-01 (9.0965e-02)
Epoch: [31][475/492]	Loss 8.0396e-02 (9.0739e-02)
Checkpoint ...
[33mEpoch 32/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00029
Train ...
Epoch: [32][  0/492]	Loss 5.2908e-02 (5.2908e-02)
Epoch: [32][ 25/492]	Loss 1.0306e-01 (8.0500e-02)
Epoch: [32][ 50/492]	Loss 8.8887e-02 (8.6636e-02)
Epoch: [32][ 75/492]	Loss 6.9935e-02 (8.7065e-02)
Epoch: [32][100/492]	Loss 7.2119e-02 (8.5512e-02)
Epoch: [32][125/492]	Loss 9.4831e-02 (8.4558e-02)
Epoch: [32][150/492]	Loss 9.6880e-02 (8.5472e-02)
Epoch: [32][175/492]	Loss 7.7996e-02 (8.5396e-02)
Epoch: [32][200/492]	Loss 6.5522e-02 (8.5564e-02)
Epoch: [32][225/492]	Loss 6.1760e-02 (8.6343e-02)
Epoch: [32][250/492]	Loss 6.3104e-02 (8.6226e-02)
Epoch: [32][275/492]	Loss 1.5091e-01 (8.6258e-02)
Epoch: [32][300/492]	Loss 1.9241e-01 (8.7323e-02)
Epoch: [32][325/492]	Loss 6.6901e-02 (8.7257e-02)
Epoch: [32][350/492]	Loss 1.8114e-01 (8.9069e-02)
Epoch: [32][375/492]	Loss 1.0719e-01 (8.8609e-02)
Epoch: [32][400/492]	Loss 1.3326e-01 (8.9138e-02)
Epoch: [32][425/492]	Loss 3.3591e-02 (8.8696e-02)
Epoch: [32][450/492]	Loss 1.9397e-01 (8.9126e-02)
Epoch: [32][475/492]	Loss 6.1953e-02 (8.9071e-02)
Checkpoint ...
[33mEpoch 33/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00026
Train ...
Epoch: [33][  0/492]	Loss 1.0773e-01 (1.0773e-01)
Epoch: [33][ 25/492]	Loss 1.2545e-01 (7.3040e-02)
Epoch: [33][ 50/492]	Loss 7.7805e-02 (8.3823e-02)
Epoch: [33][ 75/492]	Loss 6.1769e-02 (8.3278e-02)
Epoch: [33][100/492]	Loss 9.0610e-02 (8.3592e-02)
Epoch: [33][125/492]	Loss 5.7279e-02 (8.5939e-02)
Epoch: [33][150/492]	Loss 8.6585e-02 (8.5192e-02)
Epoch: [33][175/492]	Loss 1.6135e-01 (8.5706e-02)
Epoch: [33][200/492]	Loss 1.2576e-01 (8.6631e-02)
Epoch: [33][225/492]	Loss 1.3599e-01 (8.6640e-02)
Epoch: [33][250/492]	Loss 1.1359e-01 (8.5761e-02)
Epoch: [33][275/492]	Loss 4.7148e-02 (8.7064e-02)
Epoch: [33][300/492]	Loss 1.2290e-01 (8.6773e-02)
Epoch: [33][325/492]	Loss 8.6062e-02 (8.6209e-02)
Epoch: [33][350/492]	Loss 5.0310e-02 (8.6333e-02)
Epoch: [33][375/492]	Loss 1.0777e-01 (8.6396e-02)
Epoch: [33][400/492]	Loss 3.5695e-02 (8.6074e-02)
Epoch: [33][425/492]	Loss 8.3246e-02 (8.6265e-02)
Epoch: [33][450/492]	Loss 1.4565e-01 (8.6961e-02)
Epoch: [33][475/492]	Loss 8.0860e-02 (8.6981e-02)
Checkpoint ...
[33mEpoch 34/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00023
Train ...
Epoch: [34][  0/492]	Loss 7.1154e-02 (7.1154e-02)
Epoch: [34][ 25/492]	Loss 5.7303e-02 (9.5049e-02)
Epoch: [34][ 50/492]	Loss 8.5065e-02 (9.1866e-02)
Epoch: [34][ 75/492]	Loss 4.4757e-02 (9.2604e-02)
Epoch: [34][100/492]	Loss 1.3609e-01 (9.0736e-02)
Epoch: [34][125/492]	Loss 1.0811e-01 (8.8992e-02)
Epoch: [34][150/492]	Loss 9.4294e-02 (8.5911e-02)
Epoch: [34][175/492]	Loss 5.3358e-02 (8.4098e-02)
Epoch: [34][200/492]	Loss 5.7840e-02 (8.3543e-02)
Epoch: [34][225/492]	Loss 7.8443e-02 (8.4674e-02)
Epoch: [34][250/492]	Loss 9.2025e-02 (8.6254e-02)
Epoch: [34][275/492]	Loss 1.5041e-01 (8.8552e-02)
Epoch: [34][300/492]	Loss 1.3995e-01 (8.8215e-02)
Epoch: [34][325/492]	Loss 1.3341e-01 (8.8807e-02)
Epoch: [34][350/492]	Loss 7.9179e-02 (8.9192e-02)
Epoch: [34][375/492]	Loss 6.1830e-02 (8.8769e-02)
Epoch: [34][400/492]	Loss 4.0572e-02 (8.8298e-02)
Epoch: [34][425/492]	Loss 8.4916e-02 (8.8252e-02)
Epoch: [34][450/492]	Loss 1.3981e-01 (8.8286e-02)
Epoch: [34][475/492]	Loss 5.2084e-02 (8.8573e-02)
Checkpoint ...
[33mEpoch 35/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00021
Train ...
Epoch: [35][  0/492]	Loss 9.7330e-02 (9.7330e-02)
Epoch: [35][ 25/492]	Loss 5.7133e-02 (7.3722e-02)
Epoch: [35][ 50/492]	Loss 7.4101e-02 (8.0695e-02)
Epoch: [35][ 75/492]	Loss 7.1877e-02 (7.9449e-02)
Epoch: [35][100/492]	Loss 7.3040e-02 (8.0081e-02)
Epoch: [35][125/492]	Loss 8.3261e-02 (7.9890e-02)
Epoch: [35][150/492]	Loss 1.0867e-01 (7.9574e-02)
Epoch: [35][175/492]	Loss 9.1510e-02 (8.1179e-02)
Epoch: [35][200/492]	Loss 8.6622e-02 (8.1610e-02)
Epoch: [35][225/492]	Loss 5.6551e-02 (8.1083e-02)
Epoch: [35][250/492]	Loss 1.1778e-01 (8.3044e-02)
Epoch: [35][275/492]	Loss 9.8490e-02 (8.3748e-02)
Epoch: [35][300/492]	Loss 1.4176e-01 (8.4421e-02)
Epoch: [35][325/492]	Loss 1.2863e-01 (8.4770e-02)
Epoch: [35][350/492]	Loss 7.3258e-02 (8.5773e-02)
Epoch: [35][375/492]	Loss 5.5107e-02 (8.5797e-02)
Epoch: [35][400/492]	Loss 1.0413e-01 (8.6006e-02)
Epoch: [35][425/492]	Loss 7.8817e-02 (8.5321e-02)
Epoch: [35][450/492]	Loss 1.7088e-01 (8.5740e-02)
Epoch: [35][475/492]	Loss 1.0274e-01 (8.5719e-02)
Checkpoint ...
[33mEpoch 36/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00018
Train ...
Epoch: [36][  0/492]	Loss 1.5568e-01 (1.5568e-01)
Epoch: [36][ 25/492]	Loss 3.8405e-02 (8.5572e-02)
Epoch: [36][ 50/492]	Loss 9.0736e-02 (8.5356e-02)
Epoch: [36][ 75/492]	Loss 8.5102e-02 (8.8230e-02)
Epoch: [36][100/492]	Loss 8.9597e-02 (8.8585e-02)
Epoch: [36][125/492]	Loss 5.8553e-02 (8.6506e-02)
Epoch: [36][150/492]	Loss 5.7208e-02 (8.5274e-02)
Epoch: [36][175/492]	Loss 7.7480e-02 (8.4854e-02)
Epoch: [36][200/492]	Loss 7.2241e-02 (8.3320e-02)
Epoch: [36][225/492]	Loss 5.4833e-02 (8.4213e-02)
Epoch: [36][250/492]	Loss 5.1776e-02 (8.5171e-02)
Epoch: [36][275/492]	Loss 4.2752e-02 (8.4727e-02)
Epoch: [36][300/492]	Loss 1.4266e-01 (8.4651e-02)
Epoch: [36][325/492]	Loss 7.9643e-02 (8.5365e-02)
Epoch: [36][350/492]	Loss 5.1562e-02 (8.5335e-02)
Epoch: [36][375/492]	Loss 5.9042e-02 (8.5818e-02)
Epoch: [36][400/492]	Loss 7.3587e-02 (8.4680e-02)
Epoch: [36][425/492]	Loss 4.5321e-02 (8.4013e-02)
Epoch: [36][450/492]	Loss 1.1398e-01 (8.4114e-02)
Epoch: [36][475/492]	Loss 9.4607e-02 (8.4135e-02)
Checkpoint ...
[33mEpoch 37/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00016
Train ...
Epoch: [37][  0/492]	Loss 5.2379e-02 (5.2379e-02)
Epoch: [37][ 25/492]	Loss 5.1327e-02 (8.0270e-02)
Epoch: [37][ 50/492]	Loss 6.3413e-02 (7.6417e-02)
Epoch: [37][ 75/492]	Loss 1.0344e-01 (8.2071e-02)
Epoch: [37][100/492]	Loss 5.3217e-02 (8.4670e-02)
Epoch: [37][125/492]	Loss 5.8196e-02 (8.2970e-02)
Epoch: [37][150/492]	Loss 5.5598e-02 (8.3503e-02)
Epoch: [37][175/492]	Loss 4.3502e-02 (8.3645e-02)
Epoch: [37][200/492]	Loss 7.6600e-02 (8.3035e-02)
Epoch: [37][225/492]	Loss 9.2318e-02 (8.4166e-02)
Epoch: [37][250/492]	Loss 7.6536e-02 (8.3516e-02)
Epoch: [37][275/492]	Loss 4.9935e-02 (8.4378e-02)
Epoch: [37][300/492]	Loss 4.7829e-02 (8.5097e-02)
Epoch: [37][325/492]	Loss 1.0770e-01 (8.4601e-02)
Epoch: [37][350/492]	Loss 7.6571e-02 (8.4447e-02)
Epoch: [37][375/492]	Loss 5.8145e-02 (8.4371e-02)
Epoch: [37][400/492]	Loss 4.5127e-02 (8.4230e-02)
Epoch: [37][425/492]	Loss 8.5157e-02 (8.4174e-02)
Epoch: [37][450/492]	Loss 1.3202e-01 (8.4048e-02)
Epoch: [37][475/492]	Loss 5.8349e-02 (8.3883e-02)
Checkpoint ...
[33mEpoch 38/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00014
Train ...
Epoch: [38][  0/492]	Loss 7.7935e-02 (7.7935e-02)
Epoch: [38][ 25/492]	Loss 3.4198e-02 (8.5615e-02)
Epoch: [38][ 50/492]	Loss 4.7279e-02 (8.1686e-02)
Epoch: [38][ 75/492]	Loss 6.8341e-02 (8.2367e-02)
Epoch: [38][100/492]	Loss 4.5154e-02 (8.3305e-02)
Epoch: [38][125/492]	Loss 8.7255e-02 (8.4053e-02)
Epoch: [38][150/492]	Loss 5.4491e-02 (8.5343e-02)
Epoch: [38][175/492]	Loss 5.5174e-02 (8.4503e-02)
Epoch: [38][200/492]	Loss 5.1644e-02 (8.4100e-02)
Epoch: [38][225/492]	Loss 1.3749e-01 (8.4979e-02)
Epoch: [38][250/492]	Loss 1.1581e-01 (8.5627e-02)
Epoch: [38][275/492]	Loss 1.3213e-01 (8.6430e-02)
Epoch: [38][300/492]	Loss 6.1873e-02 (8.6796e-02)
Epoch: [38][325/492]	Loss 5.4698e-02 (8.7657e-02)
Epoch: [38][350/492]	Loss 8.4344e-02 (8.8467e-02)
Epoch: [38][375/492]	Loss 8.4170e-02 (8.7567e-02)
Epoch: [38][400/492]	Loss 4.2545e-02 (8.7106e-02)
Epoch: [38][425/492]	Loss 6.8075e-02 (8.7217e-02)
Epoch: [38][450/492]	Loss 7.0650e-02 (8.7316e-02)
Epoch: [38][475/492]	Loss 9.3495e-02 (8.6734e-02)
Checkpoint ...
[33mEpoch 39/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00012
Train ...
Epoch: [39][  0/492]	Loss 8.6676e-02 (8.6676e-02)
Epoch: [39][ 25/492]	Loss 6.3873e-02 (7.7795e-02)
Epoch: [39][ 50/492]	Loss 1.0056e-01 (8.4864e-02)
Epoch: [39][ 75/492]	Loss 6.7396e-02 (8.5668e-02)
Epoch: [39][100/492]	Loss 1.1704e-01 (8.7081e-02)
Epoch: [39][125/492]	Loss 7.8561e-02 (8.6176e-02)
Epoch: [39][150/492]	Loss 9.4874e-02 (8.5019e-02)
Epoch: [39][175/492]	Loss 9.0900e-02 (8.5432e-02)
Epoch: [39][200/492]	Loss 8.7334e-02 (8.5866e-02)
Epoch: [39][225/492]	Loss 6.3850e-02 (8.6179e-02)
Epoch: [39][250/492]	Loss 1.2192e-01 (8.6305e-02)
Epoch: [39][275/492]	Loss 9.4710e-02 (8.6104e-02)
Epoch: [39][300/492]	Loss 6.0220e-02 (8.5534e-02)
Epoch: [39][325/492]	Loss 1.1017e-01 (8.6081e-02)
Epoch: [39][350/492]	Loss 1.3483e-01 (8.5362e-02)
Epoch: [39][375/492]	Loss 8.6870e-02 (8.4042e-02)
Epoch: [39][400/492]	Loss 7.0622e-02 (8.3923e-02)
Epoch: [39][425/492]	Loss 4.7155e-02 (8.3594e-02)
Epoch: [39][450/492]	Loss 8.9554e-02 (8.4348e-02)
Epoch: [39][475/492]	Loss 6.3331e-02 (8.3757e-02)
Checkpoint ...
[33mEpoch 40/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [40][  0/492]	Loss 1.7596e-01 (1.7596e-01)
Epoch: [40][ 25/492]	Loss 5.7908e-02 (8.5340e-02)
Epoch: [40][ 50/492]	Loss 6.2080e-02 (7.9392e-02)
Epoch: [40][ 75/492]	Loss 6.6539e-02 (8.0164e-02)
Epoch: [40][100/492]	Loss 8.1963e-02 (8.1744e-02)
Epoch: [40][125/492]	Loss 1.7012e-01 (8.0453e-02)
Epoch: [40][150/492]	Loss 1.2590e-01 (8.1688e-02)
Epoch: [40][175/492]	Loss 9.6945e-02 (8.1474e-02)
Epoch: [40][200/492]	Loss 4.1179e-02 (8.0748e-02)
Epoch: [40][225/492]	Loss 9.4329e-02 (8.0568e-02)
Epoch: [40][250/492]	Loss 8.5878e-02 (8.1164e-02)
Epoch: [40][275/492]	Loss 6.3157e-02 (8.0674e-02)
Epoch: [40][300/492]	Loss 5.1969e-02 (8.2061e-02)
Epoch: [40][325/492]	Loss 6.5485e-02 (8.2817e-02)
Epoch: [40][350/492]	Loss 6.0834e-02 (8.3742e-02)
Epoch: [40][375/492]	Loss 8.2241e-02 (8.3904e-02)
Epoch: [40][400/492]	Loss 4.4384e-02 (8.3778e-02)
Epoch: [40][425/492]	Loss 6.2035e-02 (8.4135e-02)
Epoch: [40][450/492]	Loss 4.7562e-02 (8.3963e-02)
Epoch: [40][475/492]	Loss 8.6335e-02 (8.4031e-02)
Fill memory bank for kNN...
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Evaluate ...
Result of kNN evaluation is 97.90
Checkpoint ...
[33mEpoch 41/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00008
Train ...
Epoch: [41][  0/492]	Loss 1.2555e-01 (1.2555e-01)
Epoch: [41][ 25/492]	Loss 1.3178e-01 (9.8573e-02)
Epoch: [41][ 50/492]	Loss 7.1870e-02 (9.0452e-02)
Epoch: [41][ 75/492]	Loss 1.3048e-01 (9.0055e-02)
Epoch: [41][100/492]	Loss 2.4723e-01 (8.7034e-02)
Epoch: [41][125/492]	Loss 7.1476e-02 (8.5238e-02)
Epoch: [41][150/492]	Loss 5.4866e-02 (8.3876e-02)
Epoch: [41][175/492]	Loss 9.6763e-02 (8.4158e-02)
Epoch: [41][200/492]	Loss 1.2236e-01 (8.5927e-02)
Epoch: [41][225/492]	Loss 8.1869e-02 (8.4913e-02)
Epoch: [41][250/492]	Loss 3.3799e-02 (8.5183e-02)
Epoch: [41][275/492]	Loss 5.8795e-02 (8.5686e-02)
Epoch: [41][300/492]	Loss 6.8032e-02 (8.7059e-02)
Epoch: [41][325/492]	Loss 5.5102e-02 (8.8004e-02)
Epoch: [41][350/492]	Loss 5.0214e-02 (8.6710e-02)
Epoch: [41][375/492]	Loss 8.0307e-02 (8.6416e-02)
Epoch: [41][400/492]	Loss 1.0688e-01 (8.5886e-02)
Epoch: [41][425/492]	Loss 6.2829e-02 (8.6083e-02)
Epoch: [41][450/492]	Loss 7.0753e-02 (8.5723e-02)
Epoch: [41][475/492]	Loss 4.9136e-02 (8.5299e-02)
Checkpoint ...
[33mEpoch 42/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00006
Train ...
slurmstepd: error: *** JOB 745430 ON vulcan02 CANCELLED AT 2021-03-28T01:56:39 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 745430.0 ON vulcan02 CANCELLED AT 2021-03-28T01:56:39 DUE TO TIME LIMIT ***
