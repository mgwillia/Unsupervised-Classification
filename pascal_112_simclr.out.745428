vulcan02.umiacs.umd.edu
[31m{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-112', 'val_db_name': 'pascal-pretrained-112', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 256, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 112, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 112, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/topk-val-neighbors.npy'}[0m
[34mRetrieve model[0m
{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-112', 'val_db_name': 'pascal-pretrained-112', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 256, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 112, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 112, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/topk-val-neighbors.npy'}
loading pretrained
Model is ContrastiveModel
Model parameters: 30.02M
ContrastiveModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (contrastive_head): Sequential(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU()
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[34mSet CuDNN benchmark[0m
[34mRetrieve dataset[0m
Train transforms: Compose(
    RandomResizedCrop(size=(112, 112), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)
    RandomHorizontalFlip(p=0.5)
    RandomApply(
    p=0.8
    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[-0.1, 0.1])
)
    RandomGrayscale(p=0.2)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Validation transforms: Compose(
    CenterCrop(size=(112, 112))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Dataset contains 15774/15774 train/val samples
[34mBuild MemoryBank[0m
[34mRetrieve criterion[0m
Criterion is SimCLRLoss
[34mRetrieve optimizer[0m
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.001
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
[34mNo checkpoint file at /cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/checkpoint.pth.tar[0m
[34mStarting main loop[0m
[33mEpoch 0/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [0][ 0/61]	Loss 4.3178e+00 (4.3178e+00)
Epoch: [0][25/61]	Loss 2.2819e+00 (2.7350e+00)
Epoch: [0][50/61]	Loss 2.0228e+00 (2.3988e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 88.03
Checkpoint ...
[33mEpoch 1/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [1][ 0/61]	Loss 1.8472e+00 (1.8472e+00)
Epoch: [1][25/61]	Loss 1.8415e+00 (1.7632e+00)
Epoch: [1][50/61]	Loss 1.6103e+00 (1.6986e+00)
Checkpoint ...
[33mEpoch 2/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [2][ 0/61]	Loss 1.6172e+00 (1.6172e+00)
Epoch: [2][25/61]	Loss 1.4946e+00 (1.5398e+00)
Epoch: [2][50/61]	Loss 1.4639e+00 (1.5110e+00)
Checkpoint ...
[33mEpoch 3/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00099
Train ...
Epoch: [3][ 0/61]	Loss 1.4562e+00 (1.4562e+00)
Epoch: [3][25/61]	Loss 1.3609e+00 (1.4169e+00)
Epoch: [3][50/61]	Loss 1.4763e+00 (1.3998e+00)
Checkpoint ...
[33mEpoch 4/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [4][ 0/61]	Loss 1.4356e+00 (1.4356e+00)
Epoch: [4][25/61]	Loss 1.2690e+00 (1.3108e+00)
Epoch: [4][50/61]	Loss 1.2161e+00 (1.3081e+00)
Checkpoint ...
[33mEpoch 5/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [5][ 0/61]	Loss 1.4108e+00 (1.4108e+00)
Epoch: [5][25/61]	Loss 1.2481e+00 (1.2777e+00)
Epoch: [5][50/61]	Loss 1.2634e+00 (1.2470e+00)
Checkpoint ...
[33mEpoch 6/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00096
Train ...
Epoch: [6][ 0/61]	Loss 1.1579e+00 (1.1579e+00)
Epoch: [6][25/61]	Loss 1.2331e+00 (1.2041e+00)
Epoch: [6][50/61]	Loss 1.2270e+00 (1.2037e+00)
Checkpoint ...
[33mEpoch 7/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00095
Train ...
Epoch: [7][ 0/61]	Loss 1.0888e+00 (1.0888e+00)
Epoch: [7][25/61]	Loss 1.1289e+00 (1.1608e+00)
Epoch: [7][50/61]	Loss 1.0640e+00 (1.1547e+00)
Checkpoint ...
[33mEpoch 8/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00094
Train ...
Epoch: [8][ 0/61]	Loss 1.0447e+00 (1.0447e+00)
Epoch: [8][25/61]	Loss 1.1433e+00 (1.1324e+00)
Epoch: [8][50/61]	Loss 1.0894e+00 (1.1139e+00)
Checkpoint ...
[33mEpoch 9/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00092
Train ...
Epoch: [9][ 0/61]	Loss 1.1145e+00 (1.1145e+00)
Epoch: [9][25/61]	Loss 1.1025e+00 (1.1042e+00)
Epoch: [9][50/61]	Loss 1.0644e+00 (1.0845e+00)
Checkpoint ...
[33mEpoch 10/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00090
Train ...
Epoch: [10][ 0/61]	Loss 1.0952e+00 (1.0952e+00)
Epoch: [10][25/61]	Loss 1.0321e+00 (1.0473e+00)
Epoch: [10][50/61]	Loss 1.0423e+00 (1.0525e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 89.18
Checkpoint ...
[33mEpoch 11/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00089
Train ...
Epoch: [11][ 0/61]	Loss 1.1096e+00 (1.1096e+00)
Epoch: [11][25/61]	Loss 1.0190e+00 (1.0326e+00)
Epoch: [11][50/61]	Loss 9.5383e-01 (1.0396e+00)
Checkpoint ...
[33mEpoch 12/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00086
Train ...
Epoch: [12][ 0/61]	Loss 9.9523e-01 (9.9523e-01)
Epoch: [12][25/61]	Loss 1.0046e+00 (1.0207e+00)
Epoch: [12][50/61]	Loss 9.2473e-01 (1.0185e+00)
Checkpoint ...
[33mEpoch 13/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00084
Train ...
Epoch: [13][ 0/61]	Loss 8.9825e-01 (8.9825e-01)
Epoch: [13][25/61]	Loss 1.0193e+00 (9.9921e-01)
Epoch: [13][50/61]	Loss 9.6581e-01 (9.9782e-01)
Checkpoint ...
[33mEpoch 14/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00082
Train ...
Epoch: [14][ 0/61]	Loss 1.0368e+00 (1.0368e+00)
Epoch: [14][25/61]	Loss 9.6437e-01 (9.6964e-01)
Epoch: [14][50/61]	Loss 8.5921e-01 (9.6423e-01)
Checkpoint ...
[33mEpoch 15/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00079
Train ...
Epoch: [15][ 0/61]	Loss 9.5232e-01 (9.5232e-01)
Epoch: [15][25/61]	Loss 8.6779e-01 (9.6171e-01)
Epoch: [15][50/61]	Loss 9.0430e-01 (9.6289e-01)
Checkpoint ...
[33mEpoch 16/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00077
Train ...
Epoch: [16][ 0/61]	Loss 9.6034e-01 (9.6034e-01)
Epoch: [16][25/61]	Loss 9.0581e-01 (9.5607e-01)
Epoch: [16][50/61]	Loss 8.8041e-01 (9.5186e-01)
Checkpoint ...
[33mEpoch 17/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00074
Train ...
Epoch: [17][ 0/61]	Loss 9.2519e-01 (9.2519e-01)
Epoch: [17][25/61]	Loss 8.8060e-01 (9.1767e-01)
Epoch: [17][50/61]	Loss 9.3544e-01 (9.2633e-01)
Checkpoint ...
[33mEpoch 18/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00071
Train ...
Epoch: [18][ 0/61]	Loss 8.5467e-01 (8.5467e-01)
Epoch: [18][25/61]	Loss 8.6250e-01 (9.2287e-01)
Epoch: [18][50/61]	Loss 9.6492e-01 (9.1879e-01)
Checkpoint ...
[33mEpoch 19/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00068
Train ...
Epoch: [19][ 0/61]	Loss 1.0117e+00 (1.0117e+00)
Epoch: [19][25/61]	Loss 9.3438e-01 (9.1395e-01)
Epoch: [19][50/61]	Loss 9.4993e-01 (9.1163e-01)
Checkpoint ...
[33mEpoch 20/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00065
Train ...
Epoch: [20][ 0/61]	Loss 1.0244e+00 (1.0244e+00)
Epoch: [20][25/61]	Loss 8.8102e-01 (8.9926e-01)
Epoch: [20][50/61]	Loss 8.7736e-01 (8.9677e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 89.78
Checkpoint ...
[33mEpoch 21/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00062
Train ...
Epoch: [21][ 0/61]	Loss 8.4982e-01 (8.4982e-01)
Epoch: [21][25/61]	Loss 8.4591e-01 (8.7953e-01)
Epoch: [21][50/61]	Loss 8.8116e-01 (8.8869e-01)
Checkpoint ...
[33mEpoch 22/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00059
Train ...
Epoch: [22][ 0/61]	Loss 9.2713e-01 (9.2713e-01)
Epoch: [22][25/61]	Loss 8.6279e-01 (8.7460e-01)
Epoch: [22][50/61]	Loss 9.0140e-01 (8.8116e-01)
Checkpoint ...
[33mEpoch 23/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00056
Train ...
Epoch: [23][ 0/61]	Loss 8.4503e-01 (8.4503e-01)
Epoch: [23][25/61]	Loss 8.3286e-01 (8.6207e-01)
Epoch: [23][50/61]	Loss 8.5701e-01 (8.5644e-01)
Checkpoint ...
[33mEpoch 24/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00053
Train ...
Epoch: [24][ 0/61]	Loss 1.0147e+00 (1.0147e+00)
Epoch: [24][25/61]	Loss 9.0621e-01 (8.9124e-01)
Epoch: [24][50/61]	Loss 8.2508e-01 (8.7880e-01)
Checkpoint ...
[33mEpoch 25/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00050
Train ...
Epoch: [25][ 0/61]	Loss 8.5790e-01 (8.5790e-01)
Epoch: [25][25/61]	Loss 8.4369e-01 (8.7211e-01)
Epoch: [25][50/61]	Loss 8.0364e-01 (8.6806e-01)
Checkpoint ...
[33mEpoch 26/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00047
Train ...
Epoch: [26][ 0/61]	Loss 8.7915e-01 (8.7915e-01)
Epoch: [26][25/61]	Loss 8.3154e-01 (8.7286e-01)
Epoch: [26][50/61]	Loss 8.5627e-01 (8.6735e-01)
Checkpoint ...
[33mEpoch 27/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00044
Train ...
Epoch: [27][ 0/61]	Loss 9.3438e-01 (9.3438e-01)
Epoch: [27][25/61]	Loss 8.9757e-01 (8.5698e-01)
Epoch: [27][50/61]	Loss 8.0672e-01 (8.5018e-01)
Checkpoint ...
[33mEpoch 28/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00041
Train ...
Epoch: [28][ 0/61]	Loss 7.7269e-01 (7.7269e-01)
Epoch: [28][25/61]	Loss 9.2319e-01 (8.6015e-01)
Epoch: [28][50/61]	Loss 8.6551e-01 (8.6168e-01)
Checkpoint ...
[33mEpoch 29/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00038
Train ...
Epoch: [29][ 0/61]	Loss 8.5341e-01 (8.5341e-01)
Epoch: [29][25/61]	Loss 7.9028e-01 (8.2077e-01)
Epoch: [29][50/61]	Loss 8.0891e-01 (8.3203e-01)
Checkpoint ...
[33mEpoch 30/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00035
Train ...
Epoch: [30][ 0/61]	Loss 8.2297e-01 (8.2297e-01)
Epoch: [30][25/61]	Loss 8.3353e-01 (8.4715e-01)
Epoch: [30][50/61]	Loss 8.5161e-01 (8.3535e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 90.24
Checkpoint ...
[33mEpoch 31/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00032
Train ...
Epoch: [31][ 0/61]	Loss 8.2528e-01 (8.2528e-01)
Epoch: [31][25/61]	Loss 8.2907e-01 (8.4636e-01)
Epoch: [31][50/61]	Loss 8.0578e-01 (8.3846e-01)
Checkpoint ...
[33mEpoch 32/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00029
Train ...
Epoch: [32][ 0/61]	Loss 8.0975e-01 (8.0975e-01)
Epoch: [32][25/61]	Loss 8.4611e-01 (8.2370e-01)
Epoch: [32][50/61]	Loss 9.9456e-01 (8.3115e-01)
Checkpoint ...
[33mEpoch 33/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00026
Train ...
Epoch: [33][ 0/61]	Loss 8.0247e-01 (8.0247e-01)
Epoch: [33][25/61]	Loss 8.1282e-01 (8.3187e-01)
Epoch: [33][50/61]	Loss 8.5351e-01 (8.2919e-01)
Checkpoint ...
[33mEpoch 34/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00023
Train ...
Epoch: [34][ 0/61]	Loss 8.7266e-01 (8.7266e-01)
Epoch: [34][25/61]	Loss 7.8462e-01 (8.2948e-01)
Epoch: [34][50/61]	Loss 7.2867e-01 (8.2255e-01)
Checkpoint ...
[33mEpoch 35/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00021
Train ...
Epoch: [35][ 0/61]	Loss 7.5037e-01 (7.5037e-01)
Epoch: [35][25/61]	Loss 8.2179e-01 (8.1760e-01)
Epoch: [35][50/61]	Loss 8.5449e-01 (8.1853e-01)
Checkpoint ...
[33mEpoch 36/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00018
Train ...
Epoch: [36][ 0/61]	Loss 7.9054e-01 (7.9054e-01)
Epoch: [36][25/61]	Loss 8.0173e-01 (8.1788e-01)
Epoch: [36][50/61]	Loss 8.1851e-01 (8.1610e-01)
Checkpoint ...
[33mEpoch 37/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00016
Train ...
Epoch: [37][ 0/61]	Loss 8.3196e-01 (8.3196e-01)
Epoch: [37][25/61]	Loss 7.0661e-01 (8.0440e-01)
Epoch: [37][50/61]	Loss 8.3180e-01 (8.1469e-01)
Checkpoint ...
[33mEpoch 38/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00014
Train ...
Epoch: [38][ 0/61]	Loss 6.8487e-01 (6.8487e-01)
Epoch: [38][25/61]	Loss 8.0973e-01 (8.0967e-01)
Epoch: [38][50/61]	Loss 6.9760e-01 (8.0789e-01)
Checkpoint ...
[33mEpoch 39/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00012
Train ...
Epoch: [39][ 0/61]	Loss 8.9791e-01 (8.9791e-01)
Epoch: [39][25/61]	Loss 8.2112e-01 (8.2183e-01)
Epoch: [39][50/61]	Loss 8.2115e-01 (8.1448e-01)
Checkpoint ...
[33mEpoch 40/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [40][ 0/61]	Loss 7.3610e-01 (7.3610e-01)
Epoch: [40][25/61]	Loss 7.6212e-01 (7.9852e-01)
Epoch: [40][50/61]	Loss 7.9376e-01 (8.0752e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 90.64
Checkpoint ...
[33mEpoch 41/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00008
Train ...
Epoch: [41][ 0/61]	Loss 8.4250e-01 (8.4250e-01)
Epoch: [41][25/61]	Loss 7.7430e-01 (8.3285e-01)
Epoch: [41][50/61]	Loss 7.6830e-01 (8.2450e-01)
Checkpoint ...
[33mEpoch 42/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00006
Train ...
Epoch: [42][ 0/61]	Loss 7.9668e-01 (7.9668e-01)
Epoch: [42][25/61]	Loss 8.5349e-01 (8.1160e-01)
Epoch: [42][50/61]	Loss 7.9558e-01 (8.1103e-01)
Checkpoint ...
[33mEpoch 43/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00005
Train ...
Epoch: [43][ 0/61]	Loss 7.6848e-01 (7.6848e-01)
Epoch: [43][25/61]	Loss 8.6693e-01 (8.0572e-01)
Epoch: [43][50/61]	Loss 7.7390e-01 (8.1732e-01)
Checkpoint ...
[33mEpoch 44/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00004
Train ...
Epoch: [44][ 0/61]	Loss 8.5306e-01 (8.5306e-01)
Epoch: [44][25/61]	Loss 8.7109e-01 (8.0448e-01)
Epoch: [44][50/61]	Loss 8.2259e-01 (8.0294e-01)
Checkpoint ...
[33mEpoch 45/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00003
Train ...
Epoch: [45][ 0/61]	Loss 9.0499e-01 (9.0499e-01)
Epoch: [45][25/61]	Loss 7.2203e-01 (7.7647e-01)
Epoch: [45][50/61]	Loss 7.7475e-01 (7.9659e-01)
Checkpoint ...
[33mEpoch 46/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00002
Train ...
Epoch: [46][ 0/61]	Loss 8.2493e-01 (8.2493e-01)
Epoch: [46][25/61]	Loss 7.4560e-01 (8.1946e-01)
Epoch: [46][50/61]	Loss 7.4740e-01 (8.1204e-01)
Checkpoint ...
[33mEpoch 47/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00001
Train ...
Epoch: [47][ 0/61]	Loss 6.9363e-01 (6.9363e-01)
Epoch: [47][25/61]	Loss 7.7475e-01 (8.0456e-01)
Epoch: [47][50/61]	Loss 7.8986e-01 (8.0331e-01)
Checkpoint ...
[33mEpoch 48/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00000
Train ...
Epoch: [48][ 0/61]	Loss 8.1228e-01 (8.1228e-01)
Epoch: [48][25/61]	Loss 8.5231e-01 (8.0797e-01)
Epoch: [48][50/61]	Loss 7.7214e-01 (8.0257e-01)
Checkpoint ...
[33mEpoch 49/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00000
Train ...
Epoch: [49][ 0/61]	Loss 8.7509e-01 (8.7509e-01)
Epoch: [49][25/61]	Loss 7.6034e-01 (8.0731e-01)
Epoch: [49][50/61]	Loss 8.6040e-01 (8.1385e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 90.59
Checkpoint ...
[34mFill memory bank for mining the nearest neighbors (train) ...[0m
Fill Memory Bank [0/62]
Mine the nearest neighbors (Top-20)
Accuracy of top-20 nearest neighbors on train set is 36.23
[34mFill memory bank for mining the nearest neighbors (val) ...[0m
Fill Memory Bank [0/62]
Mine the nearest neighbors (Top-5)
Accuracy of top-5 nearest neighbors on val set is 40.08
