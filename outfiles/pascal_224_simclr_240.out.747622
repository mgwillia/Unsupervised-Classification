vulcan04.umiacs.umd.edu
[31m{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-224-240', 'val_db_name': 'pascal-pretrained-224-240', 'num_classes': 240, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 64, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 224, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 224, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/topk-val-neighbors.npy'}[0m
[34mRetrieve model[0m
{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-224-240', 'val_db_name': 'pascal-pretrained-224-240', 'num_classes': 240, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 64, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 224, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 224, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/topk-val-neighbors.npy'}
loading pretrained
Model is ContrastiveModel
Model parameters: 30.02M
ContrastiveModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (contrastive_head): Sequential(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU()
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[34mSet CuDNN benchmark[0m
[34mRetrieve dataset[0m
Train transforms: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)
    RandomHorizontalFlip(p=0.5)
    RandomApply(
    p=0.8
    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[-0.1, 0.1])
)
    RandomGrayscale(p=0.2)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Validation transforms: Compose(
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Dataset contains 15774/15774 train/val samples
[34mBuild MemoryBank[0m
[34mRetrieve criterion[0m
Criterion is SimCLRLoss
[34mRetrieve optimizer[0m
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.001
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
[34mNo checkpoint file at /cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/checkpoint.pth.tar[0m
[34mStarting main loop[0m
[33mEpoch 0/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [0][  0/246]	Loss 2.7548e+00 (2.7548e+00)
Epoch: [0][ 25/246]	Loss 8.8934e-01 (1.2471e+00)
Epoch: [0][ 50/246]	Loss 5.6203e-01 (1.0146e+00)
Epoch: [0][ 75/246]	Loss 8.2918e-01 (8.9363e-01)
Epoch: [0][100/246]	Loss 6.4534e-01 (8.3867e-01)
Epoch: [0][125/246]	Loss 6.5883e-01 (7.8942e-01)
Epoch: [0][150/246]	Loss 5.7953e-01 (7.5173e-01)
Epoch: [0][175/246]	Loss 5.7952e-01 (7.2384e-01)
Epoch: [0][200/246]	Loss 5.7934e-01 (6.9752e-01)
Epoch: [0][225/246]	Loss 5.7858e-01 (6.7914e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 93.57
Checkpoint ...
[33mEpoch 1/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [1][  0/246]	Loss 4.3702e-01 (4.3702e-01)
Epoch: [1][ 25/246]	Loss 5.8385e-01 (4.6540e-01)
Epoch: [1][ 50/246]	Loss 5.9103e-01 (4.6693e-01)
Epoch: [1][ 75/246]	Loss 5.6232e-01 (4.6413e-01)
Epoch: [1][100/246]	Loss 4.5421e-01 (4.5420e-01)
Epoch: [1][125/246]	Loss 4.4817e-01 (4.5070e-01)
Epoch: [1][150/246]	Loss 3.9171e-01 (4.4599e-01)
Epoch: [1][175/246]	Loss 4.6811e-01 (4.3963e-01)
Epoch: [1][200/246]	Loss 4.7648e-01 (4.3469e-01)
Epoch: [1][225/246]	Loss 3.4302e-01 (4.3127e-01)
Checkpoint ...
[33mEpoch 2/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [2][  0/246]	Loss 3.2969e-01 (3.2969e-01)
Epoch: [2][ 25/246]	Loss 3.9485e-01 (3.9165e-01)
Epoch: [2][ 50/246]	Loss 4.6476e-01 (3.9441e-01)
Epoch: [2][ 75/246]	Loss 3.8788e-01 (3.9491e-01)
Epoch: [2][100/246]	Loss 3.9133e-01 (3.8880e-01)
Epoch: [2][125/246]	Loss 2.8543e-01 (3.8034e-01)
Epoch: [2][150/246]	Loss 3.1313e-01 (3.7101e-01)
Epoch: [2][175/246]	Loss 3.6735e-01 (3.6840e-01)
Epoch: [2][200/246]	Loss 2.2430e-01 (3.6707e-01)
Epoch: [2][225/246]	Loss 3.1018e-01 (3.6470e-01)
Checkpoint ...
[33mEpoch 3/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00099
Train ...
Epoch: [3][  0/246]	Loss 2.1810e-01 (2.1810e-01)
Epoch: [3][ 25/246]	Loss 4.1409e-01 (3.4211e-01)
Epoch: [3][ 50/246]	Loss 4.1514e-01 (3.3301e-01)
Epoch: [3][ 75/246]	Loss 3.2903e-01 (3.3273e-01)
Epoch: [3][100/246]	Loss 2.8666e-01 (3.2741e-01)
Epoch: [3][125/246]	Loss 2.5105e-01 (3.2371e-01)
Epoch: [3][150/246]	Loss 2.9979e-01 (3.2669e-01)
Epoch: [3][175/246]	Loss 3.5541e-01 (3.2649e-01)
Epoch: [3][200/246]	Loss 2.3559e-01 (3.2228e-01)
Epoch: [3][225/246]	Loss 3.8124e-01 (3.2158e-01)
Checkpoint ...
[33mEpoch 4/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [4][  0/246]	Loss 2.4664e-01 (2.4664e-01)
Epoch: [4][ 25/246]	Loss 2.4891e-01 (3.0686e-01)
Epoch: [4][ 50/246]	Loss 2.6941e-01 (3.1956e-01)
Epoch: [4][ 75/246]	Loss 2.1689e-01 (3.1264e-01)
Epoch: [4][100/246]	Loss 3.2790e-01 (3.0727e-01)
Epoch: [4][125/246]	Loss 2.6744e-01 (3.0328e-01)
Epoch: [4][150/246]	Loss 2.3036e-01 (3.0230e-01)
Epoch: [4][175/246]	Loss 4.0494e-01 (3.0264e-01)
Epoch: [4][200/246]	Loss 2.4486e-01 (3.0189e-01)
Epoch: [4][225/246]	Loss 2.8799e-01 (3.0241e-01)
Checkpoint ...
[33mEpoch 5/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [5][  0/246]	Loss 2.8666e-01 (2.8666e-01)
Epoch: [5][ 25/246]	Loss 1.9826e-01 (2.6375e-01)
Epoch: [5][ 50/246]	Loss 2.0381e-01 (2.7168e-01)
Epoch: [5][ 75/246]	Loss 2.9899e-01 (2.8095e-01)
Epoch: [5][100/246]	Loss 2.6134e-01 (2.7695e-01)
Epoch: [5][125/246]	Loss 2.7558e-01 (2.7905e-01)
Epoch: [5][150/246]	Loss 2.4582e-01 (2.8282e-01)
Epoch: [5][175/246]	Loss 2.2368e-01 (2.8538e-01)
Epoch: [5][200/246]	Loss 2.6251e-01 (2.8344e-01)
Epoch: [5][225/246]	Loss 2.4814e-01 (2.8386e-01)
Checkpoint ...
[33mEpoch 6/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00096
Train ...
Epoch: [6][  0/246]	Loss 3.0231e-01 (3.0231e-01)
Epoch: [6][ 25/246]	Loss 2.9763e-01 (2.6527e-01)
Epoch: [6][ 50/246]	Loss 2.8486e-01 (2.7395e-01)
Epoch: [6][ 75/246]	Loss 3.2723e-01 (2.7895e-01)
Epoch: [6][100/246]	Loss 3.0938e-01 (2.7310e-01)
Epoch: [6][125/246]	Loss 3.6152e-01 (2.7310e-01)
Epoch: [6][150/246]	Loss 1.7783e-01 (2.6934e-01)
Epoch: [6][175/246]	Loss 3.8433e-01 (2.6808e-01)
Epoch: [6][200/246]	Loss 2.5491e-01 (2.6824e-01)
Epoch: [6][225/246]	Loss 3.7613e-01 (2.6564e-01)
Checkpoint ...
[33mEpoch 7/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00095
Train ...
Epoch: [7][  0/246]	Loss 2.0083e-01 (2.0083e-01)
Epoch: [7][ 25/246]	Loss 3.4480e-01 (2.4862e-01)
Epoch: [7][ 50/246]	Loss 1.7749e-01 (2.5877e-01)
Epoch: [7][ 75/246]	Loss 2.1915e-01 (2.5800e-01)
Epoch: [7][100/246]	Loss 3.2439e-01 (2.6142e-01)
Epoch: [7][125/246]	Loss 2.5011e-01 (2.5874e-01)
Epoch: [7][150/246]	Loss 3.3950e-01 (2.6004e-01)
Epoch: [7][175/246]	Loss 2.2438e-01 (2.5901e-01)
Epoch: [7][200/246]	Loss 2.8045e-01 (2.5983e-01)
Epoch: [7][225/246]	Loss 3.2176e-01 (2.5740e-01)
Checkpoint ...
[33mEpoch 8/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00094
Train ...
Epoch: [8][  0/246]	Loss 1.8889e-01 (1.8889e-01)
Epoch: [8][ 25/246]	Loss 1.8441e-01 (2.6334e-01)
Epoch: [8][ 50/246]	Loss 2.5317e-01 (2.5724e-01)
Epoch: [8][ 75/246]	Loss 2.2945e-01 (2.5208e-01)
Epoch: [8][100/246]	Loss 2.2843e-01 (2.5064e-01)
Epoch: [8][125/246]	Loss 2.6933e-01 (2.5218e-01)
Epoch: [8][150/246]	Loss 2.3868e-01 (2.5133e-01)
Epoch: [8][175/246]	Loss 2.8006e-01 (2.4970e-01)
Epoch: [8][200/246]	Loss 2.6411e-01 (2.4927e-01)
Epoch: [8][225/246]	Loss 1.6823e-01 (2.4735e-01)
Checkpoint ...
[33mEpoch 9/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00092
Train ...
Epoch: [9][  0/246]	Loss 2.2024e-01 (2.2024e-01)
Epoch: [9][ 25/246]	Loss 1.8071e-01 (2.4321e-01)
Epoch: [9][ 50/246]	Loss 2.2252e-01 (2.4838e-01)
Epoch: [9][ 75/246]	Loss 3.9179e-01 (2.4371e-01)
Epoch: [9][100/246]	Loss 2.4903e-01 (2.4184e-01)
Epoch: [9][125/246]	Loss 1.8506e-01 (2.3690e-01)
Epoch: [9][150/246]	Loss 2.7118e-01 (2.3430e-01)
Epoch: [9][175/246]	Loss 1.7778e-01 (2.3608e-01)
Epoch: [9][200/246]	Loss 2.2609e-01 (2.3695e-01)
Epoch: [9][225/246]	Loss 2.8439e-01 (2.3761e-01)
Checkpoint ...
[33mEpoch 10/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00090
Train ...
Epoch: [10][  0/246]	Loss 2.0149e-01 (2.0149e-01)
Epoch: [10][ 25/246]	Loss 2.7111e-01 (2.2851e-01)
Epoch: [10][ 50/246]	Loss 2.9141e-01 (2.3217e-01)
Epoch: [10][ 75/246]	Loss 2.1792e-01 (2.3083e-01)
Epoch: [10][100/246]	Loss 3.5405e-01 (2.3034e-01)
Epoch: [10][125/246]	Loss 1.7090e-01 (2.3035e-01)
Epoch: [10][150/246]	Loss 2.1464e-01 (2.2911e-01)
Epoch: [10][175/246]	Loss 3.6619e-01 (2.2941e-01)
Epoch: [10][200/246]	Loss 3.4915e-01 (2.3084e-01)
Epoch: [10][225/246]	Loss 2.6868e-01 (2.2981e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 96.21
Checkpoint ...
[33mEpoch 11/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00089
Train ...
Epoch: [11][  0/246]	Loss 2.3102e-01 (2.3102e-01)
Epoch: [11][ 25/246]	Loss 3.0797e-01 (2.3238e-01)
Epoch: [11][ 50/246]	Loss 1.8820e-01 (2.3512e-01)
Epoch: [11][ 75/246]	Loss 2.0758e-01 (2.2858e-01)
Epoch: [11][100/246]	Loss 3.0778e-01 (2.2676e-01)
Epoch: [11][125/246]	Loss 2.4223e-01 (2.2759e-01)
Epoch: [11][150/246]	Loss 1.4003e-01 (2.2990e-01)
Epoch: [11][175/246]	Loss 1.5625e-01 (2.2841e-01)
Epoch: [11][200/246]	Loss 2.2139e-01 (2.2793e-01)
Epoch: [11][225/246]	Loss 2.2627e-01 (2.2561e-01)
Checkpoint ...
[33mEpoch 12/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00086
Train ...
Epoch: [12][  0/246]	Loss 1.2032e-01 (1.2032e-01)
Epoch: [12][ 25/246]	Loss 1.8912e-01 (2.2073e-01)
Epoch: [12][ 50/246]	Loss 2.8311e-01 (2.2502e-01)
Epoch: [12][ 75/246]	Loss 2.7717e-01 (2.2007e-01)
Epoch: [12][100/246]	Loss 2.2586e-01 (2.2050e-01)
Epoch: [12][125/246]	Loss 2.0805e-01 (2.1675e-01)
Epoch: [12][150/246]	Loss 2.5993e-01 (2.1523e-01)
Epoch: [12][175/246]	Loss 3.2189e-01 (2.1826e-01)
Epoch: [12][200/246]	Loss 1.3351e-01 (2.2059e-01)
Epoch: [12][225/246]	Loss 2.4256e-01 (2.1941e-01)
Checkpoint ...
[33mEpoch 13/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00084
Train ...
Epoch: [13][  0/246]	Loss 1.8775e-01 (1.8775e-01)
Epoch: [13][ 25/246]	Loss 1.9042e-01 (2.2314e-01)
Epoch: [13][ 50/246]	Loss 1.9600e-01 (2.2356e-01)
Epoch: [13][ 75/246]	Loss 2.2403e-01 (2.1797e-01)
Epoch: [13][100/246]	Loss 1.9240e-01 (2.1412e-01)
Epoch: [13][125/246]	Loss 1.6650e-01 (2.1386e-01)
Epoch: [13][150/246]	Loss 3.5468e-01 (2.1406e-01)
Epoch: [13][175/246]	Loss 1.8557e-01 (2.1441e-01)
Epoch: [13][200/246]	Loss 3.1133e-01 (2.1597e-01)
Epoch: [13][225/246]	Loss 1.5663e-01 (2.1570e-01)
Checkpoint ...
[33mEpoch 14/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00082
Train ...
Epoch: [14][  0/246]	Loss 2.0995e-01 (2.0995e-01)
Epoch: [14][ 25/246]	Loss 1.6462e-01 (1.9600e-01)
Epoch: [14][ 50/246]	Loss 2.3325e-01 (2.0473e-01)
Epoch: [14][ 75/246]	Loss 2.2956e-01 (2.0519e-01)
Epoch: [14][100/246]	Loss 1.6706e-01 (2.0750e-01)
Epoch: [14][125/246]	Loss 1.9144e-01 (2.0501e-01)
Epoch: [14][150/246]	Loss 1.7997e-01 (2.0880e-01)
Epoch: [14][175/246]	Loss 1.7097e-01 (2.0973e-01)
Epoch: [14][200/246]	Loss 1.7193e-01 (2.0986e-01)
Epoch: [14][225/246]	Loss 2.3011e-01 (2.0994e-01)
Checkpoint ...
[33mEpoch 15/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00079
Train ...
Epoch: [15][  0/246]	Loss 1.4735e-01 (1.4735e-01)
Epoch: [15][ 25/246]	Loss 2.9188e-01 (2.2392e-01)
Epoch: [15][ 50/246]	Loss 1.9157e-01 (2.1346e-01)
Epoch: [15][ 75/246]	Loss 1.2615e-01 (2.1492e-01)
Epoch: [15][100/246]	Loss 2.1908e-01 (2.1090e-01)
Epoch: [15][125/246]	Loss 2.1106e-01 (2.0845e-01)
Epoch: [15][150/246]	Loss 2.2234e-01 (2.0441e-01)
Epoch: [15][175/246]	Loss 1.9278e-01 (2.0364e-01)
Epoch: [15][200/246]	Loss 2.4857e-01 (2.0197e-01)
Epoch: [15][225/246]	Loss 2.2832e-01 (2.0178e-01)
Checkpoint ...
[33mEpoch 16/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00077
Train ...
Epoch: [16][  0/246]	Loss 1.5143e-01 (1.5143e-01)
Epoch: [16][ 25/246]	Loss 2.1031e-01 (1.8781e-01)
Epoch: [16][ 50/246]	Loss 2.2277e-01 (1.8324e-01)
Epoch: [16][ 75/246]	Loss 2.4138e-01 (1.9453e-01)
Epoch: [16][100/246]	Loss 1.6488e-01 (2.0033e-01)
Epoch: [16][125/246]	Loss 2.5573e-01 (2.0293e-01)
Epoch: [16][150/246]	Loss 3.2416e-01 (2.0545e-01)
Epoch: [16][175/246]	Loss 2.1220e-01 (2.0422e-01)
Epoch: [16][200/246]	Loss 1.4076e-01 (2.0219e-01)
Epoch: [16][225/246]	Loss 2.0279e-01 (2.0283e-01)
Checkpoint ...
[33mEpoch 17/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00074
Train ...
Epoch: [17][  0/246]	Loss 2.0183e-01 (2.0183e-01)
Epoch: [17][ 25/246]	Loss 2.2877e-01 (2.0208e-01)
Epoch: [17][ 50/246]	Loss 1.4364e-01 (1.9990e-01)
Epoch: [17][ 75/246]	Loss 1.4693e-01 (2.0037e-01)
Epoch: [17][100/246]	Loss 1.7088e-01 (1.9767e-01)
Epoch: [17][125/246]	Loss 1.5712e-01 (1.9617e-01)
Epoch: [17][150/246]	Loss 1.9801e-01 (1.9389e-01)
Epoch: [17][175/246]	Loss 2.5744e-01 (1.9599e-01)
Epoch: [17][200/246]	Loss 1.3515e-01 (1.9369e-01)
Epoch: [17][225/246]	Loss 1.7097e-01 (1.9293e-01)
Checkpoint ...
[33mEpoch 18/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00071
Train ...
Epoch: [18][  0/246]	Loss 3.1101e-01 (3.1101e-01)
Epoch: [18][ 25/246]	Loss 1.5438e-01 (2.0070e-01)
Epoch: [18][ 50/246]	Loss 1.8549e-01 (1.9943e-01)
Epoch: [18][ 75/246]	Loss 1.6501e-01 (1.9483e-01)
Epoch: [18][100/246]	Loss 1.3594e-01 (1.9340e-01)
Epoch: [18][125/246]	Loss 1.5592e-01 (1.9410e-01)
Epoch: [18][150/246]	Loss 2.1796e-01 (1.9357e-01)
Epoch: [18][175/246]	Loss 2.3650e-01 (1.9180e-01)
Epoch: [18][200/246]	Loss 2.0867e-01 (1.9207e-01)
Epoch: [18][225/246]	Loss 2.1405e-01 (1.9187e-01)
Checkpoint ...
[33mEpoch 19/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00068
Train ...
Epoch: [19][  0/246]	Loss 2.7069e-01 (2.7069e-01)
Epoch: [19][ 25/246]	Loss 1.7444e-01 (1.8909e-01)
Epoch: [19][ 50/246]	Loss 1.2711e-01 (1.9311e-01)
Epoch: [19][ 75/246]	Loss 2.7743e-01 (1.9159e-01)
Epoch: [19][100/246]	Loss 1.5169e-01 (1.9005e-01)
Epoch: [19][125/246]	Loss 1.4173e-01 (1.8939e-01)
Epoch: [19][150/246]	Loss 2.1820e-01 (1.8824e-01)
Epoch: [19][175/246]	Loss 1.7316e-01 (1.8839e-01)
Epoch: [19][200/246]	Loss 2.2599e-01 (1.8971e-01)
Epoch: [19][225/246]	Loss 2.1987e-01 (1.9152e-01)
Checkpoint ...
[33mEpoch 20/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00065
Train ...
Epoch: [20][  0/246]	Loss 2.0004e-01 (2.0004e-01)
Epoch: [20][ 25/246]	Loss 1.7754e-01 (1.9038e-01)
Epoch: [20][ 50/246]	Loss 2.3867e-01 (1.8694e-01)
Epoch: [20][ 75/246]	Loss 1.1974e-01 (1.8428e-01)
Epoch: [20][100/246]	Loss 1.9837e-01 (1.8470e-01)
Epoch: [20][125/246]	Loss 2.8758e-01 (1.8614e-01)
Epoch: [20][150/246]	Loss 1.7006e-01 (1.8620e-01)
Epoch: [20][175/246]	Loss 2.6671e-01 (1.8602e-01)
Epoch: [20][200/246]	Loss 2.0101e-01 (1.8722e-01)
Epoch: [20][225/246]	Loss 2.3076e-01 (1.8610e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 97.19
Checkpoint ...
[33mEpoch 21/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00062
Train ...
Epoch: [21][  0/246]	Loss 2.1148e-01 (2.1148e-01)
Epoch: [21][ 25/246]	Loss 1.9031e-01 (1.9328e-01)
Epoch: [21][ 50/246]	Loss 1.7155e-01 (1.8996e-01)
Epoch: [21][ 75/246]	Loss 1.5485e-01 (1.8768e-01)
Epoch: [21][100/246]	Loss 2.7127e-01 (1.8789e-01)
Epoch: [21][125/246]	Loss 2.2944e-01 (1.8930e-01)
Epoch: [21][150/246]	Loss 1.5372e-01 (1.8966e-01)
Epoch: [21][175/246]	Loss 1.4818e-01 (1.9202e-01)
Epoch: [21][200/246]	Loss 1.4222e-01 (1.9314e-01)
Epoch: [21][225/246]	Loss 1.0151e-01 (1.9380e-01)
Checkpoint ...
[33mEpoch 22/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00059
Train ...
Epoch: [22][  0/246]	Loss 1.6273e-01 (1.6273e-01)
Epoch: [22][ 25/246]	Loss 2.6057e-01 (1.8249e-01)
Epoch: [22][ 50/246]	Loss 3.0844e-01 (1.9242e-01)
Epoch: [22][ 75/246]	Loss 1.3671e-01 (1.9241e-01)
Epoch: [22][100/246]	Loss 1.4473e-01 (1.8916e-01)
Epoch: [22][125/246]	Loss 1.6456e-01 (1.8784e-01)
Epoch: [22][150/246]	Loss 1.1213e-01 (1.8628e-01)
Epoch: [22][175/246]	Loss 1.5605e-01 (1.8847e-01)
Epoch: [22][200/246]	Loss 1.1433e-01 (1.8587e-01)
Epoch: [22][225/246]	Loss 1.5193e-01 (1.8405e-01)
Checkpoint ...
[33mEpoch 23/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00056
Train ...
Epoch: [23][  0/246]	Loss 1.4739e-01 (1.4739e-01)
Epoch: [23][ 25/246]	Loss 1.9744e-01 (1.7254e-01)
Epoch: [23][ 50/246]	Loss 1.5539e-01 (1.7909e-01)
Epoch: [23][ 75/246]	Loss 2.2432e-01 (1.7877e-01)
Epoch: [23][100/246]	Loss 2.1025e-01 (1.7643e-01)
Epoch: [23][125/246]	Loss 1.5404e-01 (1.7708e-01)
Epoch: [23][150/246]	Loss 1.9960e-01 (1.7495e-01)
Epoch: [23][175/246]	Loss 1.4055e-01 (1.7595e-01)
Epoch: [23][200/246]	Loss 1.7423e-01 (1.7818e-01)
Epoch: [23][225/246]	Loss 1.3879e-01 (1.7716e-01)
Checkpoint ...
[33mEpoch 24/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00053
Train ...
Epoch: [24][  0/246]	Loss 2.2523e-01 (2.2523e-01)
Epoch: [24][ 25/246]	Loss 3.0406e-01 (1.8991e-01)
Epoch: [24][ 50/246]	Loss 1.3844e-01 (1.7172e-01)
Epoch: [24][ 75/246]	Loss 1.4008e-01 (1.7696e-01)
Epoch: [24][100/246]	Loss 2.1702e-01 (1.8342e-01)
Epoch: [24][125/246]	Loss 1.3976e-01 (1.7926e-01)
Epoch: [24][150/246]	Loss 2.2304e-01 (1.7973e-01)
Epoch: [24][175/246]	Loss 1.8963e-01 (1.7927e-01)
Epoch: [24][200/246]	Loss 1.5434e-01 (1.7924e-01)
Epoch: [24][225/246]	Loss 1.6999e-01 (1.7836e-01)
Checkpoint ...
[33mEpoch 25/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00050
Train ...
Epoch: [25][  0/246]	Loss 1.5282e-01 (1.5282e-01)
Epoch: [25][ 25/246]	Loss 1.7663e-01 (1.7853e-01)
Epoch: [25][ 50/246]	Loss 1.4968e-01 (1.8353e-01)
Epoch: [25][ 75/246]	Loss 1.7420e-01 (1.7998e-01)
Epoch: [25][100/246]	Loss 1.5704e-01 (1.7759e-01)
Epoch: [25][125/246]	Loss 1.7001e-01 (1.7521e-01)
Epoch: [25][150/246]	Loss 2.1521e-01 (1.7268e-01)
Epoch: [25][175/246]	Loss 1.3230e-01 (1.7376e-01)
Epoch: [25][200/246]	Loss 1.6409e-01 (1.7383e-01)
Epoch: [25][225/246]	Loss 1.6219e-01 (1.7497e-01)
Checkpoint ...
[33mEpoch 26/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00047
Train ...
Epoch: [26][  0/246]	Loss 1.5571e-01 (1.5571e-01)
Epoch: [26][ 25/246]	Loss 2.0648e-01 (1.8775e-01)
Epoch: [26][ 50/246]	Loss 1.3637e-01 (1.8538e-01)
Epoch: [26][ 75/246]	Loss 1.3888e-01 (1.7741e-01)
Epoch: [26][100/246]	Loss 2.1711e-01 (1.7827e-01)
Epoch: [26][125/246]	Loss 1.5271e-01 (1.7824e-01)
Epoch: [26][150/246]	Loss 3.2780e-01 (1.7814e-01)
Epoch: [26][175/246]	Loss 1.1818e-01 (1.7632e-01)
Epoch: [26][200/246]	Loss 2.5157e-01 (1.7577e-01)
Epoch: [26][225/246]	Loss 1.5197e-01 (1.7495e-01)
Checkpoint ...
[33mEpoch 27/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00044
Train ...
Epoch: [27][  0/246]	Loss 1.1571e-01 (1.1571e-01)
Epoch: [27][ 25/246]	Loss 1.8864e-01 (1.6196e-01)
Epoch: [27][ 50/246]	Loss 3.0302e-01 (1.7205e-01)
Epoch: [27][ 75/246]	Loss 3.3693e-01 (1.7934e-01)
Epoch: [27][100/246]	Loss 2.2443e-01 (1.7686e-01)
Epoch: [27][125/246]	Loss 2.2963e-01 (1.7523e-01)
Epoch: [27][150/246]	Loss 1.7759e-01 (1.7788e-01)
Epoch: [27][175/246]	Loss 2.9752e-01 (1.7629e-01)
Epoch: [27][200/246]	Loss 2.0338e-01 (1.7485e-01)
Epoch: [27][225/246]	Loss 1.6453e-01 (1.7442e-01)
Checkpoint ...
[33mEpoch 28/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00041
Train ...
Epoch: [28][  0/246]	Loss 2.1212e-01 (2.1212e-01)
Epoch: [28][ 25/246]	Loss 1.7145e-01 (1.8898e-01)
Epoch: [28][ 50/246]	Loss 9.0079e-02 (1.8223e-01)
Epoch: [28][ 75/246]	Loss 1.6316e-01 (1.8101e-01)
Epoch: [28][100/246]	Loss 2.0075e-01 (1.7906e-01)
Epoch: [28][125/246]	Loss 1.9515e-01 (1.7876e-01)
Epoch: [28][150/246]	Loss 2.4804e-01 (1.7760e-01)
Epoch: [28][175/246]	Loss 1.3504e-01 (1.7763e-01)
Epoch: [28][200/246]	Loss 1.1475e-01 (1.7582e-01)
Epoch: [28][225/246]	Loss 1.2826e-01 (1.7486e-01)
Checkpoint ...
[33mEpoch 29/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00038
Train ...
Epoch: [29][  0/246]	Loss 1.6452e-01 (1.6452e-01)
Epoch: [29][ 25/246]	Loss 1.9881e-01 (1.7034e-01)
Epoch: [29][ 50/246]	Loss 1.2449e-01 (1.7088e-01)
Epoch: [29][ 75/246]	Loss 1.0640e-01 (1.6637e-01)
Epoch: [29][100/246]	Loss 1.6889e-01 (1.6877e-01)
Epoch: [29][125/246]	Loss 1.5412e-01 (1.6827e-01)
Epoch: [29][150/246]	Loss 1.7739e-01 (1.6987e-01)
Epoch: [29][175/246]	Loss 2.2210e-01 (1.7077e-01)
Epoch: [29][200/246]	Loss 1.1893e-01 (1.7079e-01)
Epoch: [29][225/246]	Loss 1.5942e-01 (1.7028e-01)
Checkpoint ...
[33mEpoch 30/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00035
Train ...
Epoch: [30][  0/246]	Loss 1.8497e-01 (1.8497e-01)
Epoch: [30][ 25/246]	Loss 1.9946e-01 (1.6826e-01)
Epoch: [30][ 50/246]	Loss 1.1484e-01 (1.6867e-01)
Epoch: [30][ 75/246]	Loss 1.2470e-01 (1.6973e-01)
Epoch: [30][100/246]	Loss 1.2356e-01 (1.7008e-01)
Epoch: [30][125/246]	Loss 1.1683e-01 (1.6895e-01)
Epoch: [30][150/246]	Loss 2.1950e-01 (1.7136e-01)
Epoch: [30][175/246]	Loss 1.2961e-01 (1.6986e-01)
Epoch: [30][200/246]	Loss 2.4724e-01 (1.7202e-01)
Epoch: [30][225/246]	Loss 2.1875e-01 (1.7199e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 97.27
Checkpoint ...
[33mEpoch 31/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00032
Train ...
Epoch: [31][  0/246]	Loss 1.2541e-01 (1.2541e-01)
Epoch: [31][ 25/246]	Loss 2.1420e-01 (1.6802e-01)
Epoch: [31][ 50/246]	Loss 1.3819e-01 (1.7225e-01)
Epoch: [31][ 75/246]	Loss 1.4036e-01 (1.7020e-01)
Epoch: [31][100/246]	Loss 1.4707e-01 (1.6997e-01)
Epoch: [31][125/246]	Loss 1.6979e-01 (1.6879e-01)
Epoch: [31][150/246]	Loss 1.2465e-01 (1.6781e-01)
Epoch: [31][175/246]	Loss 1.4228e-01 (1.6793e-01)
Epoch: [31][200/246]	Loss 2.0326e-01 (1.6937e-01)
Epoch: [31][225/246]	Loss 1.1799e-01 (1.7105e-01)
Checkpoint ...
[33mEpoch 32/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00029
Train ...
Epoch: [32][  0/246]	Loss 2.4861e-01 (2.4861e-01)
Epoch: [32][ 25/246]	Loss 1.0563e-01 (1.7268e-01)
Epoch: [32][ 50/246]	Loss 1.2985e-01 (1.6534e-01)
Epoch: [32][ 75/246]	Loss 1.1627e-01 (1.6494e-01)
Epoch: [32][100/246]	Loss 1.5445e-01 (1.6631e-01)
Epoch: [32][125/246]	Loss 1.6023e-01 (1.6659e-01)
Epoch: [32][150/246]	Loss 2.3651e-01 (1.6623e-01)
Epoch: [32][175/246]	Loss 1.9728e-01 (1.6583e-01)
Epoch: [32][200/246]	Loss 1.5009e-01 (1.6723e-01)
Epoch: [32][225/246]	Loss 2.1947e-01 (1.6718e-01)
Checkpoint ...
[33mEpoch 33/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00026
Train ...
Epoch: [33][  0/246]	Loss 1.7190e-01 (1.7190e-01)
Epoch: [33][ 25/246]	Loss 9.5348e-02 (1.5522e-01)
Epoch: [33][ 50/246]	Loss 1.2698e-01 (1.6786e-01)
Epoch: [33][ 75/246]	Loss 1.7125e-01 (1.6753e-01)
Epoch: [33][100/246]	Loss 2.1152e-01 (1.6738e-01)
Epoch: [33][125/246]	Loss 1.2577e-01 (1.6563e-01)
Epoch: [33][150/246]	Loss 1.9682e-01 (1.6661e-01)
Epoch: [33][175/246]	Loss 1.8978e-01 (1.6813e-01)
Epoch: [33][200/246]	Loss 1.9107e-01 (1.6903e-01)
Epoch: [33][225/246]	Loss 1.1038e-01 (1.7003e-01)
Checkpoint ...
[33mEpoch 34/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00023
Train ...
Epoch: [34][  0/246]	Loss 1.3765e-01 (1.3765e-01)
Epoch: [34][ 25/246]	Loss 1.7995e-01 (1.5903e-01)
Epoch: [34][ 50/246]	Loss 2.3714e-01 (1.6612e-01)
Epoch: [34][ 75/246]	Loss 1.6361e-01 (1.6673e-01)
Epoch: [34][100/246]	Loss 2.0627e-01 (1.6684e-01)
Epoch: [34][125/246]	Loss 8.7669e-02 (1.6725e-01)
Epoch: [34][150/246]	Loss 1.2547e-01 (1.6602e-01)
Epoch: [34][175/246]	Loss 1.1813e-01 (1.6671e-01)
Epoch: [34][200/246]	Loss 1.5452e-01 (1.6594e-01)
Epoch: [34][225/246]	Loss 1.5665e-01 (1.6553e-01)
Checkpoint ...
[33mEpoch 35/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00021
Train ...
Epoch: [35][  0/246]	Loss 1.7289e-01 (1.7289e-01)
Epoch: [35][ 25/246]	Loss 1.8577e-01 (1.6453e-01)
Epoch: [35][ 50/246]	Loss 1.7334e-01 (1.6178e-01)
Epoch: [35][ 75/246]	Loss 1.7831e-01 (1.6579e-01)
Epoch: [35][100/246]	Loss 1.6498e-01 (1.6434e-01)
Epoch: [35][125/246]	Loss 2.3329e-01 (1.6531e-01)
Epoch: [35][150/246]	Loss 1.0859e-01 (1.6506e-01)
Epoch: [35][175/246]	Loss 1.5568e-01 (1.6592e-01)
Epoch: [35][200/246]	Loss 1.7009e-01 (1.6586e-01)
Epoch: [35][225/246]	Loss 1.3818e-01 (1.6552e-01)
Checkpoint ...
[33mEpoch 36/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00018
Train ...
Epoch: [36][  0/246]	Loss 1.5702e-01 (1.5702e-01)
Epoch: [36][ 25/246]	Loss 2.2387e-01 (1.7348e-01)
Epoch: [36][ 50/246]	Loss 2.2489e-01 (1.6440e-01)
Epoch: [36][ 75/246]	Loss 1.8583e-01 (1.6791e-01)
Epoch: [36][100/246]	Loss 1.7558e-01 (1.6962e-01)
Epoch: [36][125/246]	Loss 1.4202e-01 (1.6972e-01)
Epoch: [36][150/246]	Loss 1.7799e-01 (1.6794e-01)
Epoch: [36][175/246]	Loss 1.5438e-01 (1.6519e-01)
Epoch: [36][200/246]	Loss 1.5064e-01 (1.6471e-01)
Epoch: [36][225/246]	Loss 1.7790e-01 (1.6414e-01)
Checkpoint ...
[33mEpoch 37/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00016
Train ...
Epoch: [37][  0/246]	Loss 1.5337e-01 (1.5337e-01)
Epoch: [37][ 25/246]	Loss 1.6804e-01 (1.6691e-01)
Epoch: [37][ 50/246]	Loss 1.4736e-01 (1.7069e-01)
Epoch: [37][ 75/246]	Loss 9.9458e-02 (1.6755e-01)
Epoch: [37][100/246]	Loss 1.7146e-01 (1.6699e-01)
Epoch: [37][125/246]	Loss 1.6163e-01 (1.6426e-01)
Epoch: [37][150/246]	Loss 1.4238e-01 (1.6406e-01)
Epoch: [37][175/246]	Loss 1.6509e-01 (1.6329e-01)
Epoch: [37][200/246]	Loss 1.8038e-01 (1.6283e-01)
Epoch: [37][225/246]	Loss 2.4066e-01 (1.6157e-01)
Checkpoint ...
[33mEpoch 38/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00014
Train ...
Epoch: [38][  0/246]	Loss 1.5524e-01 (1.5524e-01)
Epoch: [38][ 25/246]	Loss 1.6058e-01 (1.7206e-01)
Epoch: [38][ 50/246]	Loss 1.6961e-01 (1.6674e-01)
Epoch: [38][ 75/246]	Loss 1.8723e-01 (1.6095e-01)
Epoch: [38][100/246]	Loss 1.3415e-01 (1.6012e-01)
Epoch: [38][125/246]	Loss 3.2500e-01 (1.6341e-01)
Epoch: [38][150/246]	Loss 2.1108e-01 (1.6422e-01)
Epoch: [38][175/246]	Loss 1.9441e-01 (1.6557e-01)
Epoch: [38][200/246]	Loss 2.6005e-01 (1.6631e-01)
Epoch: [38][225/246]	Loss 1.7478e-01 (1.6656e-01)
Checkpoint ...
[33mEpoch 39/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00012
Train ...
Epoch: [39][  0/246]	Loss 1.3935e-01 (1.3935e-01)
Epoch: [39][ 25/246]	Loss 1.2698e-01 (1.6302e-01)
Epoch: [39][ 50/246]	Loss 1.3838e-01 (1.7252e-01)
Epoch: [39][ 75/246]	Loss 1.9091e-01 (1.6776e-01)
Epoch: [39][100/246]	Loss 1.6883e-01 (1.6707e-01)
Epoch: [39][125/246]	Loss 1.6761e-01 (1.6508e-01)
Epoch: [39][150/246]	Loss 1.7818e-01 (1.6700e-01)
Epoch: [39][175/246]	Loss 2.2582e-01 (1.6704e-01)
Epoch: [39][200/246]	Loss 1.5801e-01 (1.6563e-01)
Epoch: [39][225/246]	Loss 1.5985e-01 (1.6503e-01)
Checkpoint ...
[33mEpoch 40/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [40][  0/246]	Loss 1.4801e-01 (1.4801e-01)
Epoch: [40][ 25/246]	Loss 1.2435e-01 (1.5340e-01)
Epoch: [40][ 50/246]	Loss 1.7100e-01 (1.5358e-01)
Epoch: [40][ 75/246]	Loss 1.4770e-01 (1.5306e-01)
Epoch: [40][100/246]	Loss 1.5627e-01 (1.5703e-01)
Epoch: [40][125/246]	Loss 1.9023e-01 (1.5996e-01)
Epoch: [40][150/246]	Loss 1.9709e-01 (1.6265e-01)
Epoch: [40][175/246]	Loss 1.8683e-01 (1.6324e-01)
Epoch: [40][200/246]	Loss 1.9152e-01 (1.6388e-01)
Epoch: [40][225/246]	Loss 2.2491e-01 (1.6324e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 97.48
Checkpoint ...
[33mEpoch 41/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00008
Train ...
Epoch: [41][  0/246]	Loss 1.3719e-01 (1.3719e-01)
Epoch: [41][ 25/246]	Loss 1.6257e-01 (1.5154e-01)
Epoch: [41][ 50/246]	Loss 1.1006e-01 (1.6208e-01)
Epoch: [41][ 75/246]	Loss 2.2053e-01 (1.6398e-01)
Epoch: [41][100/246]	Loss 1.9607e-01 (1.6172e-01)
Epoch: [41][125/246]	Loss 1.3005e-01 (1.6231e-01)
Epoch: [41][150/246]	Loss 1.2876e-01 (1.6358e-01)
Epoch: [41][175/246]	Loss 1.1543e-01 (1.6199e-01)
Epoch: [41][200/246]	Loss 2.0902e-01 (1.6258e-01)
Epoch: [41][225/246]	Loss 2.8393e-01 (1.6334e-01)
Checkpoint ...
[33mEpoch 42/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00006
Train ...
Epoch: [42][  0/246]	Loss 2.0734e-01 (2.0734e-01)
Epoch: [42][ 25/246]	Loss 1.4534e-01 (1.7120e-01)
Epoch: [42][ 50/246]	Loss 2.8779e-01 (1.6741e-01)
Epoch: [42][ 75/246]	Loss 1.2117e-01 (1.6548e-01)
Epoch: [42][100/246]	Loss 1.3875e-01 (1.6670e-01)
Epoch: [42][125/246]	Loss 1.9043e-01 (1.6512e-01)
Epoch: [42][150/246]	Loss 2.0970e-01 (1.6721e-01)
Epoch: [42][175/246]	Loss 1.9009e-01 (1.6698e-01)
Epoch: [42][200/246]	Loss 1.4024e-01 (1.6589e-01)
Epoch: [42][225/246]	Loss 2.1883e-01 (1.6644e-01)
Checkpoint ...
[33mEpoch 43/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00005
Train ...
Epoch: [43][  0/246]	Loss 1.4279e-01 (1.4279e-01)
Epoch: [43][ 25/246]	Loss 1.4111e-01 (1.6199e-01)
Epoch: [43][ 50/246]	Loss 1.5924e-01 (1.5752e-01)
Epoch: [43][ 75/246]	Loss 1.4221e-01 (1.5934e-01)
Epoch: [43][100/246]	Loss 1.7867e-01 (1.6011e-01)
Epoch: [43][125/246]	Loss 1.3976e-01 (1.6044e-01)
Epoch: [43][150/246]	Loss 1.5658e-01 (1.6124e-01)
Epoch: [43][175/246]	Loss 1.5179e-01 (1.6033e-01)
Epoch: [43][200/246]	Loss 1.5136e-01 (1.6109e-01)
Epoch: [43][225/246]	Loss 1.7705e-01 (1.6164e-01)
Checkpoint ...
[33mEpoch 44/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00004
Train ...
Epoch: [44][  0/246]	Loss 2.0110e-01 (2.0110e-01)
Epoch: [44][ 25/246]	Loss 1.2810e-01 (1.5292e-01)
Epoch: [44][ 50/246]	Loss 1.8025e-01 (1.5970e-01)
Epoch: [44][ 75/246]	Loss 1.4157e-01 (1.6229e-01)
Epoch: [44][100/246]	Loss 1.5462e-01 (1.6195e-01)
Epoch: [44][125/246]	Loss 1.1896e-01 (1.6081e-01)
Epoch: [44][150/246]	Loss 1.6681e-01 (1.6037e-01)
Epoch: [44][175/246]	Loss 1.7571e-01 (1.6120e-01)
Epoch: [44][200/246]	Loss 1.3391e-01 (1.5971e-01)
Epoch: [44][225/246]	Loss 2.1699e-01 (1.6094e-01)
Checkpoint ...
[33mEpoch 45/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00003
Train ...
Epoch: [45][  0/246]	Loss 1.7295e-01 (1.7295e-01)
Epoch: [45][ 25/246]	Loss 1.2561e-01 (1.5586e-01)
Epoch: [45][ 50/246]	Loss 1.7158e-01 (1.5824e-01)
Epoch: [45][ 75/246]	Loss 1.7553e-01 (1.6215e-01)
Epoch: [45][100/246]	Loss 1.1290e-01 (1.6382e-01)
Epoch: [45][125/246]	Loss 1.0237e-01 (1.6739e-01)
Epoch: [45][150/246]	Loss 9.4300e-02 (1.6621e-01)
Epoch: [45][175/246]	Loss 1.3052e-01 (1.6813e-01)
Epoch: [45][200/246]	Loss 2.4260e-01 (1.6829e-01)
Epoch: [45][225/246]	Loss 1.0431e-01 (1.6838e-01)
Checkpoint ...
[33mEpoch 46/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00002
Train ...
Epoch: [46][  0/246]	Loss 2.4070e-01 (2.4070e-01)
Epoch: [46][ 25/246]	Loss 8.8448e-02 (1.5517e-01)
Epoch: [46][ 50/246]	Loss 1.6051e-01 (1.5836e-01)
Epoch: [46][ 75/246]	Loss 2.1383e-01 (1.6003e-01)
Epoch: [46][100/246]	Loss 1.5410e-01 (1.5976e-01)
Epoch: [46][125/246]	Loss 2.1105e-01 (1.6269e-01)
Epoch: [46][150/246]	Loss 2.1397e-01 (1.6413e-01)
Epoch: [46][175/246]	Loss 1.6741e-01 (1.6219e-01)
Epoch: [46][200/246]	Loss 1.6865e-01 (1.6278e-01)
Epoch: [46][225/246]	Loss 1.9291e-01 (1.6329e-01)
Checkpoint ...
[33mEpoch 47/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00001
Train ...
Epoch: [47][  0/246]	Loss 1.4100e-01 (1.4100e-01)
Epoch: [47][ 25/246]	Loss 1.2398e-01 (1.5713e-01)
Epoch: [47][ 50/246]	Loss 1.4686e-01 (1.5644e-01)
Epoch: [47][ 75/246]	Loss 1.3286e-01 (1.5736e-01)
Epoch: [47][100/246]	Loss 2.3187e-01 (1.5998e-01)
Epoch: [47][125/246]	Loss 1.7263e-01 (1.6100e-01)
Epoch: [47][150/246]	Loss 1.5096e-01 (1.6070e-01)
Epoch: [47][175/246]	Loss 1.2347e-01 (1.6253e-01)
Epoch: [47][200/246]	Loss 1.8298e-01 (1.6017e-01)
Epoch: [47][225/246]	Loss 9.9411e-02 (1.6073e-01)
Checkpoint ...
[33mEpoch 48/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00000
Train ...
Epoch: [48][  0/246]	Loss 1.5353e-01 (1.5353e-01)
Epoch: [48][ 25/246]	Loss 1.3692e-01 (1.5842e-01)
Epoch: [48][ 50/246]	Loss 2.1710e-01 (1.6324e-01)
Epoch: [48][ 75/246]	Loss 1.8080e-01 (1.6164e-01)
Epoch: [48][100/246]	Loss 1.9852e-01 (1.6624e-01)
Epoch: [48][125/246]	Loss 2.2531e-01 (1.6613e-01)
Epoch: [48][150/246]	Loss 1.9152e-01 (1.6659e-01)
Epoch: [48][175/246]	Loss 1.4153e-01 (1.6422e-01)
Epoch: [48][200/246]	Loss 1.6085e-01 (1.6248e-01)
Epoch: [48][225/246]	Loss 2.7409e-01 (1.6244e-01)
Checkpoint ...
[33mEpoch 49/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00000
Train ...
Epoch: [49][  0/246]	Loss 1.8108e-01 (1.8108e-01)
Epoch: [49][ 25/246]	Loss 2.8964e-01 (1.6001e-01)
Epoch: [49][ 50/246]	Loss 1.8663e-01 (1.6252e-01)
Epoch: [49][ 75/246]	Loss 2.2252e-01 (1.6654e-01)
Epoch: [49][100/246]	Loss 2.4077e-01 (1.7083e-01)
Epoch: [49][125/246]	Loss 1.7565e-01 (1.6698e-01)
Epoch: [49][150/246]	Loss 1.2583e-01 (1.6877e-01)
Epoch: [49][175/246]	Loss 1.3483e-01 (1.6792e-01)
Epoch: [49][200/246]	Loss 9.8100e-02 (1.6562e-01)
Epoch: [49][225/246]	Loss 2.0779e-01 (1.6707e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 97.49
Checkpoint ...
[34mFill memory bank for mining the nearest neighbors (train) ...[0m
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Mine the nearest neighbors (Top-20)
Accuracy of top-20 nearest neighbors on train set is 56.87
[34mFill memory bank for mining the nearest neighbors (val) ...[0m
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Mine the nearest neighbors (Top-5)
Accuracy of top-5 nearest neighbors on val set is 61.72
