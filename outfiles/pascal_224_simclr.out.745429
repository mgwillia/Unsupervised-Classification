vulcan03.umiacs.umd.edu
[31m{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-224', 'val_db_name': 'pascal-pretrained-224', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 64, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 224, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 224, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224/pretext/topk-val-neighbors.npy'}[0m
[34mRetrieve model[0m
{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-224', 'val_db_name': 'pascal-pretrained-224', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 64, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 224, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 224, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224/pretext/topk-val-neighbors.npy'}
loading pretrained
Model is ContrastiveModel
Model parameters: 30.02M
ContrastiveModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (contrastive_head): Sequential(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU()
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[34mSet CuDNN benchmark[0m
[34mRetrieve dataset[0m
Train transforms: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)
    RandomHorizontalFlip(p=0.5)
    RandomApply(
    p=0.8
    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[-0.1, 0.1])
)
    RandomGrayscale(p=0.2)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Validation transforms: Compose(
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Dataset contains 15774/15774 train/val samples
[34mBuild MemoryBank[0m
[34mRetrieve criterion[0m
Criterion is SimCLRLoss
[34mRetrieve optimizer[0m
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.001
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
[34mNo checkpoint file at /cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224/pretext/checkpoint.pth.tar[0m
[34mStarting main loop[0m
[33mEpoch 0/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [0][  0/246]	Loss 2.7321e+00 (2.7321e+00)
Epoch: [0][ 25/246]	Loss 7.1008e-01 (1.2848e+00)
Epoch: [0][ 50/246]	Loss 9.3204e-01 (1.0231e+00)
Epoch: [0][ 75/246]	Loss 6.3535e-01 (9.1047e-01)
Epoch: [0][100/246]	Loss 6.1579e-01 (8.3796e-01)
Epoch: [0][125/246]	Loss 4.3919e-01 (7.9445e-01)
Epoch: [0][150/246]	Loss 6.9498e-01 (7.5523e-01)
Epoch: [0][175/246]	Loss 5.1813e-01 (7.2039e-01)
Epoch: [0][200/246]	Loss 3.6812e-01 (6.9136e-01)
Epoch: [0][225/246]	Loss 5.4526e-01 (6.7379e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 94.36
Checkpoint ...
[33mEpoch 1/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [1][  0/246]	Loss 3.7983e-01 (3.7983e-01)
Epoch: [1][ 25/246]	Loss 5.8126e-01 (4.9668e-01)
Epoch: [1][ 50/246]	Loss 5.2083e-01 (4.8796e-01)
Epoch: [1][ 75/246]	Loss 5.0840e-01 (4.6357e-01)
Epoch: [1][100/246]	Loss 3.8286e-01 (4.5601e-01)
Epoch: [1][125/246]	Loss 3.5199e-01 (4.5742e-01)
Epoch: [1][150/246]	Loss 3.7490e-01 (4.5662e-01)
Epoch: [1][175/246]	Loss 5.3333e-01 (4.4895e-01)
Epoch: [1][200/246]	Loss 4.2747e-01 (4.4373e-01)
Epoch: [1][225/246]	Loss 3.2261e-01 (4.3563e-01)
Checkpoint ...
[33mEpoch 2/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [2][  0/246]	Loss 5.5858e-01 (5.5858e-01)
Epoch: [2][ 25/246]	Loss 3.5230e-01 (3.7170e-01)
Epoch: [2][ 50/246]	Loss 2.9667e-01 (3.9203e-01)
Epoch: [2][ 75/246]	Loss 2.9066e-01 (3.8519e-01)
Epoch: [2][100/246]	Loss 3.3506e-01 (3.8025e-01)
Epoch: [2][125/246]	Loss 2.7474e-01 (3.7825e-01)
Epoch: [2][150/246]	Loss 3.6143e-01 (3.7325e-01)
Epoch: [2][175/246]	Loss 2.6338e-01 (3.6925e-01)
Epoch: [2][200/246]	Loss 5.2281e-01 (3.6888e-01)
Epoch: [2][225/246]	Loss 3.5534e-01 (3.6659e-01)
Checkpoint ...
[33mEpoch 3/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00099
Train ...
Epoch: [3][  0/246]	Loss 4.3389e-01 (4.3389e-01)
Epoch: [3][ 25/246]	Loss 4.0978e-01 (3.2172e-01)
Epoch: [3][ 50/246]	Loss 2.9690e-01 (3.3899e-01)
Epoch: [3][ 75/246]	Loss 2.6853e-01 (3.3696e-01)
Epoch: [3][100/246]	Loss 2.8493e-01 (3.3462e-01)
Epoch: [3][125/246]	Loss 2.9536e-01 (3.3391e-01)
Epoch: [3][150/246]	Loss 3.1488e-01 (3.3033e-01)
Epoch: [3][175/246]	Loss 2.7883e-01 (3.2754e-01)
Epoch: [3][200/246]	Loss 2.3487e-01 (3.2713e-01)
Epoch: [3][225/246]	Loss 2.5458e-01 (3.2403e-01)
Checkpoint ...
[33mEpoch 4/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [4][  0/246]	Loss 2.8429e-01 (2.8429e-01)
Epoch: [4][ 25/246]	Loss 3.0757e-01 (2.9501e-01)
Epoch: [4][ 50/246]	Loss 3.4174e-01 (2.9288e-01)
Epoch: [4][ 75/246]	Loss 3.3093e-01 (3.0136e-01)
Epoch: [4][100/246]	Loss 2.5926e-01 (3.0346e-01)
Epoch: [4][125/246]	Loss 2.2339e-01 (2.9837e-01)
Epoch: [4][150/246]	Loss 3.1099e-01 (2.9962e-01)
Epoch: [4][175/246]	Loss 2.8977e-01 (3.0066e-01)
Epoch: [4][200/246]	Loss 4.2018e-01 (2.9885e-01)
Epoch: [4][225/246]	Loss 3.4869e-01 (2.9557e-01)
Checkpoint ...
[33mEpoch 5/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [5][  0/246]	Loss 3.6126e-01 (3.6126e-01)
Epoch: [5][ 25/246]	Loss 4.3195e-01 (2.9964e-01)
Epoch: [5][ 50/246]	Loss 2.8055e-01 (2.9334e-01)
Epoch: [5][ 75/246]	Loss 2.5347e-01 (2.8471e-01)
Epoch: [5][100/246]	Loss 2.5234e-01 (2.8169e-01)
Epoch: [5][125/246]	Loss 3.0490e-01 (2.8078e-01)
Epoch: [5][150/246]	Loss 2.2795e-01 (2.8312e-01)
Epoch: [5][175/246]	Loss 3.1312e-01 (2.8605e-01)
Epoch: [5][200/246]	Loss 2.0467e-01 (2.8607e-01)
Epoch: [5][225/246]	Loss 3.7967e-01 (2.8511e-01)
Checkpoint ...
[33mEpoch 6/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00096
Train ...
Epoch: [6][  0/246]	Loss 3.4262e-01 (3.4262e-01)
Epoch: [6][ 25/246]	Loss 3.0069e-01 (2.6974e-01)
Epoch: [6][ 50/246]	Loss 2.1100e-01 (2.6320e-01)
Epoch: [6][ 75/246]	Loss 2.6438e-01 (2.6486e-01)
Epoch: [6][100/246]	Loss 2.6592e-01 (2.6371e-01)
Epoch: [6][125/246]	Loss 2.4463e-01 (2.6682e-01)
Epoch: [6][150/246]	Loss 2.3354e-01 (2.6733e-01)
Epoch: [6][175/246]	Loss 1.9641e-01 (2.6838e-01)
Epoch: [6][200/246]	Loss 2.5562e-01 (2.6757e-01)
Epoch: [6][225/246]	Loss 1.8981e-01 (2.6732e-01)
Checkpoint ...
[33mEpoch 7/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00095
Train ...
Epoch: [7][  0/246]	Loss 2.5385e-01 (2.5385e-01)
Epoch: [7][ 25/246]	Loss 1.9284e-01 (2.6023e-01)
Epoch: [7][ 50/246]	Loss 3.3629e-01 (2.6168e-01)
Epoch: [7][ 75/246]	Loss 2.0493e-01 (2.5542e-01)
Epoch: [7][100/246]	Loss 3.0294e-01 (2.5838e-01)
Epoch: [7][125/246]	Loss 2.0553e-01 (2.5422e-01)
Epoch: [7][150/246]	Loss 2.4128e-01 (2.5111e-01)
Epoch: [7][175/246]	Loss 1.8421e-01 (2.4652e-01)
Epoch: [7][200/246]	Loss 2.3165e-01 (2.4633e-01)
Epoch: [7][225/246]	Loss 2.9380e-01 (2.4654e-01)
Checkpoint ...
[33mEpoch 8/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00094
Train ...
Epoch: [8][  0/246]	Loss 2.4580e-01 (2.4580e-01)
Epoch: [8][ 25/246]	Loss 2.7354e-01 (2.4689e-01)
Epoch: [8][ 50/246]	Loss 2.3580e-01 (2.4804e-01)
Epoch: [8][ 75/246]	Loss 1.7362e-01 (2.4700e-01)
Epoch: [8][100/246]	Loss 2.7016e-01 (2.4453e-01)
Epoch: [8][125/246]	Loss 3.0828e-01 (2.4533e-01)
Epoch: [8][150/246]	Loss 2.3235e-01 (2.4587e-01)
Epoch: [8][175/246]	Loss 1.3835e-01 (2.4499e-01)
Epoch: [8][200/246]	Loss 1.8859e-01 (2.4686e-01)
Epoch: [8][225/246]	Loss 2.7329e-01 (2.4509e-01)
Checkpoint ...
[33mEpoch 9/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00092
Train ...
Epoch: [9][  0/246]	Loss 2.0863e-01 (2.0863e-01)
Epoch: [9][ 25/246]	Loss 2.4421e-01 (2.4683e-01)
Epoch: [9][ 50/246]	Loss 2.3792e-01 (2.4199e-01)
Epoch: [9][ 75/246]	Loss 3.4480e-01 (2.4259e-01)
Epoch: [9][100/246]	Loss 2.2968e-01 (2.4239e-01)
Epoch: [9][125/246]	Loss 2.2421e-01 (2.4373e-01)
Epoch: [9][150/246]	Loss 1.3894e-01 (2.4481e-01)
Epoch: [9][175/246]	Loss 3.0832e-01 (2.4560e-01)
Epoch: [9][200/246]	Loss 2.0845e-01 (2.4520e-01)
Epoch: [9][225/246]	Loss 2.2679e-01 (2.4335e-01)
Checkpoint ...
[33mEpoch 10/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00090
Train ...
Epoch: [10][  0/246]	Loss 1.4074e-01 (1.4074e-01)
Epoch: [10][ 25/246]	Loss 2.4871e-01 (2.3983e-01)
Epoch: [10][ 50/246]	Loss 1.3626e-01 (2.3963e-01)
Epoch: [10][ 75/246]	Loss 1.5638e-01 (2.2992e-01)
Epoch: [10][100/246]	Loss 2.1288e-01 (2.2849e-01)
Epoch: [10][125/246]	Loss 2.3910e-01 (2.2873e-01)
Epoch: [10][150/246]	Loss 1.5107e-01 (2.2660e-01)
Epoch: [10][175/246]	Loss 2.7308e-01 (2.2384e-01)
Epoch: [10][200/246]	Loss 2.8703e-01 (2.2325e-01)
Epoch: [10][225/246]	Loss 1.7737e-01 (2.2503e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 96.56
Checkpoint ...
[33mEpoch 11/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00089
Train ...
Epoch: [11][  0/246]	Loss 2.9633e-01 (2.9633e-01)
Epoch: [11][ 25/246]	Loss 1.8147e-01 (2.2151e-01)
Epoch: [11][ 50/246]	Loss 3.2780e-01 (2.2127e-01)
Epoch: [11][ 75/246]	Loss 1.8504e-01 (2.1512e-01)
Epoch: [11][100/246]	Loss 1.9235e-01 (2.1815e-01)
Epoch: [11][125/246]	Loss 1.6200e-01 (2.1829e-01)
Epoch: [11][150/246]	Loss 2.4136e-01 (2.1955e-01)
Epoch: [11][175/246]	Loss 2.2862e-01 (2.1904e-01)
Epoch: [11][200/246]	Loss 2.2614e-01 (2.1859e-01)
Epoch: [11][225/246]	Loss 3.0518e-01 (2.1944e-01)
Checkpoint ...
[33mEpoch 12/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00086
Train ...
Epoch: [12][  0/246]	Loss 2.0975e-01 (2.0975e-01)
Epoch: [12][ 25/246]	Loss 1.7041e-01 (2.1970e-01)
Epoch: [12][ 50/246]	Loss 2.6706e-01 (2.1776e-01)
Epoch: [12][ 75/246]	Loss 1.8180e-01 (2.2386e-01)
Epoch: [12][100/246]	Loss 1.5071e-01 (2.2336e-01)
Epoch: [12][125/246]	Loss 1.6097e-01 (2.1992e-01)
Epoch: [12][150/246]	Loss 2.1442e-01 (2.1676e-01)
Epoch: [12][175/246]	Loss 1.7023e-01 (2.1624e-01)
Epoch: [12][200/246]	Loss 1.8674e-01 (2.1753e-01)
Epoch: [12][225/246]	Loss 2.3428e-01 (2.1781e-01)
Checkpoint ...
[33mEpoch 13/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00084
Train ...
Epoch: [13][  0/246]	Loss 1.6392e-01 (1.6392e-01)
Epoch: [13][ 25/246]	Loss 3.4910e-01 (2.0088e-01)
Epoch: [13][ 50/246]	Loss 2.7104e-01 (2.1287e-01)
Epoch: [13][ 75/246]	Loss 2.2468e-01 (2.1554e-01)
Epoch: [13][100/246]	Loss 2.5218e-01 (2.1547e-01)
Epoch: [13][125/246]	Loss 2.1955e-01 (2.1512e-01)
Epoch: [13][150/246]	Loss 2.9906e-01 (2.1514e-01)
Epoch: [13][175/246]	Loss 2.4337e-01 (2.1366e-01)
Epoch: [13][200/246]	Loss 2.1844e-01 (2.1295e-01)
Epoch: [13][225/246]	Loss 2.9888e-01 (2.1223e-01)
Checkpoint ...
[33mEpoch 14/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00082
Train ...
Epoch: [14][  0/246]	Loss 2.8439e-01 (2.8439e-01)
Epoch: [14][ 25/246]	Loss 1.8433e-01 (2.1702e-01)
Epoch: [14][ 50/246]	Loss 3.5540e-01 (2.2034e-01)
Epoch: [14][ 75/246]	Loss 1.8114e-01 (2.1778e-01)
Epoch: [14][100/246]	Loss 1.2836e-01 (2.1539e-01)
Epoch: [14][125/246]	Loss 1.4835e-01 (2.1247e-01)
Epoch: [14][150/246]	Loss 2.4488e-01 (2.1026e-01)
Epoch: [14][175/246]	Loss 3.1460e-01 (2.1087e-01)
Epoch: [14][200/246]	Loss 2.7596e-01 (2.1222e-01)
Epoch: [14][225/246]	Loss 1.8901e-01 (2.1078e-01)
Checkpoint ...
[33mEpoch 15/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00079
Train ...
Epoch: [15][  0/246]	Loss 1.6523e-01 (1.6523e-01)
Epoch: [15][ 25/246]	Loss 1.9349e-01 (1.9334e-01)
Epoch: [15][ 50/246]	Loss 1.4842e-01 (1.9645e-01)
Epoch: [15][ 75/246]	Loss 2.6675e-01 (1.9803e-01)
Epoch: [15][100/246]	Loss 1.7235e-01 (2.0193e-01)
Epoch: [15][125/246]	Loss 1.8646e-01 (2.0246e-01)
Epoch: [15][150/246]	Loss 1.8811e-01 (2.0153e-01)
Epoch: [15][175/246]	Loss 1.7005e-01 (2.0170e-01)
Epoch: [15][200/246]	Loss 2.0307e-01 (2.0145e-01)
Epoch: [15][225/246]	Loss 1.3740e-01 (2.0228e-01)
Checkpoint ...
[33mEpoch 16/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00077
Train ...
Epoch: [16][  0/246]	Loss 1.7814e-01 (1.7814e-01)
Epoch: [16][ 25/246]	Loss 2.5160e-01 (2.0671e-01)
Epoch: [16][ 50/246]	Loss 2.9021e-01 (2.0486e-01)
Epoch: [16][ 75/246]	Loss 1.5362e-01 (2.0174e-01)
Epoch: [16][100/246]	Loss 2.1309e-01 (1.9832e-01)
Epoch: [16][125/246]	Loss 1.9865e-01 (1.9796e-01)
Epoch: [16][150/246]	Loss 1.5691e-01 (1.9679e-01)
Epoch: [16][175/246]	Loss 1.8999e-01 (1.9814e-01)
Epoch: [16][200/246]	Loss 2.4090e-01 (2.0087e-01)
Epoch: [16][225/246]	Loss 1.5893e-01 (2.0166e-01)
Checkpoint ...
[33mEpoch 17/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00074
Train ...
Epoch: [17][  0/246]	Loss 2.1852e-01 (2.1852e-01)
Epoch: [17][ 25/246]	Loss 1.7359e-01 (1.8369e-01)
Epoch: [17][ 50/246]	Loss 2.0691e-01 (1.9263e-01)
Epoch: [17][ 75/246]	Loss 2.1404e-01 (1.9428e-01)
Epoch: [17][100/246]	Loss 4.1792e-01 (1.9518e-01)
Epoch: [17][125/246]	Loss 1.8607e-01 (1.9398e-01)
Epoch: [17][150/246]	Loss 1.5982e-01 (1.9386e-01)
Epoch: [17][175/246]	Loss 1.6662e-01 (1.9521e-01)
Epoch: [17][200/246]	Loss 1.3740e-01 (1.9483e-01)
Epoch: [17][225/246]	Loss 1.5461e-01 (1.9548e-01)
Checkpoint ...
[33mEpoch 18/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00071
Train ...
Epoch: [18][  0/246]	Loss 1.5852e-01 (1.5852e-01)
Epoch: [18][ 25/246]	Loss 1.7979e-01 (1.8487e-01)
Epoch: [18][ 50/246]	Loss 1.9440e-01 (1.8852e-01)
Epoch: [18][ 75/246]	Loss 1.8464e-01 (1.9038e-01)
Epoch: [18][100/246]	Loss 1.4323e-01 (1.8845e-01)
Epoch: [18][125/246]	Loss 1.8601e-01 (1.8773e-01)
Epoch: [18][150/246]	Loss 1.6513e-01 (1.8951e-01)
Epoch: [18][175/246]	Loss 1.6142e-01 (1.9103e-01)
Epoch: [18][200/246]	Loss 1.5499e-01 (1.9202e-01)
Epoch: [18][225/246]	Loss 1.9433e-01 (1.9191e-01)
Checkpoint ...
[33mEpoch 19/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00068
Train ...
Epoch: [19][  0/246]	Loss 1.5516e-01 (1.5516e-01)
Epoch: [19][ 25/246]	Loss 2.0027e-01 (1.8606e-01)
Epoch: [19][ 50/246]	Loss 2.0757e-01 (1.9404e-01)
Epoch: [19][ 75/246]	Loss 2.2859e-01 (1.9017e-01)
Epoch: [19][100/246]	Loss 2.6820e-01 (1.9371e-01)
Epoch: [19][125/246]	Loss 2.3199e-01 (1.9721e-01)
Epoch: [19][150/246]	Loss 2.0589e-01 (1.9421e-01)
Epoch: [19][175/246]	Loss 2.7193e-01 (1.9249e-01)
Epoch: [19][200/246]	Loss 1.1041e-01 (1.9188e-01)
Epoch: [19][225/246]	Loss 1.3115e-01 (1.8982e-01)
Checkpoint ...
[33mEpoch 20/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00065
Train ...
Epoch: [20][  0/246]	Loss 1.1901e-01 (1.1901e-01)
Epoch: [20][ 25/246]	Loss 2.0080e-01 (1.8955e-01)
Epoch: [20][ 50/246]	Loss 2.1305e-01 (1.8980e-01)
Epoch: [20][ 75/246]	Loss 1.6961e-01 (1.8705e-01)
Epoch: [20][100/246]	Loss 2.1957e-01 (1.8294e-01)
Epoch: [20][125/246]	Loss 2.0530e-01 (1.8489e-01)
Epoch: [20][150/246]	Loss 1.5563e-01 (1.8325e-01)
Epoch: [20][175/246]	Loss 1.5600e-01 (1.8325e-01)
Epoch: [20][200/246]	Loss 1.1341e-01 (1.8433e-01)
Epoch: [20][225/246]	Loss 2.8144e-01 (1.8472e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 97.04
Checkpoint ...
[33mEpoch 21/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00062
Train ...
Epoch: [21][  0/246]	Loss 1.5792e-01 (1.5792e-01)
Epoch: [21][ 25/246]	Loss 2.4161e-01 (1.7727e-01)
Epoch: [21][ 50/246]	Loss 2.3876e-01 (1.8589e-01)
Epoch: [21][ 75/246]	Loss 2.4269e-01 (1.8615e-01)
Epoch: [21][100/246]	Loss 2.2005e-01 (1.8776e-01)
Epoch: [21][125/246]	Loss 1.7378e-01 (1.8632e-01)
Epoch: [21][150/246]	Loss 1.7985e-01 (1.8442e-01)
Epoch: [21][175/246]	Loss 2.1709e-01 (1.8639e-01)
Epoch: [21][200/246]	Loss 2.0682e-01 (1.8761e-01)
Epoch: [21][225/246]	Loss 1.7196e-01 (1.8723e-01)
Checkpoint ...
[33mEpoch 22/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00059
Train ...
Epoch: [22][  0/246]	Loss 1.7346e-01 (1.7346e-01)
Epoch: [22][ 25/246]	Loss 2.1292e-01 (1.9910e-01)
Epoch: [22][ 50/246]	Loss 1.4046e-01 (1.8790e-01)
Epoch: [22][ 75/246]	Loss 1.5766e-01 (1.8400e-01)
Epoch: [22][100/246]	Loss 1.1819e-01 (1.8038e-01)
Epoch: [22][125/246]	Loss 1.6253e-01 (1.8287e-01)
Epoch: [22][150/246]	Loss 2.0168e-01 (1.8075e-01)
Epoch: [22][175/246]	Loss 1.6839e-01 (1.8216e-01)
Epoch: [22][200/246]	Loss 1.8225e-01 (1.8275e-01)
Epoch: [22][225/246]	Loss 2.0142e-01 (1.8175e-01)
Checkpoint ...
[33mEpoch 23/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00056
Train ...
Epoch: [23][  0/246]	Loss 2.0027e-01 (2.0027e-01)
Epoch: [23][ 25/246]	Loss 1.4721e-01 (1.8952e-01)
Epoch: [23][ 50/246]	Loss 1.5853e-01 (1.8187e-01)
Epoch: [23][ 75/246]	Loss 1.4805e-01 (1.8180e-01)
Epoch: [23][100/246]	Loss 1.2446e-01 (1.7906e-01)
Epoch: [23][125/246]	Loss 1.7774e-01 (1.7533e-01)
Epoch: [23][150/246]	Loss 1.3522e-01 (1.7340e-01)
Epoch: [23][175/246]	Loss 2.2378e-01 (1.7477e-01)
Epoch: [23][200/246]	Loss 1.5887e-01 (1.7503e-01)
Epoch: [23][225/246]	Loss 1.8344e-01 (1.7550e-01)
Checkpoint ...
[33mEpoch 24/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00053
Train ...
Epoch: [24][  0/246]	Loss 1.8648e-01 (1.8648e-01)
Epoch: [24][ 25/246]	Loss 2.3638e-01 (1.7908e-01)
Epoch: [24][ 50/246]	Loss 1.6629e-01 (1.7741e-01)
Epoch: [24][ 75/246]	Loss 1.5497e-01 (1.8091e-01)
Epoch: [24][100/246]	Loss 1.6718e-01 (1.8180e-01)
Epoch: [24][125/246]	Loss 3.5656e-01 (1.8110e-01)
Epoch: [24][150/246]	Loss 7.5138e-02 (1.7866e-01)
Epoch: [24][175/246]	Loss 1.7261e-01 (1.7686e-01)
Epoch: [24][200/246]	Loss 1.3414e-01 (1.7534e-01)
Epoch: [24][225/246]	Loss 1.6331e-01 (1.7542e-01)
Checkpoint ...
[33mEpoch 25/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00050
Train ...
Epoch: [25][  0/246]	Loss 1.7599e-01 (1.7599e-01)
Epoch: [25][ 25/246]	Loss 1.7738e-01 (1.7865e-01)
Epoch: [25][ 50/246]	Loss 2.2866e-01 (1.7621e-01)
Epoch: [25][ 75/246]	Loss 2.0153e-01 (1.8227e-01)
Epoch: [25][100/246]	Loss 1.4895e-01 (1.8533e-01)
Epoch: [25][125/246]	Loss 1.3306e-01 (1.8059e-01)
Epoch: [25][150/246]	Loss 1.3285e-01 (1.7890e-01)
Epoch: [25][175/246]	Loss 1.6618e-01 (1.7795e-01)
Epoch: [25][200/246]	Loss 2.3286e-01 (1.7798e-01)
Epoch: [25][225/246]	Loss 1.2946e-01 (1.7784e-01)
Checkpoint ...
[33mEpoch 26/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00047
Train ...
Epoch: [26][  0/246]	Loss 1.6415e-01 (1.6415e-01)
Epoch: [26][ 25/246]	Loss 1.6983e-01 (1.7383e-01)
Epoch: [26][ 50/246]	Loss 9.7196e-02 (1.6765e-01)
Epoch: [26][ 75/246]	Loss 2.4352e-01 (1.6965e-01)
Epoch: [26][100/246]	Loss 2.7297e-01 (1.7197e-01)
Epoch: [26][125/246]	Loss 2.4755e-01 (1.7344e-01)
Epoch: [26][150/246]	Loss 1.2656e-01 (1.7345e-01)
Epoch: [26][175/246]	Loss 1.4505e-01 (1.7363e-01)
Epoch: [26][200/246]	Loss 1.4640e-01 (1.7297e-01)
Epoch: [26][225/246]	Loss 1.3266e-01 (1.7341e-01)
Checkpoint ...
[33mEpoch 27/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00044
Train ...
Epoch: [27][  0/246]	Loss 1.7959e-01 (1.7959e-01)
Epoch: [27][ 25/246]	Loss 1.2760e-01 (1.6441e-01)
Epoch: [27][ 50/246]	Loss 2.2207e-01 (1.6763e-01)
Epoch: [27][ 75/246]	Loss 1.6744e-01 (1.6886e-01)
Epoch: [27][100/246]	Loss 1.7381e-01 (1.7089e-01)
Epoch: [27][125/246]	Loss 1.5043e-01 (1.7459e-01)
Epoch: [27][150/246]	Loss 1.3498e-01 (1.7564e-01)
Epoch: [27][175/246]	Loss 1.3595e-01 (1.7385e-01)
Epoch: [27][200/246]	Loss 1.7923e-01 (1.7303e-01)
Epoch: [27][225/246]	Loss 1.5455e-01 (1.7182e-01)
Checkpoint ...
[33mEpoch 28/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00041
Train ...
Epoch: [28][  0/246]	Loss 1.3389e-01 (1.3389e-01)
Epoch: [28][ 25/246]	Loss 1.4675e-01 (1.7577e-01)
Epoch: [28][ 50/246]	Loss 2.2959e-01 (1.7807e-01)
Epoch: [28][ 75/246]	Loss 1.3653e-01 (1.7189e-01)
Epoch: [28][100/246]	Loss 1.3956e-01 (1.7261e-01)
Epoch: [28][125/246]	Loss 9.0797e-02 (1.7248e-01)
Epoch: [28][150/246]	Loss 2.2092e-01 (1.7312e-01)
Epoch: [28][175/246]	Loss 1.5105e-01 (1.7307e-01)
Epoch: [28][200/246]	Loss 2.8595e-01 (1.7356e-01)
Epoch: [28][225/246]	Loss 1.7053e-01 (1.7303e-01)
Checkpoint ...
[33mEpoch 29/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00038
Train ...
Epoch: [29][  0/246]	Loss 2.1683e-01 (2.1683e-01)
Epoch: [29][ 25/246]	Loss 1.5920e-01 (1.7146e-01)
Epoch: [29][ 50/246]	Loss 2.1893e-01 (1.7172e-01)
Epoch: [29][ 75/246]	Loss 1.7411e-01 (1.7249e-01)
Epoch: [29][100/246]	Loss 1.6955e-01 (1.7477e-01)
Epoch: [29][125/246]	Loss 1.6647e-01 (1.7160e-01)
Epoch: [29][150/246]	Loss 2.2317e-01 (1.7072e-01)
Epoch: [29][175/246]	Loss 1.5946e-01 (1.7289e-01)
Epoch: [29][200/246]	Loss 1.2318e-01 (1.7184e-01)
Epoch: [29][225/246]	Loss 1.4289e-01 (1.7169e-01)
Checkpoint ...
[33mEpoch 30/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00035
Train ...
Epoch: [30][  0/246]	Loss 1.7325e-01 (1.7325e-01)
Epoch: [30][ 25/246]	Loss 1.1252e-01 (1.8151e-01)
Epoch: [30][ 50/246]	Loss 1.2518e-01 (1.7112e-01)
Epoch: [30][ 75/246]	Loss 1.2150e-01 (1.7105e-01)
Epoch: [30][100/246]	Loss 1.8538e-01 (1.7090e-01)
Epoch: [30][125/246]	Loss 1.4282e-01 (1.7126e-01)
Epoch: [30][150/246]	Loss 1.6698e-01 (1.7128e-01)
Epoch: [30][175/246]	Loss 1.4670e-01 (1.6874e-01)
Epoch: [30][200/246]	Loss 2.1059e-01 (1.6963e-01)
Epoch: [30][225/246]	Loss 1.6434e-01 (1.7035e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 96.82
Checkpoint ...
[33mEpoch 31/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00032
Train ...
Epoch: [31][  0/246]	Loss 1.7789e-01 (1.7789e-01)
Epoch: [31][ 25/246]	Loss 1.2414e-01 (1.7251e-01)
Epoch: [31][ 50/246]	Loss 1.7586e-01 (1.7041e-01)
Epoch: [31][ 75/246]	Loss 1.2827e-01 (1.6669e-01)
Epoch: [31][100/246]	Loss 1.8388e-01 (1.6861e-01)
Epoch: [31][125/246]	Loss 1.2101e-01 (1.7010e-01)
Epoch: [31][150/246]	Loss 1.6913e-01 (1.6803e-01)
Epoch: [31][175/246]	Loss 2.2051e-01 (1.6966e-01)
Epoch: [31][200/246]	Loss 1.3829e-01 (1.6843e-01)
Epoch: [31][225/246]	Loss 1.2730e-01 (1.6745e-01)
Checkpoint ...
[33mEpoch 32/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00029
Train ...
Epoch: [32][  0/246]	Loss 1.6022e-01 (1.6022e-01)
Epoch: [32][ 25/246]	Loss 2.1826e-01 (1.6782e-01)
Epoch: [32][ 50/246]	Loss 2.0227e-01 (1.7907e-01)
Epoch: [32][ 75/246]	Loss 1.7309e-01 (1.7260e-01)
Epoch: [32][100/246]	Loss 1.7812e-01 (1.7026e-01)
Epoch: [32][125/246]	Loss 2.7856e-01 (1.7011e-01)
Epoch: [32][150/246]	Loss 1.4235e-01 (1.6812e-01)
Epoch: [32][175/246]	Loss 8.1141e-02 (1.6865e-01)
Epoch: [32][200/246]	Loss 2.2739e-01 (1.6798e-01)
Epoch: [32][225/246]	Loss 2.8341e-01 (1.6869e-01)
Checkpoint ...
[33mEpoch 33/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00026
Train ...
Epoch: [33][  0/246]	Loss 1.1699e-01 (1.1699e-01)
Epoch: [33][ 25/246]	Loss 1.7888e-01 (1.7045e-01)
Epoch: [33][ 50/246]	Loss 1.8173e-01 (1.6027e-01)
Epoch: [33][ 75/246]	Loss 1.4096e-01 (1.6520e-01)
Epoch: [33][100/246]	Loss 1.5858e-01 (1.6761e-01)
Epoch: [33][125/246]	Loss 2.0048e-01 (1.6861e-01)
Epoch: [33][150/246]	Loss 1.3476e-01 (1.6561e-01)
Epoch: [33][175/246]	Loss 1.2094e-01 (1.6698e-01)
Epoch: [33][200/246]	Loss 2.2459e-01 (1.6845e-01)
Epoch: [33][225/246]	Loss 1.7433e-01 (1.6690e-01)
Checkpoint ...
[33mEpoch 34/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00023
Train ...
Epoch: [34][  0/246]	Loss 1.0722e-01 (1.0722e-01)
Epoch: [34][ 25/246]	Loss 2.0191e-01 (1.7422e-01)
Epoch: [34][ 50/246]	Loss 2.5523e-01 (1.6727e-01)
Epoch: [34][ 75/246]	Loss 1.5163e-01 (1.6909e-01)
Epoch: [34][100/246]	Loss 2.4994e-01 (1.6911e-01)
Epoch: [34][125/246]	Loss 1.2435e-01 (1.7027e-01)
Epoch: [34][150/246]	Loss 2.1756e-01 (1.7156e-01)
Epoch: [34][175/246]	Loss 1.1611e-01 (1.6901e-01)
Epoch: [34][200/246]	Loss 3.1741e-01 (1.6843e-01)
Epoch: [34][225/246]	Loss 1.3863e-01 (1.6972e-01)
Checkpoint ...
[33mEpoch 35/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00021
Train ...
Epoch: [35][  0/246]	Loss 7.2340e-02 (7.2340e-02)
Epoch: [35][ 25/246]	Loss 1.0887e-01 (1.6145e-01)
Epoch: [35][ 50/246]	Loss 1.6616e-01 (1.6506e-01)
Epoch: [35][ 75/246]	Loss 1.9321e-01 (1.6539e-01)
Epoch: [35][100/246]	Loss 1.9760e-01 (1.6327e-01)
Epoch: [35][125/246]	Loss 1.6668e-01 (1.6385e-01)
Epoch: [35][150/246]	Loss 1.6037e-01 (1.6464e-01)
Epoch: [35][175/246]	Loss 2.3207e-01 (1.6473e-01)
Epoch: [35][200/246]	Loss 1.6964e-01 (1.6707e-01)
Epoch: [35][225/246]	Loss 1.4038e-01 (1.6715e-01)
Checkpoint ...
[33mEpoch 36/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00018
Train ...
Epoch: [36][  0/246]	Loss 1.6128e-01 (1.6128e-01)
Epoch: [36][ 25/246]	Loss 3.2870e-01 (1.6628e-01)
Epoch: [36][ 50/246]	Loss 2.7409e-01 (1.6783e-01)
Epoch: [36][ 75/246]	Loss 1.6104e-01 (1.6471e-01)
Epoch: [36][100/246]	Loss 1.4932e-01 (1.6701e-01)
Epoch: [36][125/246]	Loss 1.2510e-01 (1.6206e-01)
Epoch: [36][150/246]	Loss 1.8451e-01 (1.6085e-01)
Epoch: [36][175/246]	Loss 1.7345e-01 (1.6048e-01)
Epoch: [36][200/246]	Loss 1.4668e-01 (1.5933e-01)
Epoch: [36][225/246]	Loss 2.0410e-01 (1.5945e-01)
Checkpoint ...
[33mEpoch 37/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00016
Train ...
Epoch: [37][  0/246]	Loss 1.2822e-01 (1.2822e-01)
Epoch: [37][ 25/246]	Loss 1.4586e-01 (1.5853e-01)
Epoch: [37][ 50/246]	Loss 1.2286e-01 (1.6857e-01)
Epoch: [37][ 75/246]	Loss 1.3203e-01 (1.6621e-01)
Epoch: [37][100/246]	Loss 2.1611e-01 (1.7001e-01)
Epoch: [37][125/246]	Loss 1.6969e-01 (1.6867e-01)
Epoch: [37][150/246]	Loss 1.7823e-01 (1.6925e-01)
Epoch: [37][175/246]	Loss 2.1740e-01 (1.6742e-01)
Epoch: [37][200/246]	Loss 1.1272e-01 (1.6598e-01)
Epoch: [37][225/246]	Loss 1.1933e-01 (1.6782e-01)
Checkpoint ...
[33mEpoch 38/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00014
Train ...
Epoch: [38][  0/246]	Loss 1.1725e-01 (1.1725e-01)
Epoch: [38][ 25/246]	Loss 1.1324e-01 (1.6564e-01)
Epoch: [38][ 50/246]	Loss 1.9466e-01 (1.6476e-01)
Epoch: [38][ 75/246]	Loss 1.4564e-01 (1.6284e-01)
Epoch: [38][100/246]	Loss 1.3083e-01 (1.6425e-01)
Epoch: [38][125/246]	Loss 2.4436e-01 (1.6413e-01)
Epoch: [38][150/246]	Loss 1.6779e-01 (1.6495e-01)
Epoch: [38][175/246]	Loss 1.7448e-01 (1.6576e-01)
Epoch: [38][200/246]	Loss 2.1043e-01 (1.6660e-01)
Epoch: [38][225/246]	Loss 2.2122e-01 (1.6652e-01)
Checkpoint ...
[33mEpoch 39/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00012
Train ...
Epoch: [39][  0/246]	Loss 1.3101e-01 (1.3101e-01)
Epoch: [39][ 25/246]	Loss 1.0155e-01 (1.5796e-01)
Epoch: [39][ 50/246]	Loss 2.0352e-01 (1.6182e-01)
Epoch: [39][ 75/246]	Loss 1.0963e-01 (1.6269e-01)
Epoch: [39][100/246]	Loss 2.4311e-01 (1.6213e-01)
Epoch: [39][125/246]	Loss 2.3699e-01 (1.6269e-01)
Epoch: [39][150/246]	Loss 1.2677e-01 (1.6263e-01)
Epoch: [39][175/246]	Loss 2.3490e-01 (1.6494e-01)
Epoch: [39][200/246]	Loss 9.1212e-02 (1.6265e-01)
Epoch: [39][225/246]	Loss 1.5700e-01 (1.6188e-01)
Checkpoint ...
[33mEpoch 40/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [40][  0/246]	Loss 1.9845e-01 (1.9845e-01)
Epoch: [40][ 25/246]	Loss 2.2139e-01 (1.6451e-01)
Epoch: [40][ 50/246]	Loss 1.1997e-01 (1.6106e-01)
Epoch: [40][ 75/246]	Loss 1.1208e-01 (1.6334e-01)
Epoch: [40][100/246]	Loss 2.0035e-01 (1.6116e-01)
Epoch: [40][125/246]	Loss 1.4531e-01 (1.6137e-01)
Epoch: [40][150/246]	Loss 1.3784e-01 (1.5910e-01)
Epoch: [40][175/246]	Loss 1.5321e-01 (1.6176e-01)
Epoch: [40][200/246]	Loss 1.1259e-01 (1.6151e-01)
Epoch: [40][225/246]	Loss 1.9159e-01 (1.6282e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 97.45
Checkpoint ...
[33mEpoch 41/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00008
Train ...
Epoch: [41][  0/246]	Loss 2.1622e-01 (2.1622e-01)
Epoch: [41][ 25/246]	Loss 2.5459e-01 (1.7897e-01)
Epoch: [41][ 50/246]	Loss 1.3914e-01 (1.7041e-01)
Epoch: [41][ 75/246]	Loss 1.1023e-01 (1.6258e-01)
Epoch: [41][100/246]	Loss 1.2018e-01 (1.5998e-01)
Epoch: [41][125/246]	Loss 1.7970e-01 (1.6066e-01)
Epoch: [41][150/246]	Loss 1.2037e-01 (1.6142e-01)
Epoch: [41][175/246]	Loss 1.8681e-01 (1.6030e-01)
Epoch: [41][200/246]	Loss 2.4339e-01 (1.6087e-01)
Epoch: [41][225/246]	Loss 2.3845e-01 (1.6130e-01)
Checkpoint ...
[33mEpoch 42/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00006
Train ...
Epoch: [42][  0/246]	Loss 1.5390e-01 (1.5390e-01)
Epoch: [42][ 25/246]	Loss 1.9028e-01 (1.6886e-01)
Epoch: [42][ 50/246]	Loss 1.3956e-01 (1.6515e-01)
Epoch: [42][ 75/246]	Loss 1.6550e-01 (1.6288e-01)
Epoch: [42][100/246]	Loss 1.7246e-01 (1.6050e-01)
Epoch: [42][125/246]	Loss 1.3021e-01 (1.6544e-01)
Epoch: [42][150/246]	Loss 1.7317e-01 (1.6522e-01)
Epoch: [42][175/246]	Loss 1.8254e-01 (1.6513e-01)
Epoch: [42][200/246]	Loss 1.1604e-01 (1.6336e-01)
Epoch: [42][225/246]	Loss 1.3100e-01 (1.6239e-01)
Checkpoint ...
[33mEpoch 43/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00005
Train ...
Epoch: [43][  0/246]	Loss 1.3884e-01 (1.3884e-01)
Epoch: [43][ 25/246]	Loss 2.1403e-01 (1.5111e-01)
Epoch: [43][ 50/246]	Loss 1.4871e-01 (1.5893e-01)
Epoch: [43][ 75/246]	Loss 1.6637e-01 (1.5997e-01)
Epoch: [43][100/246]	Loss 1.1985e-01 (1.6081e-01)
Epoch: [43][125/246]	Loss 1.4418e-01 (1.6124e-01)
Epoch: [43][150/246]	Loss 1.7092e-01 (1.6220e-01)
Epoch: [43][175/246]	Loss 1.3824e-01 (1.6155e-01)
Epoch: [43][200/246]	Loss 1.8289e-01 (1.6003e-01)
Epoch: [43][225/246]	Loss 1.7855e-01 (1.6089e-01)
Checkpoint ...
[33mEpoch 44/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00004
Train ...
Epoch: [44][  0/246]	Loss 1.8742e-01 (1.8742e-01)
Epoch: [44][ 25/246]	Loss 1.7488e-01 (1.6493e-01)
Epoch: [44][ 50/246]	Loss 1.9953e-01 (1.7302e-01)
Epoch: [44][ 75/246]	Loss 1.5318e-01 (1.7056e-01)
Epoch: [44][100/246]	Loss 1.2928e-01 (1.6917e-01)
Epoch: [44][125/246]	Loss 1.8906e-01 (1.6881e-01)
Epoch: [44][150/246]	Loss 1.4766e-01 (1.6696e-01)
Epoch: [44][175/246]	Loss 1.8337e-01 (1.6726e-01)
Epoch: [44][200/246]	Loss 1.9464e-01 (1.6828e-01)
Epoch: [44][225/246]	Loss 1.3862e-01 (1.6729e-01)
Checkpoint ...
[33mEpoch 45/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00003
Train ...
Epoch: [45][  0/246]	Loss 1.4974e-01 (1.4974e-01)
Epoch: [45][ 25/246]	Loss 1.3734e-01 (1.7659e-01)
Epoch: [45][ 50/246]	Loss 1.3208e-01 (1.7253e-01)
Epoch: [45][ 75/246]	Loss 1.4076e-01 (1.6637e-01)
Epoch: [45][100/246]	Loss 1.8388e-01 (1.6720e-01)
Epoch: [45][125/246]	Loss 1.3465e-01 (1.6522e-01)
Epoch: [45][150/246]	Loss 2.1699e-01 (1.6330e-01)
Epoch: [45][175/246]	Loss 1.3808e-01 (1.6203e-01)
Epoch: [45][200/246]	Loss 9.5440e-02 (1.6133e-01)
Epoch: [45][225/246]	Loss 1.0992e-01 (1.6100e-01)
Checkpoint ...
[33mEpoch 46/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00002
Train ...
Epoch: [46][  0/246]	Loss 1.8115e-01 (1.8115e-01)
Epoch: [46][ 25/246]	Loss 1.0465e-01 (1.5435e-01)
Epoch: [46][ 50/246]	Loss 1.2970e-01 (1.5848e-01)
Epoch: [46][ 75/246]	Loss 1.4668e-01 (1.5792e-01)
Epoch: [46][100/246]	Loss 2.1067e-01 (1.5868e-01)
Epoch: [46][125/246]	Loss 1.3068e-01 (1.5812e-01)
Epoch: [46][150/246]	Loss 1.2783e-01 (1.5865e-01)
Epoch: [46][175/246]	Loss 1.6903e-01 (1.5819e-01)
Epoch: [46][200/246]	Loss 1.3384e-01 (1.5857e-01)
Epoch: [46][225/246]	Loss 1.6633e-01 (1.5799e-01)
Checkpoint ...
[33mEpoch 47/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00001
Train ...
Epoch: [47][  0/246]	Loss 1.5661e-01 (1.5661e-01)
Epoch: [47][ 25/246]	Loss 1.0187e-01 (1.5769e-01)
Epoch: [47][ 50/246]	Loss 1.8023e-01 (1.5854e-01)
Epoch: [47][ 75/246]	Loss 1.9834e-01 (1.5663e-01)
Epoch: [47][100/246]	Loss 1.9477e-01 (1.5563e-01)
Epoch: [47][125/246]	Loss 1.8752e-01 (1.5819e-01)
Epoch: [47][150/246]	Loss 1.4656e-01 (1.5907e-01)
Epoch: [47][175/246]	Loss 1.2389e-01 (1.5947e-01)
Epoch: [47][200/246]	Loss 1.2998e-01 (1.5842e-01)
Epoch: [47][225/246]	Loss 1.0989e-01 (1.5897e-01)
Checkpoint ...
[33mEpoch 48/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00000
Train ...
Epoch: [48][  0/246]	Loss 2.0625e-01 (2.0625e-01)
Epoch: [48][ 25/246]	Loss 1.2377e-01 (1.7421e-01)
Epoch: [48][ 50/246]	Loss 1.8610e-01 (1.6499e-01)
Epoch: [48][ 75/246]	Loss 1.5706e-01 (1.6665e-01)
Epoch: [48][100/246]	Loss 2.2588e-01 (1.6608e-01)
Epoch: [48][125/246]	Loss 1.1583e-01 (1.6489e-01)
Epoch: [48][150/246]	Loss 2.6098e-01 (1.6632e-01)
Epoch: [48][175/246]	Loss 1.0761e-01 (1.6502e-01)
Epoch: [48][200/246]	Loss 1.8758e-01 (1.6547e-01)
Epoch: [48][225/246]	Loss 1.8087e-01 (1.6571e-01)
Checkpoint ...
[33mEpoch 49/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00000
Train ...
Epoch: [49][  0/246]	Loss 1.0346e-01 (1.0346e-01)
Epoch: [49][ 25/246]	Loss 1.5155e-01 (1.6220e-01)
Epoch: [49][ 50/246]	Loss 2.2884e-01 (1.6295e-01)
Epoch: [49][ 75/246]	Loss 1.3452e-01 (1.6186e-01)
Epoch: [49][100/246]	Loss 2.6276e-01 (1.6247e-01)
Epoch: [49][125/246]	Loss 1.7015e-01 (1.6147e-01)
Epoch: [49][150/246]	Loss 1.4361e-01 (1.6204e-01)
Epoch: [49][175/246]	Loss 1.4357e-01 (1.6084e-01)
Epoch: [49][200/246]	Loss 1.1344e-01 (1.6028e-01)
Epoch: [49][225/246]	Loss 1.6558e-01 (1.6046e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Evaluate ...
Result of kNN evaluation is 97.23
Checkpoint ...
[34mFill memory bank for mining the nearest neighbors (train) ...[0m
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Mine the nearest neighbors (Top-20)
Accuracy of top-20 nearest neighbors on train set is 56.94
[34mFill memory bank for mining the nearest neighbors (val) ...[0m
Fill Memory Bank [0/247]
Fill Memory Bank [100/247]
Fill Memory Bank [200/247]
Mine the nearest neighbors (Top-5)
Accuracy of top-5 nearest neighbors on val set is 61.69
