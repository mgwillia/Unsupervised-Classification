vulcan03.umiacs.umd.edu
[31m{'setup': 'scan', 'criterion': 'scan', 'criterion_kwargs': {'entropy_weight': 5.0}, 'update_cluster_head_only': False, 'num_heads': 1, 'backbone': 'resnet50', 'train_db_name': 'pascal-pretrained-224-240', 'val_db_name': 'pascal-pretrained-224-240', 'num_classes': 240, 'num_neighbors': 60, 'augmentation_strategy': 'ours', 'augmentation_kwargs': {'crop_size': 224, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, 'num_strong_augs': 4, 'cutout_kwargs': {'n_holes': 1, 'length': 16, 'random': True}}, 'transformation_kwargs': {'crop_size': 224, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'optimizer': 'adam', 'optimizer_kwargs': {'lr': 0.0001, 'weight_decay': 0.0001}, 'epochs': 50, 'batch_size': 128, 'num_workers': 8, 'scheduler': 'constant', 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/topk-val-neighbors.npy', 'scan_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/scan', 'scan_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/scan/checkpoint.pth.tar', 'scan_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/scan/model.pth.tar', 'selflabel_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/selflabel', 'selflabel_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/selflabel/checkpoint.pth.tar', 'selflabel_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/selflabel/model.pth.tar'}[0m
[34mGet dataset and dataloaders[0m
Train transforms: Compose(
    RandomHorizontalFlip(p=0.5)
    RandomCrop(size=(224, 224), padding=None)
    <data.augment.Augment object at 0x7fda364b1b00>
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    <data.augment.Cutout object at 0x7fda364b1c50>
)
Validation transforms: Compose(
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Train samples 15774 - Val samples 15774
[34mGet model[0m
{'setup': 'scan', 'criterion': 'scan', 'criterion_kwargs': {'entropy_weight': 5.0}, 'update_cluster_head_only': False, 'num_heads': 1, 'backbone': 'resnet50', 'train_db_name': 'pascal-pretrained-224-240', 'val_db_name': 'pascal-pretrained-224-240', 'num_classes': 240, 'num_neighbors': 60, 'augmentation_strategy': 'ours', 'augmentation_kwargs': {'crop_size': 224, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, 'num_strong_augs': 4, 'cutout_kwargs': {'n_holes': 1, 'length': 16, 'random': True}}, 'transformation_kwargs': {'crop_size': 224, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'optimizer': 'adam', 'optimizer_kwargs': {'lr': 0.0001, 'weight_decay': 0.0001}, 'epochs': 50, 'batch_size': 128, 'num_workers': 8, 'scheduler': 'constant', 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/pretext/topk-val-neighbors.npy', 'scan_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/scan', 'scan_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/scan/checkpoint.pth.tar', 'scan_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/scan/model.pth.tar', 'selflabel_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/selflabel', 'selflabel_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/selflabel/checkpoint.pth.tar', 'selflabel_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/selflabel/model.pth.tar'}
loading pretrained
ClusteringModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (cluster_head): ModuleList(
    (0): Linear(in_features=2048, out_features=240, bias=True)
  )
)
[34mGet optimizer[0m
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0001
)
[34mGet loss[0m
SCANLoss(
  (softmax): Softmax(dim=1)
  (bce): BCELoss()
)
[34mRestart from checkpoint /cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-224-240/scan/checkpoint.pth.tar[0m
[34mStarting main loop[0m
[33mEpoch 25/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [24][  0/123]	Total Loss -2.3851e+01 (-2.3851e+01)	Consistency Loss 3.1896e+00 (3.1896e+00)	Entropy 5.4080e+00 (5.4080e+00)
Epoch: [24][ 25/123]	Total Loss -2.4011e+01 (-2.3871e+01)	Consistency Loss 3.0720e+00 (3.2070e+00)	Entropy 5.4167e+00 (5.4157e+00)
Epoch: [24][ 50/123]	Total Loss -2.3836e+01 (-2.3856e+01)	Consistency Loss 3.2758e+00 (3.2119e+00)	Entropy 5.4224e+00 (5.4136e+00)
Epoch: [24][ 75/123]	Total Loss -2.3868e+01 (-2.3865e+01)	Consistency Loss 3.2645e+00 (3.2027e+00)	Entropy 5.4264e+00 (5.4135e+00)
Epoch: [24][100/123]	Total Loss -2.3881e+01 (-2.3853e+01)	Consistency Loss 3.2693e+00 (3.2119e+00)	Entropy 5.4301e+00 (5.4129e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.458617687225342, 'consistency': 2.378178834915161, 'total_loss': -3.0804388523101807}], 'lowest_loss_head': 0, 'lowest_loss': -3.0804388523101807}
No new lowest loss on validation set: -3.0830 -> -3.0804
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05432990997844554, 'ARI': 0.0294224069824699, 'NMI': 0.38729973140316576, 'ACC Top-5': 0.25586408013186257, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 13), (5, 1), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 17), (13, 12), (14, 11), (15, 19), (16, 14), (17, 8), (18, 7), (19, 18)]}
Checkpoint ...
[33mEpoch 26/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [25][  0/123]	Total Loss -2.3706e+01 (-2.3706e+01)	Consistency Loss 3.3317e+00 (3.3317e+00)	Entropy 5.4075e+00 (5.4075e+00)
Epoch: [25][ 25/123]	Total Loss -2.3749e+01 (-2.3857e+01)	Consistency Loss 3.2828e+00 (3.2258e+00)	Entropy 5.4063e+00 (5.4165e+00)
Epoch: [25][ 50/123]	Total Loss -2.4123e+01 (-2.3880e+01)	Consistency Loss 2.9297e+00 (3.1972e+00)	Entropy 5.4105e+00 (5.4154e+00)
Epoch: [25][ 75/123]	Total Loss -2.3735e+01 (-2.3870e+01)	Consistency Loss 3.3119e+00 (3.2038e+00)	Entropy 5.4094e+00 (5.4148e+00)
Epoch: [25][100/123]	Total Loss -2.3844e+01 (-2.3871e+01)	Consistency Loss 3.2210e+00 (3.2030e+00)	Entropy 5.4130e+00 (5.4148e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.458983898162842, 'consistency': 2.35294508934021, 'total_loss': -3.106038808822632}], 'lowest_loss_head': 0, 'lowest_loss': -3.106038808822632}
New lowest loss on validation set: -3.0830 -> -3.1060
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05502726004818055, 'ARI': 0.030333984881701626, 'NMI': 0.38943517702928865, 'ACC Top-5': 0.2589704577152276, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 1), (4, 19), (5, 13), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 17), (13, 12), (14, 11), (15, 6), (16, 14), (17, 8), (18, 7), (19, 18)]}
Checkpoint ...
[33mEpoch 27/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [26][  0/123]	Total Loss -2.3732e+01 (-2.3732e+01)	Consistency Loss 3.3926e+00 (3.3926e+00)	Entropy 5.4250e+00 (5.4250e+00)
Epoch: [26][ 25/123]	Total Loss -2.3816e+01 (-2.3877e+01)	Consistency Loss 3.2708e+00 (3.2069e+00)	Entropy 5.4174e+00 (5.4168e+00)
Epoch: [26][ 50/123]	Total Loss -2.3974e+01 (-2.3889e+01)	Consistency Loss 3.1175e+00 (3.1838e+00)	Entropy 5.4182e+00 (5.4145e+00)
Epoch: [26][ 75/123]	Total Loss -2.3984e+01 (-2.3891e+01)	Consistency Loss 3.1085e+00 (3.1791e+00)	Entropy 5.4185e+00 (5.4140e+00)
Epoch: [26][100/123]	Total Loss -2.3807e+01 (-2.3879e+01)	Consistency Loss 3.2087e+00 (3.1883e+00)	Entropy 5.4032e+00 (5.4134e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.456206321716309, 'consistency': 2.328659772872925, 'total_loss': -3.127546548843384}], 'lowest_loss_head': 0, 'lowest_loss': -3.127546548843384}
New lowest loss on validation set: -3.1060 -> -3.1275
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.056485355648535567, 'ARI': 0.031126433522925618, 'NMI': 0.39143304754358216, 'ACC Top-5': 0.25586408013186257, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 19), (5, 1), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 17), (13, 12), (14, 11), (15, 13), (16, 14), (17, 8), (18, 7), (19, 18)]}
Checkpoint ...
[33mEpoch 28/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [27][  0/123]	Total Loss -2.3948e+01 (-2.3948e+01)	Consistency Loss 3.0991e+00 (3.0991e+00)	Entropy 5.4094e+00 (5.4094e+00)
Epoch: [27][ 25/123]	Total Loss -2.3838e+01 (-2.3887e+01)	Consistency Loss 3.2266e+00 (3.1758e+00)	Entropy 5.4129e+00 (5.4126e+00)
Epoch: [27][ 50/123]	Total Loss -2.3937e+01 (-2.3884e+01)	Consistency Loss 3.0930e+00 (3.1807e+00)	Entropy 5.4059e+00 (5.4129e+00)
Epoch: [27][ 75/123]	Total Loss -2.3792e+01 (-2.3892e+01)	Consistency Loss 3.2509e+00 (3.1730e+00)	Entropy 5.4086e+00 (5.4130e+00)
Epoch: [27][100/123]	Total Loss -2.3829e+01 (-2.3884e+01)	Consistency Loss 3.2498e+00 (3.1818e+00)	Entropy 5.4157e+00 (5.4131e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.456052303314209, 'consistency': 2.316847324371338, 'total_loss': -3.139204978942871}], 'lowest_loss_head': 0, 'lowest_loss': -3.139204978942871}
New lowest loss on validation set: -3.1275 -> -3.1392
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05718270571827057, 'ARI': 0.03128386596144359, 'NMI': 0.39128179317345474, 'ACC Top-5': 0.25484975275770255, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 13), (4, 18), (5, 4), (6, 9), (7, 0), (8, 1), (9, 5), (10, 16), (11, 15), (12, 17), (13, 12), (14, 11), (15, 6), (16, 14), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[33mEpoch 29/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [28][  0/123]	Total Loss -2.3866e+01 (-2.3866e+01)	Consistency Loss 3.1992e+00 (3.1992e+00)	Entropy 5.4131e+00 (5.4131e+00)
Epoch: [28][ 25/123]	Total Loss -2.3837e+01 (-2.3869e+01)	Consistency Loss 3.2130e+00 (3.1970e+00)	Entropy 5.4101e+00 (5.4133e+00)
Epoch: [28][ 50/123]	Total Loss -2.3845e+01 (-2.3868e+01)	Consistency Loss 3.1642e+00 (3.2028e+00)	Entropy 5.4018e+00 (5.4142e+00)
Epoch: [28][ 75/123]	Total Loss -2.3829e+01 (-2.3866e+01)	Consistency Loss 3.2073e+00 (3.1983e+00)	Entropy 5.4072e+00 (5.4128e+00)
Epoch: [28][100/123]	Total Loss -2.4051e+01 (-2.3876e+01)	Consistency Loss 3.0390e+00 (3.1907e+00)	Entropy 5.4179e+00 (5.4132e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.452469825744629, 'consistency': 2.3077523708343506, 'total_loss': -3.1447174549102783}], 'lowest_loss_head': 0, 'lowest_loss': -3.1447174549102783}
New lowest loss on validation set: -3.1392 -> -3.1447
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.0601622923798656, 'ARI': 0.03260070256414624, 'NMI': 0.39116285550588725, 'ACC Top-5': 0.2668948903258527, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 18), (5, 4), (6, 9), (7, 0), (8, 1), (9, 5), (10, 16), (11, 15), (12, 19), (13, 17), (14, 11), (15, 12), (16, 14), (17, 8), (18, 7), (19, 13)]}
Checkpoint ...
[33mEpoch 30/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [29][  0/123]	Total Loss -2.3726e+01 (-2.3726e+01)	Consistency Loss 3.3267e+00 (3.3267e+00)	Entropy 5.4106e+00 (5.4106e+00)
Epoch: [29][ 25/123]	Total Loss -2.3927e+01 (-2.3885e+01)	Consistency Loss 3.1991e+00 (3.1853e+00)	Entropy 5.4252e+00 (5.4141e+00)
Epoch: [29][ 50/123]	Total Loss -2.4007e+01 (-2.3889e+01)	Consistency Loss 3.0846e+00 (3.1697e+00)	Entropy 5.4184e+00 (5.4117e+00)
Epoch: [29][ 75/123]	Total Loss -2.3971e+01 (-2.3879e+01)	Consistency Loss 3.1588e+00 (3.1835e+00)	Entropy 5.4261e+00 (5.4125e+00)
Epoch: [29][100/123]	Total Loss -2.3807e+01 (-2.3873e+01)	Consistency Loss 3.2913e+00 (3.1881e+00)	Entropy 5.4197e+00 (5.4122e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.459197044372559, 'consistency': 2.3096866607666016, 'total_loss': -3.149510383605957}], 'lowest_loss_head': 0, 'lowest_loss': -3.149510383605957}
New lowest loss on validation set: -3.1447 -> -3.1495
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.055280841891720556, 'ARI': 0.03004844625665899, 'NMI': 0.38798069173728406, 'ACC Top-5': 0.25852668948903257, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 18), (5, 1), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 13), (13, 17), (14, 11), (15, 12), (16, 14), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[33mEpoch 31/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [30][  0/123]	Total Loss -2.4052e+01 (-2.4052e+01)	Consistency Loss 3.0526e+00 (3.0526e+00)	Entropy 5.4210e+00 (5.4210e+00)
Epoch: [30][ 25/123]	Total Loss -2.3922e+01 (-2.3929e+01)	Consistency Loss 3.2162e+00 (3.1578e+00)	Entropy 5.4276e+00 (5.4173e+00)
Epoch: [30][ 50/123]	Total Loss -2.3862e+01 (-2.3918e+01)	Consistency Loss 3.1810e+00 (3.1574e+00)	Entropy 5.4086e+00 (5.4150e+00)
Epoch: [30][ 75/123]	Total Loss -2.4060e+01 (-2.3920e+01)	Consistency Loss 3.0243e+00 (3.1548e+00)	Entropy 5.4169e+00 (5.4151e+00)
Epoch: [30][100/123]	Total Loss -2.3828e+01 (-2.3911e+01)	Consistency Loss 3.2052e+00 (3.1600e+00)	Entropy 5.4066e+00 (5.4142e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.459476947784424, 'consistency': 2.298272132873535, 'total_loss': -3.1612048149108887}], 'lowest_loss_head': 0, 'lowest_loss': -3.1612048149108887}
New lowest loss on validation set: -3.1495 -> -3.1612
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05749968302269558, 'ARI': 0.030318089668008603, 'NMI': 0.39076870978155226, 'ACC Top-5': 0.2601115760111576, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 12), (5, 1), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 19), (13, 17), (14, 11), (15, 13), (16, 14), (17, 8), (18, 7), (19, 18)]}
Checkpoint ...
[33mEpoch 32/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [31][  0/123]	Total Loss -2.3850e+01 (-2.3850e+01)	Consistency Loss 3.2558e+00 (3.2558e+00)	Entropy 5.4212e+00 (5.4212e+00)
Epoch: [31][ 25/123]	Total Loss -2.3886e+01 (-2.3885e+01)	Consistency Loss 3.1257e+00 (3.1812e+00)	Entropy 5.4023e+00 (5.4132e+00)
Epoch: [31][ 50/123]	Total Loss -2.3852e+01 (-2.3887e+01)	Consistency Loss 3.2693e+00 (3.1738e+00)	Entropy 5.4243e+00 (5.4121e+00)
Epoch: [31][ 75/123]	Total Loss -2.3973e+01 (-2.3889e+01)	Consistency Loss 3.1491e+00 (3.1767e+00)	Entropy 5.4245e+00 (5.4131e+00)
Epoch: [31][100/123]	Total Loss -2.3993e+01 (-2.3888e+01)	Consistency Loss 3.0993e+00 (3.1689e+00)	Entropy 5.4185e+00 (5.4115e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.461752891540527, 'consistency': 2.2987782955169678, 'total_loss': -3.1629745960235596}], 'lowest_loss_head': 0, 'lowest_loss': -3.1629745960235596}
New lowest loss on validation set: -3.1612 -> -3.1630
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05534423735260555, 'ARI': 0.030743312754524365, 'NMI': 0.38930325446845776, 'ACC Top-5': 0.2590338531761126, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 12), (5, 13), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 8), (13, 17), (14, 11), (15, 19), (16, 14), (17, 1), (18, 7), (19, 18)]}
Checkpoint ...
[33mEpoch 33/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [32][  0/123]	Total Loss -2.3720e+01 (-2.3720e+01)	Consistency Loss 3.3211e+00 (3.3211e+00)	Entropy 5.4082e+00 (5.4082e+00)
Epoch: [32][ 25/123]	Total Loss -2.3989e+01 (-2.3906e+01)	Consistency Loss 3.1412e+00 (3.1638e+00)	Entropy 5.4261e+00 (5.4139e+00)
Epoch: [32][ 50/123]	Total Loss -2.3906e+01 (-2.3906e+01)	Consistency Loss 3.0708e+00 (3.1573e+00)	Entropy 5.3954e+00 (5.4127e+00)
Epoch: [32][ 75/123]	Total Loss -2.3878e+01 (-2.3909e+01)	Consistency Loss 3.1826e+00 (3.1564e+00)	Entropy 5.4121e+00 (5.4131e+00)
Epoch: [32][100/123]	Total Loss -2.3968e+01 (-2.3909e+01)	Consistency Loss 3.0714e+00 (3.1533e+00)	Entropy 5.4078e+00 (5.4124e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.455070972442627, 'consistency': 2.290118455886841, 'total_loss': -3.164952516555786}], 'lowest_loss_head': 0, 'lowest_loss': -3.164952516555786}
New lowest loss on validation set: -3.1630 -> -3.1650
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05794345124889058, 'ARI': 0.031580793154129275, 'NMI': 0.3918259663194145, 'ACC Top-5': 0.2684163813870927, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 19), (5, 16), (6, 12), (7, 0), (8, 4), (9, 5), (10, 9), (11, 15), (12, 13), (13, 17), (14, 11), (15, 1), (16, 14), (17, 8), (18, 7), (19, 18)]}
Checkpoint ...
[33mEpoch 34/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [33][  0/123]	Total Loss -2.4032e+01 (-2.4032e+01)	Consistency Loss 3.0744e+00 (3.0744e+00)	Entropy 5.4212e+00 (5.4212e+00)
Epoch: [33][ 25/123]	Total Loss -2.3838e+01 (-2.3875e+01)	Consistency Loss 3.2382e+00 (3.1910e+00)	Entropy 5.4152e+00 (5.4131e+00)
Epoch: [33][ 50/123]	Total Loss -2.3876e+01 (-2.3895e+01)	Consistency Loss 3.1568e+00 (3.1740e+00)	Entropy 5.4066e+00 (5.4139e+00)
Epoch: [33][ 75/123]	Total Loss -2.3979e+01 (-2.3902e+01)	Consistency Loss 3.0454e+00 (3.1636e+00)	Entropy 5.4049e+00 (5.4131e+00)
Epoch: [33][100/123]	Total Loss -2.3733e+01 (-2.3900e+01)	Consistency Loss 3.3873e+00 (3.1625e+00)	Entropy 5.4240e+00 (5.4124e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.453747272491455, 'consistency': 2.298865795135498, 'total_loss': -3.154881477355957}], 'lowest_loss_head': 0, 'lowest_loss': -3.154881477355957}
No new lowest loss on validation set: -3.1650 -> -3.1549
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05578800557880056, 'ARI': 0.030121216654029125, 'NMI': 0.38819685462037684, 'ACC Top-5': 0.2619500443768226, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 13), (5, 4), (6, 12), (7, 0), (8, 1), (9, 5), (10, 9), (11, 15), (12, 17), (13, 16), (14, 11), (15, 19), (16, 14), (17, 8), (18, 7), (19, 18)]}
Checkpoint ...
[33mEpoch 35/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [34][  0/123]	Total Loss -2.3546e+01 (-2.3546e+01)	Consistency Loss 3.5022e+00 (3.5022e+00)	Entropy 5.4096e+00 (5.4096e+00)
Epoch: [34][ 25/123]	Total Loss -2.3840e+01 (-2.3890e+01)	Consistency Loss 3.1954e+00 (3.1700e+00)	Entropy 5.4070e+00 (5.4120e+00)
Epoch: [34][ 50/123]	Total Loss -2.3890e+01 (-2.3911e+01)	Consistency Loss 3.1328e+00 (3.1560e+00)	Entropy 5.4046e+00 (5.4133e+00)
Epoch: [34][ 75/123]	Total Loss -2.3766e+01 (-2.3912e+01)	Consistency Loss 3.1996e+00 (3.1508e+00)	Entropy 5.3932e+00 (5.4126e+00)
Epoch: [34][100/123]	Total Loss -2.3763e+01 (-2.3906e+01)	Consistency Loss 3.2134e+00 (3.1537e+00)	Entropy 5.3953e+00 (5.4119e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.4585137367248535, 'consistency': 2.3018503189086914, 'total_loss': -3.156663417816162}], 'lowest_loss_head': 0, 'lowest_loss': -3.156663417816162}
No new lowest loss on validation set: -3.1650 -> -3.1567
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05483707366552555, 'ARI': 0.029823551762817944, 'NMI': 0.38864041771213, 'ACC Top-5': 0.2566882211233676, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 18), (5, 13), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 17), (13, 14), (14, 11), (15, 12), (16, 1), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[33mEpoch 36/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [35][  0/123]	Total Loss -2.3819e+01 (-2.3819e+01)	Consistency Loss 3.3254e+00 (3.3254e+00)	Entropy 5.4289e+00 (5.4289e+00)
Epoch: [35][ 25/123]	Total Loss -2.3845e+01 (-2.3921e+01)	Consistency Loss 3.2899e+00 (3.1391e+00)	Entropy 5.4270e+00 (5.4120e+00)
Epoch: [35][ 50/123]	Total Loss -2.3844e+01 (-2.3907e+01)	Consistency Loss 3.1544e+00 (3.1515e+00)	Entropy 5.3997e+00 (5.4118e+00)
Epoch: [35][ 75/123]	Total Loss -2.3722e+01 (-2.3917e+01)	Consistency Loss 3.2901e+00 (3.1444e+00)	Entropy 5.4024e+00 (5.4122e+00)
Epoch: [35][100/123]	Total Loss -2.4022e+01 (-2.3911e+01)	Consistency Loss 3.1117e+00 (3.1497e+00)	Entropy 5.4268e+00 (5.4121e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.459057331085205, 'consistency': 2.28861403465271, 'total_loss': -3.170443296432495}], 'lowest_loss_head': 0, 'lowest_loss': -3.170443296432495}
New lowest loss on validation set: -3.1650 -> -3.1704
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05921136046659059, 'ARI': 0.030452854223821244, 'NMI': 0.38968427383508425, 'ACC Top-5': 0.2618866489159376, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 12), (5, 1), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 13), (13, 17), (14, 11), (15, 18), (16, 14), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[33mEpoch 37/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [36][  0/123]	Total Loss -2.3908e+01 (-2.3908e+01)	Consistency Loss 3.1792e+00 (3.1792e+00)	Entropy 5.4173e+00 (5.4173e+00)
Epoch: [36][ 25/123]	Total Loss -2.4011e+01 (-2.3910e+01)	Consistency Loss 3.0330e+00 (3.1353e+00)	Entropy 5.4088e+00 (5.4090e+00)
Epoch: [36][ 50/123]	Total Loss -2.3954e+01 (-2.3920e+01)	Consistency Loss 3.1669e+00 (3.1255e+00)	Entropy 5.4242e+00 (5.4092e+00)
Epoch: [36][ 75/123]	Total Loss -2.4056e+01 (-2.3917e+01)	Consistency Loss 2.9292e+00 (3.1314e+00)	Entropy 5.3970e+00 (5.4097e+00)
Epoch: [36][100/123]	Total Loss -2.3972e+01 (-2.3925e+01)	Consistency Loss 3.0987e+00 (3.1247e+00)	Entropy 5.4141e+00 (5.4100e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.4601593017578125, 'consistency': 2.2732748985290527, 'total_loss': -3.1868844032287598}], 'lowest_loss_head': 0, 'lowest_loss': -3.1868844032287598}
New lowest loss on validation set: -3.1704 -> -3.1869
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05952833777101559, 'ARI': 0.030921525799352288, 'NMI': 0.3921276105897319, 'ACC Top-5': 0.26619754025611764, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 18), (5, 13), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 12), (13, 17), (14, 11), (15, 1), (16, 14), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[33mEpoch 38/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [37][  0/123]	Total Loss -2.3817e+01 (-2.3817e+01)	Consistency Loss 3.2970e+00 (3.2970e+00)	Entropy 5.4228e+00 (5.4228e+00)
Epoch: [37][ 25/123]	Total Loss -2.3829e+01 (-2.3928e+01)	Consistency Loss 3.1784e+00 (3.1318e+00)	Entropy 5.4015e+00 (5.4119e+00)
Epoch: [37][ 50/123]	Total Loss -2.3966e+01 (-2.3925e+01)	Consistency Loss 3.0477e+00 (3.1286e+00)	Entropy 5.4027e+00 (5.4108e+00)
Epoch: [37][ 75/123]	Total Loss -2.3944e+01 (-2.3926e+01)	Consistency Loss 3.1656e+00 (3.1232e+00)	Entropy 5.4219e+00 (5.4099e+00)
Epoch: [37][100/123]	Total Loss -2.4026e+01 (-2.3926e+01)	Consistency Loss 3.0754e+00 (3.1248e+00)	Entropy 5.4203e+00 (5.4101e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.460167407989502, 'consistency': 2.274013042449951, 'total_loss': -3.186154365539551}], 'lowest_loss_head': 0, 'lowest_loss': -3.186154365539551}
No new lowest loss on validation set: -3.1869 -> -3.1862
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05452009636110054, 'ARI': 0.030219281588865964, 'NMI': 0.3907736216591562, 'ACC Top-5': 0.25250412070495754, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 1), (4, 18), (5, 4), (6, 9), (7, 0), (8, 13), (9, 5), (10, 16), (11, 15), (12, 17), (13, 12), (14, 11), (15, 6), (16, 14), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[33mEpoch 39/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [38][  0/123]	Total Loss -2.3972e+01 (-2.3972e+01)	Consistency Loss 3.1061e+00 (3.1061e+00)	Entropy 5.4157e+00 (5.4157e+00)
Epoch: [38][ 25/123]	Total Loss -2.4051e+01 (-2.3945e+01)	Consistency Loss 2.9693e+00 (3.1141e+00)	Entropy 5.4041e+00 (5.4117e+00)
Epoch: [38][ 50/123]	Total Loss -2.4079e+01 (-2.3925e+01)	Consistency Loss 2.9948e+00 (3.1237e+00)	Entropy 5.4148e+00 (5.4097e+00)
Epoch: [38][ 75/123]	Total Loss -2.4067e+01 (-2.3945e+01)	Consistency Loss 3.0626e+00 (3.1047e+00)	Entropy 5.4259e+00 (5.4100e+00)
Epoch: [38][100/123]	Total Loss -2.3798e+01 (-2.3938e+01)	Consistency Loss 3.1848e+00 (3.1102e+00)	Entropy 5.3966e+00 (5.4096e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.4550065994262695, 'consistency': 2.2724645137786865, 'total_loss': -3.182542085647583}], 'lowest_loss_head': 0, 'lowest_loss': -3.182542085647583}
No new lowest loss on validation set: -3.1869 -> -3.1825
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.06244452897172562, 'ARI': 0.030994157440359247, 'NMI': 0.39288543603699805, 'ACC Top-5': 0.2698110815265627, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 18), (5, 4), (6, 9), (7, 0), (8, 12), (9, 5), (10, 16), (11, 15), (12, 17), (13, 13), (14, 11), (15, 1), (16, 14), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[33mEpoch 40/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [39][  0/123]	Total Loss -2.3699e+01 (-2.3699e+01)	Consistency Loss 3.3096e+00 (3.3096e+00)	Entropy 5.4017e+00 (5.4017e+00)
Epoch: [39][ 25/123]	Total Loss -2.3914e+01 (-2.3914e+01)	Consistency Loss 3.1456e+00 (3.1283e+00)	Entropy 5.4119e+00 (5.4085e+00)
Epoch: [39][ 50/123]	Total Loss -2.4005e+01 (-2.3927e+01)	Consistency Loss 3.0894e+00 (3.1260e+00)	Entropy 5.4188e+00 (5.4105e+00)
Epoch: [39][ 75/123]	Total Loss -2.3951e+01 (-2.3928e+01)	Consistency Loss 3.0941e+00 (3.1159e+00)	Entropy 5.4090e+00 (5.4088e+00)
Epoch: [39][100/123]	Total Loss -2.3854e+01 (-2.3929e+01)	Consistency Loss 3.1081e+00 (3.1160e+00)	Entropy 5.3923e+00 (5.4091e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.463737487792969, 'consistency': 2.2839083671569824, 'total_loss': -3.1798291206359863}], 'lowest_loss_head': 0, 'lowest_loss': -3.1798291206359863}
No new lowest loss on validation set: -3.1869 -> -3.1798
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05604158742234056, 'ARI': 0.02937743679827041, 'NMI': 0.3889334954170472, 'ACC Top-5': 0.2511094205654875, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 18), (5, 4), (6, 9), (7, 0), (8, 12), (9, 5), (10, 16), (11, 15), (12, 17), (13, 14), (14, 11), (15, 13), (16, 1), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[33mEpoch 41/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [40][  0/123]	Total Loss -2.3981e+01 (-2.3981e+01)	Consistency Loss 3.0626e+00 (3.0626e+00)	Entropy 5.4087e+00 (5.4087e+00)
Epoch: [40][ 25/123]	Total Loss -2.3935e+01 (-2.3939e+01)	Consistency Loss 3.1433e+00 (3.1162e+00)	Entropy 5.4157e+00 (5.4110e+00)
Epoch: [40][ 50/123]	Total Loss -2.3913e+01 (-2.3926e+01)	Consistency Loss 3.1287e+00 (3.1301e+00)	Entropy 5.4083e+00 (5.4111e+00)
Epoch: [40][ 75/123]	Total Loss -2.3831e+01 (-2.3933e+01)	Consistency Loss 3.1903e+00 (3.1183e+00)	Entropy 5.4042e+00 (5.4102e+00)
Epoch: [40][100/123]	Total Loss -2.3927e+01 (-2.3927e+01)	Consistency Loss 3.0944e+00 (3.1218e+00)	Entropy 5.4043e+00 (5.4097e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.458752632141113, 'consistency': 2.2451560497283936, 'total_loss': -3.2135965824127197}], 'lowest_loss_head': 0, 'lowest_loss': -3.2135965824127197}
New lowest loss on validation set: -3.1869 -> -3.2136
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.0594015468492456, 'ARI': 0.030556693040526994, 'NMI': 0.39095217589266773, 'ACC Top-5': 0.2659439584125777, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 18), (5, 1), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 17), (13, 12), (14, 11), (15, 13), (16, 14), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[33mEpoch 42/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [41][  0/123]	Total Loss -2.3996e+01 (-2.3996e+01)	Consistency Loss 3.0912e+00 (3.0912e+00)	Entropy 5.4174e+00 (5.4174e+00)
Epoch: [41][ 25/123]	Total Loss -2.4035e+01 (-2.3946e+01)	Consistency Loss 3.0623e+00 (3.1055e+00)	Entropy 5.4194e+00 (5.4104e+00)
Epoch: [41][ 50/123]	Total Loss -2.3862e+01 (-2.3936e+01)	Consistency Loss 3.2308e+00 (3.1103e+00)	Entropy 5.4185e+00 (5.4093e+00)
Epoch: [41][ 75/123]	Total Loss -2.4116e+01 (-2.3941e+01)	Consistency Loss 2.9026e+00 (3.1059e+00)	Entropy 5.4037e+00 (5.4094e+00)
Epoch: [41][100/123]	Total Loss -2.4074e+01 (-2.3946e+01)	Consistency Loss 2.9003e+00 (3.1007e+00)	Entropy 5.3949e+00 (5.4094e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.458685398101807, 'consistency': 2.271305799484253, 'total_loss': -3.1873795986175537}], 'lowest_loss_head': 0, 'lowest_loss': -3.1873795986175537}
No new lowest loss on validation set: -3.2136 -> -3.1874
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.056548751109420564, 'ARI': 0.029176962361279268, 'NMI': 0.38850512090882133, 'ACC Top-5': 0.2618866489159376, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 1), (4, 18), (5, 13), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 17), (13, 12), (14, 11), (15, 6), (16, 14), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[33mEpoch 43/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [42][  0/123]	Total Loss -2.3967e+01 (-2.3967e+01)	Consistency Loss 3.0722e+00 (3.0722e+00)	Entropy 5.4078e+00 (5.4078e+00)
Epoch: [42][ 25/123]	Total Loss -2.3840e+01 (-2.3913e+01)	Consistency Loss 3.2204e+00 (3.1217e+00)	Entropy 5.4121e+00 (5.4069e+00)
Epoch: [42][ 50/123]	Total Loss -2.3895e+01 (-2.3935e+01)	Consistency Loss 3.1272e+00 (3.1080e+00)	Entropy 5.4045e+00 (5.4087e+00)
Epoch: [42][ 75/123]	Total Loss -2.3920e+01 (-2.3943e+01)	Consistency Loss 3.0922e+00 (3.1078e+00)	Entropy 5.4025e+00 (5.4102e+00)
Epoch: [42][100/123]	Total Loss -2.3909e+01 (-2.3939e+01)	Consistency Loss 3.0143e+00 (3.1094e+00)	Entropy 5.3847e+00 (5.4096e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.46078634262085, 'consistency': 2.258009195327759, 'total_loss': -3.202777147293091}], 'lowest_loss_head': 0, 'lowest_loss': -3.202777147293091}
No new lowest loss on validation set: -3.2136 -> -3.2028
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05914796500570559, 'ARI': 0.029964547603938876, 'NMI': 0.3899265883344843, 'ACC Top-5': 0.2687967541524027, 'hungarian_match': [(0, 10), (1, 1), (2, 2), (3, 6), (4, 13), (5, 19), (6, 9), (7, 0), (8, 18), (9, 5), (10, 16), (11, 15), (12, 17), (13, 12), (14, 11), (15, 4), (16, 14), (17, 8), (18, 7), (19, 3)]}
Checkpoint ...
[33mEpoch 44/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [43][  0/123]	Total Loss -2.3721e+01 (-2.3721e+01)	Consistency Loss 3.3239e+00 (3.3239e+00)	Entropy 5.4090e+00 (5.4090e+00)
Epoch: [43][ 25/123]	Total Loss -2.3923e+01 (-2.3952e+01)	Consistency Loss 3.1455e+00 (3.0984e+00)	Entropy 5.4138e+00 (5.4101e+00)
Epoch: [43][ 50/123]	Total Loss -2.3937e+01 (-2.3946e+01)	Consistency Loss 3.1013e+00 (3.0982e+00)	Entropy 5.4076e+00 (5.4088e+00)
Epoch: [43][ 75/123]	Total Loss -2.3840e+01 (-2.3946e+01)	Consistency Loss 3.2313e+00 (3.0982e+00)	Entropy 5.4142e+00 (5.4088e+00)
Epoch: [43][100/123]	Total Loss -2.3930e+01 (-2.3948e+01)	Consistency Loss 3.1457e+00 (3.0970e+00)	Entropy 5.4151e+00 (5.4091e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.457098007202148, 'consistency': 2.2578017711639404, 'total_loss': -3.199296236038208}], 'lowest_loss_head': 0, 'lowest_loss': -3.199296236038208}
No new lowest loss on validation set: -3.2136 -> -3.1993
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.058260428553315584, 'ARI': 0.02989034052610404, 'NMI': 0.38934684695449917, 'ACC Top-5': 0.2608723215417776, 'hungarian_match': [(0, 10), (1, 1), (2, 2), (3, 19), (4, 18), (5, 13), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 17), (13, 12), (14, 11), (15, 6), (16, 14), (17, 8), (18, 7), (19, 3)]}
Checkpoint ...
[33mEpoch 45/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [44][  0/123]	Total Loss -2.4012e+01 (-2.4012e+01)	Consistency Loss 3.0331e+00 (3.0331e+00)	Entropy 5.4090e+00 (5.4090e+00)
Epoch: [44][ 25/123]	Total Loss -2.3918e+01 (-2.3952e+01)	Consistency Loss 3.0869e+00 (3.0758e+00)	Entropy 5.4010e+00 (5.4055e+00)
Epoch: [44][ 50/123]	Total Loss -2.3918e+01 (-2.3959e+01)	Consistency Loss 3.1054e+00 (3.0774e+00)	Entropy 5.4047e+00 (5.4072e+00)
Epoch: [44][ 75/123]	Total Loss -2.3925e+01 (-2.3958e+01)	Consistency Loss 3.1617e+00 (3.0830e+00)	Entropy 5.4173e+00 (5.4082e+00)
Epoch: [44][100/123]	Total Loss -2.3979e+01 (-2.3958e+01)	Consistency Loss 3.0849e+00 (3.0832e+00)	Entropy 5.4127e+00 (5.4083e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.456850051879883, 'consistency': 2.255753517150879, 'total_loss': -3.201096534729004}], 'lowest_loss_head': 0, 'lowest_loss': -3.201096534729004}
No new lowest loss on validation set: -3.2136 -> -3.2011
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05629516926588056, 'ARI': 0.030510571735776995, 'NMI': 0.3923947460786443, 'ACC Top-5': 0.25561049828832255, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 1), (4, 19), (5, 13), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 17), (13, 12), (14, 11), (15, 6), (16, 14), (17, 8), (18, 7), (19, 18)]}
Checkpoint ...
[33mEpoch 46/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [45][  0/123]	Total Loss -2.4092e+01 (-2.4092e+01)	Consistency Loss 2.9722e+00 (2.9722e+00)	Entropy 5.4129e+00 (5.4129e+00)
Epoch: [45][ 25/123]	Total Loss -2.3801e+01 (-2.3979e+01)	Consistency Loss 3.1824e+00 (3.0831e+00)	Entropy 5.3966e+00 (5.4124e+00)
Epoch: [45][ 50/123]	Total Loss -2.3979e+01 (-2.3957e+01)	Consistency Loss 2.9739e+00 (3.0810e+00)	Entropy 5.3906e+00 (5.4076e+00)
Epoch: [45][ 75/123]	Total Loss -2.3905e+01 (-2.3951e+01)	Consistency Loss 3.0879e+00 (3.0857e+00)	Entropy 5.3986e+00 (5.4073e+00)
Epoch: [45][100/123]	Total Loss -2.3722e+01 (-2.3958e+01)	Consistency Loss 3.3293e+00 (3.0817e+00)	Entropy 5.4103e+00 (5.4079e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.459578037261963, 'consistency': 2.2299976348876953, 'total_loss': -3.2295804023742676}], 'lowest_loss_head': 0, 'lowest_loss': -3.2295804023742676}
New lowest loss on validation set: -3.2136 -> -3.2296
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05578800557880056, 'ARI': 0.029214392193142508, 'NMI': 0.39056764170220803, 'ACC Top-5': 0.26486623557753264, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 1), (4, 12), (5, 13), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 17), (13, 19), (14, 11), (15, 6), (16, 14), (17, 8), (18, 7), (19, 18)]}
Checkpoint ...
[33mEpoch 47/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [46][  0/123]	Total Loss -2.4042e+01 (-2.4042e+01)	Consistency Loss 2.9492e+00 (2.9492e+00)	Entropy 5.3983e+00 (5.3983e+00)
Epoch: [46][ 25/123]	Total Loss -2.3906e+01 (-2.3932e+01)	Consistency Loss 3.1828e+00 (3.1043e+00)	Entropy 5.4177e+00 (5.4073e+00)
Epoch: [46][ 50/123]	Total Loss -2.3925e+01 (-2.3940e+01)	Consistency Loss 3.0988e+00 (3.0922e+00)	Entropy 5.4048e+00 (5.4064e+00)
Epoch: [46][ 75/123]	Total Loss -2.3962e+01 (-2.3956e+01)	Consistency Loss 3.0947e+00 (3.0824e+00)	Entropy 5.4113e+00 (5.4076e+00)
Epoch: [46][100/123]	Total Loss -2.3796e+01 (-2.3947e+01)	Consistency Loss 3.2541e+00 (3.0946e+00)	Entropy 5.4101e+00 (5.4083e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.461372375488281, 'consistency': 2.257561206817627, 'total_loss': -3.2038111686706543}], 'lowest_loss_head': 0, 'lowest_loss': -3.2038111686706543}
No new lowest loss on validation set: -3.2296 -> -3.2038
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05692912387473057, 'ARI': 0.02899145616696528, 'NMI': 0.38410278834456774, 'ACC Top-5': 0.25681501204513757, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 1), (4, 19), (5, 13), (6, 12), (7, 0), (8, 4), (9, 5), (10, 9), (11, 15), (12, 17), (13, 16), (14, 11), (15, 6), (16, 14), (17, 8), (18, 7), (19, 18)]}
Checkpoint ...
[33mEpoch 48/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [47][  0/123]	Total Loss -2.4003e+01 (-2.4003e+01)	Consistency Loss 3.0362e+00 (3.0362e+00)	Entropy 5.4078e+00 (5.4078e+00)
Epoch: [47][ 25/123]	Total Loss -2.3961e+01 (-2.3988e+01)	Consistency Loss 3.0484e+00 (3.0639e+00)	Entropy 5.4019e+00 (5.4103e+00)
Epoch: [47][ 50/123]	Total Loss -2.3647e+01 (-2.3947e+01)	Consistency Loss 3.3977e+00 (3.0936e+00)	Entropy 5.4089e+00 (5.4081e+00)
Epoch: [47][ 75/123]	Total Loss -2.3927e+01 (-2.3951e+01)	Consistency Loss 3.0802e+00 (3.0917e+00)	Entropy 5.4015e+00 (5.4085e+00)
Epoch: [47][100/123]	Total Loss -2.3872e+01 (-2.3953e+01)	Consistency Loss 3.2233e+00 (3.0893e+00)	Entropy 5.4190e+00 (5.4085e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.456374168395996, 'consistency': 2.240964412689209, 'total_loss': -3.215409755706787}], 'lowest_loss_head': 0, 'lowest_loss': -3.215409755706787}
No new lowest loss on validation set: -3.2296 -> -3.2154
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.056168378344110564, 'ARI': 0.029666735435492638, 'NMI': 0.3852720698536632, 'ACC Top-5': 0.2514897933307975, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 1), (4, 19), (5, 4), (6, 12), (7, 0), (8, 16), (9, 5), (10, 9), (11, 15), (12, 17), (13, 13), (14, 11), (15, 6), (16, 14), (17, 8), (18, 7), (19, 18)]}
Checkpoint ...
[33mEpoch 49/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [48][  0/123]	Total Loss -2.3737e+01 (-2.3737e+01)	Consistency Loss 3.2643e+00 (3.2643e+00)	Entropy 5.4003e+00 (5.4003e+00)
Epoch: [48][ 25/123]	Total Loss -2.3899e+01 (-2.3928e+01)	Consistency Loss 3.1654e+00 (3.1118e+00)	Entropy 5.4128e+00 (5.4080e+00)
Epoch: [48][ 50/123]	Total Loss -2.4146e+01 (-2.3958e+01)	Consistency Loss 2.9269e+00 (3.0788e+00)	Entropy 5.4145e+00 (5.4075e+00)
Epoch: [48][ 75/123]	Total Loss -2.4009e+01 (-2.3954e+01)	Consistency Loss 3.0500e+00 (3.0826e+00)	Entropy 5.4119e+00 (5.4073e+00)
Epoch: [48][100/123]	Total Loss -2.3947e+01 (-2.3956e+01)	Consistency Loss 3.0695e+00 (3.0817e+00)	Entropy 5.4034e+00 (5.4075e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.452986717224121, 'consistency': 2.2445383071899414, 'total_loss': -3.2084484100341797}], 'lowest_loss_head': 0, 'lowest_loss': -3.2084484100341797}
No new lowest loss on validation set: -3.2296 -> -3.2084
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.06092303791048561, 'ARI': 0.03014825757535647, 'NMI': 0.3876118589021138, 'ACC Top-5': 0.26397869912514266, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 1), (4, 18), (5, 13), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 17), (13, 12), (14, 11), (15, 6), (16, 14), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[33mEpoch 50/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [49][  0/123]	Total Loss -2.3916e+01 (-2.3916e+01)	Consistency Loss 3.1036e+00 (3.1036e+00)	Entropy 5.4039e+00 (5.4039e+00)
Epoch: [49][ 25/123]	Total Loss -2.3999e+01 (-2.3973e+01)	Consistency Loss 3.0325e+00 (3.0744e+00)	Entropy 5.4063e+00 (5.4095e+00)
Epoch: [49][ 50/123]	Total Loss -2.4081e+01 (-2.3963e+01)	Consistency Loss 2.9966e+00 (3.0817e+00)	Entropy 5.4155e+00 (5.4089e+00)
Epoch: [49][ 75/123]	Total Loss -2.3914e+01 (-2.3960e+01)	Consistency Loss 3.1387e+00 (3.0848e+00)	Entropy 5.4105e+00 (5.4090e+00)
Epoch: [49][100/123]	Total Loss -2.3964e+01 (-2.3963e+01)	Consistency Loss 3.0437e+00 (3.0787e+00)	Entropy 5.4015e+00 (5.4084e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 5.45554780960083, 'consistency': 2.2694075107574463, 'total_loss': -3.186140298843384}], 'lowest_loss_head': 0, 'lowest_loss': -3.186140298843384}
No new lowest loss on validation set: -3.2296 -> -3.1861
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.05788005578800558, 'ARI': 0.029127014996981854, 'NMI': 0.38849601799220657, 'ACC Top-5': 0.26163306707239764, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 6), (4, 18), (5, 4), (6, 9), (7, 0), (8, 12), (9, 5), (10, 16), (11, 15), (12, 17), (13, 13), (14, 11), (15, 1), (16, 14), (17, 8), (18, 7), (19, 19)]}
Checkpoint ...
[34mEvaluate best model based on SCAN metric at the end[0m
torch.Size([15774])
torch.Size([15774])
{'ACC': 0.05578800557880056, 'ARI': 0.029214392193142508, 'NMI': 0.39056764170220803, 'ACC Top-5': 0.26486623557753264, 'hungarian_match': [(0, 10), (1, 3), (2, 2), (3, 1), (4, 12), (5, 13), (6, 9), (7, 0), (8, 4), (9, 5), (10, 16), (11, 15), (12, 17), (13, 19), (14, 11), (15, 6), (16, 14), (17, 8), (18, 7), (19, 18)]}
