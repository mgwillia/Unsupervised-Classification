vulcan06.umiacs.umd.edu
[31m{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-112', 'val_db_name': 'pascal-pretrained-112', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 256, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 112, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 112, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/topk-val-neighbors.npy'}[0m
[34mRetrieve model[0m
{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-112', 'val_db_name': 'pascal-pretrained-112', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 256, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 112, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 112, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/topk-val-neighbors.npy'}
loading pretrained
Model is ContrastiveModel
Model parameters: 30.02M
ContrastiveModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (contrastive_head): Sequential(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU()
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[34mSet CuDNN benchmark[0m
[34mRetrieve dataset[0m
Train transforms: Compose(
    RandomResizedCrop(size=(112, 112), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)
    RandomHorizontalFlip(p=0.5)
    RandomApply(
    p=0.8
    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[-0.1, 0.1])
)
    RandomGrayscale(p=0.2)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Validation transforms: Compose(
    CenterCrop(size=(112, 112))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Dataset contains 15774/15774 train/val samples
[34mBuild MemoryBank[0m
[34mRetrieve criterion[0m
Criterion is SimCLRLoss
[34mRetrieve optimizer[0m
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.001
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
[34mNo checkpoint file at /cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-112/pretext/checkpoint.pth.tar[0m
[34mStarting main loop[0m
[33mEpoch 0/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [0][ 0/61]	Loss 5.0589e+00 (5.0589e+00)
Epoch: [0][25/61]	Loss 3.2236e+00 (3.7026e+00)
Epoch: [0][50/61]	Loss 2.7200e+00 (3.2656e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 92.49
Checkpoint ...
[33mEpoch 1/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [1][ 0/61]	Loss 2.4693e+00 (2.4693e+00)
Epoch: [1][25/61]	Loss 2.2461e+00 (2.4184e+00)
Epoch: [1][50/61]	Loss 2.1952e+00 (2.3356e+00)
Checkpoint ...
[33mEpoch 2/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [2][ 0/61]	Loss 2.1247e+00 (2.1247e+00)
Epoch: [2][25/61]	Loss 2.1504e+00 (2.1036e+00)
Epoch: [2][50/61]	Loss 2.1967e+00 (2.0730e+00)
Checkpoint ...
[33mEpoch 3/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00099
Train ...
Epoch: [3][ 0/61]	Loss 1.8979e+00 (1.8979e+00)
Epoch: [3][25/61]	Loss 1.8123e+00 (1.9152e+00)
Epoch: [3][50/61]	Loss 1.9425e+00 (1.9021e+00)
Checkpoint ...
[33mEpoch 4/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [4][ 0/61]	Loss 1.7544e+00 (1.7544e+00)
Epoch: [4][25/61]	Loss 1.7903e+00 (1.8141e+00)
Epoch: [4][50/61]	Loss 1.7949e+00 (1.7962e+00)
Checkpoint ...
[33mEpoch 5/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [5][ 0/61]	Loss 1.6270e+00 (1.6270e+00)
Epoch: [5][25/61]	Loss 1.7373e+00 (1.7262e+00)
Epoch: [5][50/61]	Loss 1.6588e+00 (1.7200e+00)
Checkpoint ...
[33mEpoch 6/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00096
Train ...
Epoch: [6][ 0/61]	Loss 1.5742e+00 (1.5742e+00)
Epoch: [6][25/61]	Loss 1.7046e+00 (1.6449e+00)
Epoch: [6][50/61]	Loss 1.4751e+00 (1.6391e+00)
Checkpoint ...
[33mEpoch 7/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00095
Train ...
Epoch: [7][ 0/61]	Loss 1.7916e+00 (1.7916e+00)
Epoch: [7][25/61]	Loss 1.6820e+00 (1.5801e+00)
Epoch: [7][50/61]	Loss 1.4770e+00 (1.5672e+00)
Checkpoint ...
[33mEpoch 8/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00094
Train ...
Epoch: [8][ 0/61]	Loss 1.6072e+00 (1.6072e+00)
Epoch: [8][25/61]	Loss 1.4729e+00 (1.5362e+00)
Epoch: [8][50/61]	Loss 1.5102e+00 (1.5375e+00)
Checkpoint ...
[33mEpoch 9/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00092
Train ...
Epoch: [9][ 0/61]	Loss 1.5040e+00 (1.5040e+00)
Epoch: [9][25/61]	Loss 1.6143e+00 (1.5074e+00)
Epoch: [9][50/61]	Loss 1.4361e+00 (1.5113e+00)
Checkpoint ...
[33mEpoch 10/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00090
Train ...
Epoch: [10][ 0/61]	Loss 1.3665e+00 (1.3665e+00)
Epoch: [10][25/61]	Loss 1.3558e+00 (1.4445e+00)
Epoch: [10][50/61]	Loss 1.4747e+00 (1.4517e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 88.46
Checkpoint ...
[33mEpoch 11/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00089
Train ...
Epoch: [11][ 0/61]	Loss 1.5728e+00 (1.5728e+00)
Epoch: [11][25/61]	Loss 1.4598e+00 (1.4487e+00)
Epoch: [11][50/61]	Loss 1.4456e+00 (1.4535e+00)
Checkpoint ...
[33mEpoch 12/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00086
Train ...
Epoch: [12][ 0/61]	Loss 1.5390e+00 (1.5390e+00)
Epoch: [12][25/61]	Loss 1.5855e+00 (1.4262e+00)
Epoch: [12][50/61]	Loss 1.3587e+00 (1.4235e+00)
Checkpoint ...
[33mEpoch 13/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00084
Train ...
Epoch: [13][ 0/61]	Loss 1.4776e+00 (1.4776e+00)
Epoch: [13][25/61]	Loss 1.2376e+00 (1.3669e+00)
Epoch: [13][50/61]	Loss 1.3158e+00 (1.3719e+00)
Checkpoint ...
[33mEpoch 14/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00082
Train ...
Epoch: [14][ 0/61]	Loss 1.3949e+00 (1.3949e+00)
Epoch: [14][25/61]	Loss 1.4139e+00 (1.3613e+00)
Epoch: [14][50/61]	Loss 1.2984e+00 (1.3690e+00)
Checkpoint ...
[33mEpoch 15/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00079
Train ...
Epoch: [15][ 0/61]	Loss 1.4063e+00 (1.4063e+00)
Epoch: [15][25/61]	Loss 1.4384e+00 (1.3451e+00)
Epoch: [15][50/61]	Loss 1.2928e+00 (1.3425e+00)
Checkpoint ...
[33mEpoch 16/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00077
Train ...
Epoch: [16][ 0/61]	Loss 1.2850e+00 (1.2850e+00)
Epoch: [16][25/61]	Loss 1.3899e+00 (1.3134e+00)
Epoch: [16][50/61]	Loss 1.4932e+00 (1.3162e+00)
Checkpoint ...
[33mEpoch 17/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00074
Train ...
Epoch: [17][ 0/61]	Loss 1.2710e+00 (1.2710e+00)
Epoch: [17][25/61]	Loss 1.2152e+00 (1.2969e+00)
Epoch: [17][50/61]	Loss 1.2842e+00 (1.2954e+00)
Checkpoint ...
[33mEpoch 18/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00071
Train ...
Epoch: [18][ 0/61]	Loss 1.1711e+00 (1.1711e+00)
Epoch: [18][25/61]	Loss 1.2959e+00 (1.3045e+00)
Epoch: [18][50/61]	Loss 1.2893e+00 (1.2870e+00)
Checkpoint ...
[33mEpoch 19/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00068
Train ...
Epoch: [19][ 0/61]	Loss 1.3205e+00 (1.3205e+00)
Epoch: [19][25/61]	Loss 1.1786e+00 (1.2701e+00)
Epoch: [19][50/61]	Loss 1.1602e+00 (1.2662e+00)
Checkpoint ...
[33mEpoch 20/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00065
Train ...
Epoch: [20][ 0/61]	Loss 1.2971e+00 (1.2971e+00)
Epoch: [20][25/61]	Loss 1.2395e+00 (1.2568e+00)
Epoch: [20][50/61]	Loss 1.1514e+00 (1.2529e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 89.94
Checkpoint ...
[33mEpoch 21/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00062
Train ...
Epoch: [21][ 0/61]	Loss 1.2501e+00 (1.2501e+00)
Epoch: [21][25/61]	Loss 1.2120e+00 (1.2489e+00)
Epoch: [21][50/61]	Loss 1.1391e+00 (1.2421e+00)
Checkpoint ...
[33mEpoch 22/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00059
Train ...
Epoch: [22][ 0/61]	Loss 1.2124e+00 (1.2124e+00)
Epoch: [22][25/61]	Loss 1.2349e+00 (1.2315e+00)
Epoch: [22][50/61]	Loss 1.3769e+00 (1.2442e+00)
Checkpoint ...
[33mEpoch 23/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00056
Train ...
Epoch: [23][ 0/61]	Loss 1.2496e+00 (1.2496e+00)
Epoch: [23][25/61]	Loss 1.1179e+00 (1.2268e+00)
Epoch: [23][50/61]	Loss 1.4141e+00 (1.2279e+00)
Checkpoint ...
[33mEpoch 24/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00053
Train ...
Epoch: [24][ 0/61]	Loss 1.1455e+00 (1.1455e+00)
Epoch: [24][25/61]	Loss 1.2435e+00 (1.2359e+00)
Epoch: [24][50/61]	Loss 1.2501e+00 (1.2215e+00)
Checkpoint ...
[33mEpoch 25/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00050
Train ...
Epoch: [25][ 0/61]	Loss 1.2146e+00 (1.2146e+00)
Epoch: [25][25/61]	Loss 1.3616e+00 (1.2173e+00)
Epoch: [25][50/61]	Loss 1.1544e+00 (1.2051e+00)
Checkpoint ...
[33mEpoch 26/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00047
Train ...
Epoch: [26][ 0/61]	Loss 1.1751e+00 (1.1751e+00)
Epoch: [26][25/61]	Loss 1.2631e+00 (1.2164e+00)
Epoch: [26][50/61]	Loss 1.2045e+00 (1.2012e+00)
Checkpoint ...
[33mEpoch 27/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00044
Train ...
Epoch: [27][ 0/61]	Loss 1.2833e+00 (1.2833e+00)
Epoch: [27][25/61]	Loss 1.1351e+00 (1.1980e+00)
Epoch: [27][50/61]	Loss 1.3185e+00 (1.1908e+00)
Checkpoint ...
[33mEpoch 28/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00041
Train ...
Epoch: [28][ 0/61]	Loss 1.1957e+00 (1.1957e+00)
Epoch: [28][25/61]	Loss 1.1424e+00 (1.1648e+00)
Epoch: [28][50/61]	Loss 1.1969e+00 (1.1705e+00)
Checkpoint ...
[33mEpoch 29/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00038
Train ...
Epoch: [29][ 0/61]	Loss 1.1128e+00 (1.1128e+00)
Epoch: [29][25/61]	Loss 1.0885e+00 (1.1993e+00)
Epoch: [29][50/61]	Loss 1.3518e+00 (1.1997e+00)
Checkpoint ...
[33mEpoch 30/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00035
Train ...
Epoch: [30][ 0/61]	Loss 1.1112e+00 (1.1112e+00)
Epoch: [30][25/61]	Loss 1.3542e+00 (1.1759e+00)
Epoch: [30][50/61]	Loss 1.2247e+00 (1.1672e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 90.20
Checkpoint ...
[33mEpoch 31/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00032
Train ...
Epoch: [31][ 0/61]	Loss 1.2596e+00 (1.2596e+00)
Epoch: [31][25/61]	Loss 1.2017e+00 (1.1731e+00)
Epoch: [31][50/61]	Loss 1.0147e+00 (1.1670e+00)
Checkpoint ...
[33mEpoch 32/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00029
Train ...
Epoch: [32][ 0/61]	Loss 1.0688e+00 (1.0688e+00)
Epoch: [32][25/61]	Loss 1.0572e+00 (1.1458e+00)
Epoch: [32][50/61]	Loss 1.2112e+00 (1.1434e+00)
Checkpoint ...
[33mEpoch 33/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00026
Train ...
Epoch: [33][ 0/61]	Loss 1.0790e+00 (1.0790e+00)
Epoch: [33][25/61]	Loss 1.0412e+00 (1.1284e+00)
Epoch: [33][50/61]	Loss 1.0565e+00 (1.1527e+00)
Checkpoint ...
[33mEpoch 34/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00023
Train ...
Epoch: [34][ 0/61]	Loss 1.0272e+00 (1.0272e+00)
Epoch: [34][25/61]	Loss 1.0942e+00 (1.1749e+00)
Epoch: [34][50/61]	Loss 1.0892e+00 (1.1673e+00)
Checkpoint ...
[33mEpoch 35/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00021
Train ...
Epoch: [35][ 0/61]	Loss 1.1366e+00 (1.1366e+00)
Epoch: [35][25/61]	Loss 1.2311e+00 (1.1477e+00)
Epoch: [35][50/61]	Loss 1.1473e+00 (1.1491e+00)
Checkpoint ...
[33mEpoch 36/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00018
Train ...
Epoch: [36][ 0/61]	Loss 1.1721e+00 (1.1721e+00)
Epoch: [36][25/61]	Loss 1.1052e+00 (1.1584e+00)
Epoch: [36][50/61]	Loss 1.1260e+00 (1.1608e+00)
Checkpoint ...
[33mEpoch 37/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00016
Train ...
Epoch: [37][ 0/61]	Loss 1.0950e+00 (1.0950e+00)
Epoch: [37][25/61]	Loss 1.1037e+00 (1.1295e+00)
Epoch: [37][50/61]	Loss 1.2252e+00 (1.1391e+00)
Checkpoint ...
[33mEpoch 38/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00014
Train ...
Epoch: [38][ 0/61]	Loss 1.1723e+00 (1.1723e+00)
Epoch: [38][25/61]	Loss 1.2756e+00 (1.1616e+00)
Epoch: [38][50/61]	Loss 1.2043e+00 (1.1415e+00)
Checkpoint ...
[33mEpoch 39/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00012
Train ...
Epoch: [39][ 0/61]	Loss 1.1445e+00 (1.1445e+00)
Epoch: [39][25/61]	Loss 1.1379e+00 (1.1401e+00)
Epoch: [39][50/61]	Loss 1.0891e+00 (1.1318e+00)
Checkpoint ...
[33mEpoch 40/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [40][ 0/61]	Loss 1.1850e+00 (1.1850e+00)
Epoch: [40][25/61]	Loss 1.1135e+00 (1.1506e+00)
Epoch: [40][50/61]	Loss 1.1866e+00 (1.1438e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 90.67
Checkpoint ...
[33mEpoch 41/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00008
Train ...
Epoch: [41][ 0/61]	Loss 1.0710e+00 (1.0710e+00)
Epoch: [41][25/61]	Loss 1.1671e+00 (1.1575e+00)
Epoch: [41][50/61]	Loss 1.1322e+00 (1.1596e+00)
Checkpoint ...
[33mEpoch 42/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00006
Train ...
Epoch: [42][ 0/61]	Loss 1.1394e+00 (1.1394e+00)
Epoch: [42][25/61]	Loss 1.1571e+00 (1.1264e+00)
Epoch: [42][50/61]	Loss 1.1722e+00 (1.1475e+00)
Checkpoint ...
[33mEpoch 43/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00005
Train ...
Epoch: [43][ 0/61]	Loss 1.1219e+00 (1.1219e+00)
Epoch: [43][25/61]	Loss 1.0490e+00 (1.1495e+00)
Epoch: [43][50/61]	Loss 1.1765e+00 (1.1478e+00)
Checkpoint ...
[33mEpoch 44/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00004
Train ...
Epoch: [44][ 0/61]	Loss 1.0754e+00 (1.0754e+00)
Epoch: [44][25/61]	Loss 1.1040e+00 (1.1179e+00)
Epoch: [44][50/61]	Loss 1.0843e+00 (1.1356e+00)
Checkpoint ...
[33mEpoch 45/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00003
Train ...
Epoch: [45][ 0/61]	Loss 1.0221e+00 (1.0221e+00)
Epoch: [45][25/61]	Loss 1.0622e+00 (1.1206e+00)
Epoch: [45][50/61]	Loss 1.0312e+00 (1.1351e+00)
Checkpoint ...
[33mEpoch 46/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00002
Train ...
Epoch: [46][ 0/61]	Loss 9.9699e-01 (9.9699e-01)
Epoch: [46][25/61]	Loss 1.0263e+00 (1.1278e+00)
Epoch: [46][50/61]	Loss 1.0837e+00 (1.1373e+00)
Checkpoint ...
[33mEpoch 47/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00001
Train ...
Epoch: [47][ 0/61]	Loss 1.1322e+00 (1.1322e+00)
Epoch: [47][25/61]	Loss 1.1284e+00 (1.1237e+00)
Epoch: [47][50/61]	Loss 1.0760e+00 (1.1294e+00)
Checkpoint ...
[33mEpoch 48/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00000
Train ...
Epoch: [48][ 0/61]	Loss 1.1027e+00 (1.1027e+00)
Epoch: [48][25/61]	Loss 1.3065e+00 (1.1359e+00)
Epoch: [48][50/61]	Loss 1.1240e+00 (1.1371e+00)
Checkpoint ...
[33mEpoch 49/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00000
Train ...
Epoch: [49][ 0/61]	Loss 1.0330e+00 (1.0330e+00)
Epoch: [49][25/61]	Loss 1.0670e+00 (1.1424e+00)
Epoch: [49][50/61]	Loss 1.2580e+00 (1.1417e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/62]
Evaluate ...
Result of kNN evaluation is 90.55
Checkpoint ...
[34mFill memory bank for mining the nearest neighbors (train) ...[0m
Fill Memory Bank [0/62]
Mine the nearest neighbors (Top-20)
Accuracy of top-20 nearest neighbors on train set is 38.39
[34mFill memory bank for mining the nearest neighbors (val) ...[0m
Fill Memory Bank [0/62]
Mine the nearest neighbors (Top-5)
Accuracy of top-5 nearest neighbors on val set is 42.82
