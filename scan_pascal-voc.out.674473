vulcan14.umiacs.umd.edu
[31m{'setup': 'scan', 'criterion': 'scan', 'criterion_kwargs': {'entropy_weight': 5.0}, 'update_cluster_head_only': False, 'num_heads': 1, 'backbone': 'resnet18', 'train_db_name': 'pascal-voc', 'val_db_name': 'pascal-voc', 'num_classes': 10, 'num_neighbors': 20, 'augmentation_strategy': 'ours', 'augmentation_kwargs': {'crop_size': 32, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, 'num_strong_augs': 4, 'cutout_kwargs': {'n_holes': 1, 'length': 16, 'random': True}}, 'transformation_kwargs': {'crop_size': 32, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'optimizer': 'adam', 'optimizer_kwargs': {'lr': 0.0001, 'weight_decay': 0.0001}, 'epochs': 50, 'batch_size': 128, 'num_workers': 8, 'scheduler': 'constant', 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/pretext/topk-val-neighbors.npy', 'scan_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/scan', 'scan_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/scan/checkpoint.pth.tar', 'scan_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/scan/model.pth.tar', 'selflabel_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/selflabel', 'selflabel_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/selflabel/checkpoint.pth.tar', 'selflabel_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/selflabel/model.pth.tar'}[0m
[34mGet dataset and dataloaders[0m
Train transforms: Compose(
    RandomHorizontalFlip(p=0.5)
    RandomCrop(size=(32, 32), padding=None)
    <data.augment.Augment object at 0x7f87380591d0>
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    <data.augment.Cutout object at 0x7f8738059320>
)
Validation transforms: Compose(
    CenterCrop(size=(32, 32))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Train samples 15774 - Val samples 15774
[34mGet model[0m
ClusteringModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  )
  (cluster_head): ModuleList(
    (0): Linear(in_features=512, out_features=10, bias=True)
  )
)
[34mGet optimizer[0m
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0001
)
[34mGet loss[0m
SCANLoss(
  (softmax): Softmax(dim=1)
  (bce): BCELoss()
)
[34mNo checkpoint file at /cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-voc/scan/checkpoint.pth.tar[0m
[34mStarting main loop[0m
[33mEpoch 1/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [0][  0/123]	Total Loss -9.2084e+00 (-9.2084e+00)	Consistency Loss 2.3014e+00 (2.3014e+00)	Entropy 2.3020e+00 (2.3020e+00)
Epoch: [0][ 25/123]	Total Loss -9.2110e+00 (-9.2104e+00)	Consistency Loss 2.3018e+00 (2.3020e+00)	Entropy 2.3026e+00 (2.3025e+00)
Epoch: [0][ 50/123]	Total Loss -9.2157e+00 (-9.2111e+00)	Consistency Loss 2.2970e+00 (2.3014e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [0][ 75/123]	Total Loss -9.2077e+00 (-9.2125e+00)	Consistency Loss 2.3046e+00 (2.3000e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [0][100/123]	Total Loss -9.2241e+00 (-9.2145e+00)	Consistency Loss 2.2886e+00 (2.2981e+00)	Entropy 2.3025e+00 (2.3025e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 2.0520524978637695, 'consistency': 1.7894381284713745, 'total_loss': -0.262614369392395}], 'lowest_loss_head': 0, 'lowest_loss': -0.262614369392395}
New lowest loss on validation set: 10000.0000 -> -0.2626
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.16337010270064664, 'ARI': -0.0137921950657323, 'NMI': 0.032915016849785185, 'ACC Top-5': 0.5146443514644351, 'hungarian_match': [(0, 14), (1, 16), (2, 8), (3, 15), (4, 4), (5, 19), (6, 0), (7, 6), (8, 3), (9, 1), (10, 2), (11, 5), (12, 7), (13, 9), (14, 10), (15, 11), (16, 12), (17, 13), (18, 17), (19, 18)]}
Checkpoint ...
[33mEpoch 2/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [1][  0/123]	Total Loss -9.2034e+00 (-9.2034e+00)	Consistency Loss 2.3086e+00 (2.3086e+00)	Entropy 2.3024e+00 (2.3024e+00)
Epoch: [1][ 25/123]	Total Loss -9.2309e+00 (-9.2207e+00)	Consistency Loss 2.2814e+00 (2.2917e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [1][ 50/123]	Total Loss -9.2077e+00 (-9.2200e+00)	Consistency Loss 2.3050e+00 (2.2923e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [1][ 75/123]	Total Loss -9.1912e+00 (-9.2216e+00)	Consistency Loss 2.3213e+00 (2.2908e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [1][100/123]	Total Loss -9.2229e+00 (-9.2220e+00)	Consistency Loss 2.2899e+00 (2.2904e+00)	Entropy 2.3025e+00 (2.3025e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 1.9694560766220093, 'consistency': 1.6530275344848633, 'total_loss': -0.316428542137146}], 'lowest_loss_head': 0, 'lowest_loss': -0.316428542137146}
New lowest loss on validation set: -0.2626 -> -0.3164
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.19379992392544693, 'ARI': -0.004085189610224124, 'NMI': 0.03789879029417684, 'ACC Top-5': 0.5388614175225054, 'hungarian_match': [(0, 8), (1, 16), (2, 11), (3, 6), (4, 2), (5, 4), (6, 0), (7, 14), (8, 3), (9, 1), (10, 5), (11, 7), (12, 9), (13, 10), (14, 12), (15, 13), (16, 15), (17, 17), (18, 18), (19, 19)]}
Checkpoint ...
[33mEpoch 3/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [2][  0/123]	Total Loss -9.2240e+00 (-9.2240e+00)	Consistency Loss 2.2886e+00 (2.2886e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [2][ 25/123]	Total Loss -9.2475e+00 (-9.2259e+00)	Consistency Loss 2.2641e+00 (2.2867e+00)	Entropy 2.3023e+00 (2.3025e+00)
Epoch: [2][ 50/123]	Total Loss -9.2159e+00 (-9.2247e+00)	Consistency Loss 2.2967e+00 (2.2879e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [2][ 75/123]	Total Loss -9.2325e+00 (-9.2252e+00)	Consistency Loss 2.2804e+00 (2.2874e+00)	Entropy 2.3026e+00 (2.3025e+00)
Epoch: [2][100/123]	Total Loss -9.2318e+00 (-9.2230e+00)	Consistency Loss 2.2808e+00 (2.2896e+00)	Entropy 2.3025e+00 (2.3025e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 2.0414323806762695, 'consistency': 1.813913345336914, 'total_loss': -0.22751903533935547}], 'lowest_loss_head': 0, 'lowest_loss': -0.22751903533935547}
No new lowest loss on validation set: -0.3164 -> -0.2275
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.19468746037783694, 'ARI': -0.014863069349447105, 'NMI': 0.031047051940787752, 'ACC Top-5': 0.5535691644478256, 'hungarian_match': [(0, 14), (1, 4), (2, 11), (3, 15), (4, 16), (5, 6), (6, 0), (7, 8), (8, 3), (9, 9), (10, 1), (11, 2), (12, 5), (13, 7), (14, 10), (15, 12), (16, 13), (17, 17), (18, 18), (19, 19)]}
Checkpoint ...
[33mEpoch 4/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [3][  0/123]	Total Loss -9.2186e+00 (-9.2186e+00)	Consistency Loss 2.2937e+00 (2.2937e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [3][ 25/123]	Total Loss -9.2147e+00 (-9.2193e+00)	Consistency Loss 2.2978e+00 (2.2934e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [3][ 50/123]	Total Loss -9.2292e+00 (-9.2230e+00)	Consistency Loss 2.2831e+00 (2.2896e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [3][ 75/123]	Total Loss -9.2312e+00 (-9.2255e+00)	Consistency Loss 2.2816e+00 (2.2872e+00)	Entropy 2.3026e+00 (2.3025e+00)
Epoch: [3][100/123]	Total Loss -9.2088e+00 (-9.2241e+00)	Consistency Loss 2.3039e+00 (2.2886e+00)	Entropy 2.3025e+00 (2.3025e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 2.010228395462036, 'consistency': 1.7886167764663696, 'total_loss': -0.2216116189956665}], 'lowest_loss_head': 0, 'lowest_loss': -0.2216116189956665}
No new lowest loss on validation set: -0.3164 -> -0.2216
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.19063015088119692, 'ARI': -0.0035839953569700776, 'NMI': 0.03980692455207442, 'ACC Top-5': 0.5498288322556105, 'hungarian_match': [(0, 14), (1, 4), (2, 8), (3, 3), (4, 16), (5, 6), (6, 0), (7, 15), (8, 2), (9, 1), (10, 5), (11, 7), (12, 9), (13, 10), (14, 11), (15, 12), (16, 13), (17, 17), (18, 18), (19, 19)]}
Checkpoint ...
[33mEpoch 5/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [4][  0/123]	Total Loss -9.2273e+00 (-9.2273e+00)	Consistency Loss 2.2855e+00 (2.2855e+00)	Entropy 2.3026e+00 (2.3026e+00)
Epoch: [4][ 25/123]	Total Loss -9.2384e+00 (-9.2243e+00)	Consistency Loss 2.2745e+00 (2.2885e+00)	Entropy 2.3026e+00 (2.3025e+00)
Epoch: [4][ 50/123]	Total Loss -9.2368e+00 (-9.2258e+00)	Consistency Loss 2.2758e+00 (2.2868e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [4][ 75/123]	Total Loss -9.2026e+00 (-9.2242e+00)	Consistency Loss 2.3100e+00 (2.2884e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [4][100/123]	Total Loss -9.2151e+00 (-9.2236e+00)	Consistency Loss 2.2976e+00 (2.2891e+00)	Entropy 2.3025e+00 (2.3025e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 2.002685546875, 'consistency': 1.7771168947219849, 'total_loss': -0.22556865215301514}], 'lowest_loss_head': 0, 'lowest_loss': -0.22556865215301514}
No new lowest loss on validation set: -0.3164 -> -0.2256
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.20318245213642702, 'ARI': 0.00759664820995817, 'NMI': 0.04369940849346081, 'ACC Top-5': 0.5741726892354507, 'hungarian_match': [(0, 8), (1, 4), (2, 11), (3, 5), (4, 16), (5, 6), (6, 0), (7, 14), (8, 2), (9, 1), (10, 3), (11, 7), (12, 9), (13, 10), (14, 12), (15, 13), (16, 15), (17, 17), (18, 18), (19, 19)]}
Checkpoint ...
[33mEpoch 6/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [5][  0/123]	Total Loss -9.2203e+00 (-9.2203e+00)	Consistency Loss 2.2923e+00 (2.2923e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [5][ 25/123]	Total Loss -9.2617e+00 (-9.2366e+00)	Consistency Loss 2.2511e+00 (2.2761e+00)	Entropy 2.3026e+00 (2.3025e+00)
Epoch: [5][ 50/123]	Total Loss -9.2172e+00 (-9.2303e+00)	Consistency Loss 2.2953e+00 (2.2823e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [5][ 75/123]	Total Loss -9.2339e+00 (-9.2281e+00)	Consistency Loss 2.2789e+00 (2.2846e+00)	Entropy 2.3026e+00 (2.3025e+00)
Epoch: [5][100/123]	Total Loss -9.2123e+00 (-9.2275e+00)	Consistency Loss 2.3006e+00 (2.2852e+00)	Entropy 2.3026e+00 (2.3025e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 1.931550145149231, 'consistency': 1.681348443031311, 'total_loss': -0.2502017021179199}], 'lowest_loss_head': 0, 'lowest_loss': -0.2502017021179199}
No new lowest loss on validation set: -0.3164 -> -0.2502
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.20666920248510207, 'ARI': 0.003272955534710663, 'NMI': 0.040328922436779976, 'ACC Top-5': 0.5763281349055408, 'hungarian_match': [(0, 8), (1, 4), (2, 11), (3, 5), (4, 16), (5, 6), (6, 0), (7, 14), (8, 3), (9, 1), (10, 2), (11, 7), (12, 9), (13, 10), (14, 12), (15, 13), (16, 15), (17, 17), (18, 18), (19, 19)]}
Checkpoint ...
[33mEpoch 7/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [6][  0/123]	Total Loss -9.2211e+00 (-9.2211e+00)	Consistency Loss 2.2918e+00 (2.2918e+00)	Entropy 2.3026e+00 (2.3026e+00)
Epoch: [6][ 25/123]	Total Loss -9.1954e+00 (-9.2231e+00)	Consistency Loss 2.3173e+00 (2.2896e+00)	Entropy 2.3025e+00 (2.3025e+00)
Epoch: [6][ 50/123]	Total Loss -9.2205e+00 (-9.2247e+00)	Consistency Loss 2.2924e+00 (2.2880e+00)	Entropy 2.3026e+00 (2.3026e+00)
Epoch: [6][ 75/123]	Total Loss -9.2154e+00 (-9.2253e+00)	Consistency Loss 2.2974e+00 (2.2875e+00)	Entropy 2.3026e+00 (2.3026e+00)
Epoch: [6][100/123]	Total Loss -9.2451e+00 (-9.2257e+00)	Consistency Loss 2.2677e+00 (2.2871e+00)	Entropy 2.3026e+00 (2.3026e+00)
Make prediction on validation set ...
