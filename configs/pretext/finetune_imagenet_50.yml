# Setup
setup: simclr-distill
teacher: selflabel
num_heads: 1
teacher_path: selflabel_imagenet_50.pth.tar

finetune: True
finetune_model: tutorial/imagenet_50/pretext/model.pth.tar

# Model
backbone: resnet50
model_kwargs:
   head: mlp
   features_dim: 128

# Dataset
train_db_name: imagenet_50-f
val_db_name: imagenet_50-f
num_classes: 50

# Loss
criterion: simclr-distill
criterion_kwargs:
   temperature: 0.1 
   distill_alpha: 10.0

# Hyperparameters
epochs: 10
optimizer: sgd
optimizer_kwargs:
   nesterov: False
   weight_decay: 0.0001 
   momentum: 0.9
   lr: 0.0006
scheduler: cosine
scheduler_kwargs:
   lr_decay_rate: 0.1
batch_size: 448 
num_workers: 4

# Transformations
augmentation_strategy: simclr 
augmentation_kwargs:
   random_resized_crop:
      size: 224
      scale: [0.2, 1.0]
   color_jitter_random_apply:
      p: 0.8
   color_jitter:
      brightness: 0.4
      contrast: 0.4
      saturation: 0.4
      hue: 0.1
   random_grayscale: 
      p: 0.2
   normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

transformation_kwargs:
   crop_size: 224
   normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
