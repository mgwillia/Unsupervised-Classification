vulcan03.umiacs.umd.edu
[31m{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-448', 'val_db_name': 'pascal-pretrained-448', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 32, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 448, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 448, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-val-neighbors.npy'}[0m
[34mRetrieve model[0m
{'setup': 'simclr', 'backbone': 'resnet50', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'pascal-pretrained-448', 'val_db_name': 'pascal-pretrained-448', 'num_classes': 20, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 50, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.001, 'momentum': 0.9, 'lr': 0.001}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 32, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 448, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 448, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext', 'pretext_checkpoint': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/checkpoint.pth.tar', 'pretext_model': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/model.pth.tar', 'topk_neighbors_train_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/topk-val-neighbors.npy'}
loading pretrained
Model is ContrastiveModel
Model parameters: 30.02M
ContrastiveModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (contrastive_head): Sequential(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU()
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[34mSet CuDNN benchmark[0m
[34mRetrieve dataset[0m
Train transforms: Compose(
    RandomResizedCrop(size=(448, 448), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)
    RandomHorizontalFlip(p=0.5)
    RandomApply(
    p=0.8
    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[-0.1, 0.1])
)
    RandomGrayscale(p=0.2)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Validation transforms: Compose(
    CenterCrop(size=(448, 448))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Dataset contains 15774/15774 train/val samples
[34mBuild MemoryBank[0m
[34mRetrieve criterion[0m
Criterion is SimCLRLoss
[34mRetrieve optimizer[0m
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.001
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
[34mNo checkpoint file at /cfarhomes/mgwillia/scan-adaptation/Unsupervised-Classification/tutorial/pascal-pretrained-448/pretext/checkpoint.pth.tar[0m
[34mStarting main loop[0m
[33mEpoch 0/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [0][  0/492]	Loss 3.2597e+00 (3.2597e+00)
Epoch: [0][ 25/492]	Loss 9.7024e-01 (1.5623e+00)
Epoch: [0][ 50/492]	Loss 5.5965e-01 (1.1433e+00)
Epoch: [0][ 75/492]	Loss 5.8526e-01 (9.7553e-01)
Epoch: [0][100/492]	Loss 4.0933e-01 (8.5810e-01)
Epoch: [0][125/492]	Loss 4.3276e-01 (7.8124e-01)
Epoch: [0][150/492]	Loss 8.7358e-01 (7.3101e-01)
Epoch: [0][175/492]	Loss 3.2956e-01 (6.9123e-01)
Epoch: [0][200/492]	Loss 5.9576e-01 (6.6236e-01)
Epoch: [0][225/492]	Loss 3.4171e-01 (6.2893e-01)
Epoch: [0][250/492]	Loss 4.4583e-01 (6.0342e-01)
Epoch: [0][275/492]	Loss 2.8305e-01 (5.8085e-01)
Epoch: [0][300/492]	Loss 2.9707e-01 (5.6364e-01)
Epoch: [0][325/492]	Loss 2.6993e-01 (5.4624e-01)
Epoch: [0][350/492]	Loss 4.4052e-01 (5.3027e-01)
Epoch: [0][375/492]	Loss 2.2322e-01 (5.1717e-01)
Epoch: [0][400/492]	Loss 3.4780e-01 (5.0583e-01)
Epoch: [0][425/492]	Loss 2.2970e-01 (4.9339e-01)
Epoch: [0][450/492]	Loss 2.6038e-01 (4.8206e-01)
Epoch: [0][475/492]	Loss 3.6615e-01 (4.6859e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Evaluate ...
Result of kNN evaluation is 82.93
Checkpoint ...
[33mEpoch 1/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [1][  0/492]	Loss 3.5618e-01 (3.5618e-01)
Epoch: [1][ 25/492]	Loss 2.2240e-01 (2.8859e-01)
Epoch: [1][ 50/492]	Loss 2.3797e-01 (2.8406e-01)
Epoch: [1][ 75/492]	Loss 2.6994e-01 (2.8407e-01)
Epoch: [1][100/492]	Loss 1.7602e-01 (2.8641e-01)
Epoch: [1][125/492]	Loss 2.1411e-01 (2.9074e-01)
Epoch: [1][150/492]	Loss 1.4009e-01 (2.8541e-01)
Epoch: [1][175/492]	Loss 8.5649e-01 (2.8386e-01)
Epoch: [1][200/492]	Loss 5.5873e-01 (2.7982e-01)
Epoch: [1][225/492]	Loss 1.2573e-01 (2.7727e-01)
Epoch: [1][250/492]	Loss 3.9975e-01 (2.7705e-01)
Epoch: [1][275/492]	Loss 1.4558e-01 (2.7525e-01)
Epoch: [1][300/492]	Loss 4.2763e-01 (2.7125e-01)
Epoch: [1][325/492]	Loss 2.4968e-01 (2.6933e-01)
Epoch: [1][350/492]	Loss 1.5320e-01 (2.6817e-01)
Epoch: [1][375/492]	Loss 2.3036e-01 (2.6717e-01)
Epoch: [1][400/492]	Loss 4.7891e-01 (2.6362e-01)
Epoch: [1][425/492]	Loss 1.4437e-01 (2.6092e-01)
Epoch: [1][450/492]	Loss 4.1902e-01 (2.5927e-01)
Epoch: [1][475/492]	Loss 1.5494e-01 (2.5731e-01)
Checkpoint ...
[33mEpoch 2/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00100
Train ...
Epoch: [2][  0/492]	Loss 1.4917e-01 (1.4917e-01)
Epoch: [2][ 25/492]	Loss 3.9476e-01 (2.2241e-01)
Epoch: [2][ 50/492]	Loss 4.2171e-01 (2.0509e-01)
Epoch: [2][ 75/492]	Loss 2.1217e-01 (2.1029e-01)
Epoch: [2][100/492]	Loss 2.9224e-01 (2.1020e-01)
Epoch: [2][125/492]	Loss 2.1234e-01 (2.1443e-01)
Epoch: [2][150/492]	Loss 3.5819e-01 (2.1705e-01)
Epoch: [2][175/492]	Loss 3.0742e-01 (2.2094e-01)
Epoch: [2][200/492]	Loss 3.0417e-01 (2.1903e-01)
Epoch: [2][225/492]	Loss 1.1708e-01 (2.1647e-01)
Epoch: [2][250/492]	Loss 1.2836e-01 (2.1775e-01)
Epoch: [2][275/492]	Loss 3.5422e-01 (2.2042e-01)
Epoch: [2][300/492]	Loss 1.1254e-01 (2.1868e-01)
Epoch: [2][325/492]	Loss 1.3538e-01 (2.1719e-01)
Epoch: [2][350/492]	Loss 2.9051e-01 (2.1906e-01)
Epoch: [2][375/492]	Loss 1.7383e-01 (2.1755e-01)
Epoch: [2][400/492]	Loss 2.4828e-01 (2.1687e-01)
Epoch: [2][425/492]	Loss 1.7467e-01 (2.1569e-01)
Epoch: [2][450/492]	Loss 9.2371e-02 (2.1584e-01)
Epoch: [2][475/492]	Loss 1.2007e-01 (2.1515e-01)
Checkpoint ...
[33mEpoch 3/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00099
Train ...
Epoch: [3][  0/492]	Loss 4.6015e-01 (4.6015e-01)
Epoch: [3][ 25/492]	Loss 1.3810e-01 (1.9614e-01)
Epoch: [3][ 50/492]	Loss 1.4954e-01 (1.9519e-01)
Epoch: [3][ 75/492]	Loss 1.0986e-01 (1.9301e-01)
Epoch: [3][100/492]	Loss 1.9990e-01 (2.1350e-01)
Epoch: [3][125/492]	Loss 2.7695e-01 (2.0897e-01)
Epoch: [3][150/492]	Loss 4.0297e-01 (2.0832e-01)
Epoch: [3][175/492]	Loss 2.4251e-01 (2.0890e-01)
Epoch: [3][200/492]	Loss 1.3857e-01 (2.1014e-01)
Epoch: [3][225/492]	Loss 3.3140e-01 (2.0930e-01)
Epoch: [3][250/492]	Loss 9.6229e-02 (2.0650e-01)
Epoch: [3][275/492]	Loss 2.8206e-01 (2.0580e-01)
Epoch: [3][300/492]	Loss 6.1325e-01 (2.0858e-01)
Epoch: [3][325/492]	Loss 4.0132e-01 (2.0871e-01)
Epoch: [3][350/492]	Loss 1.4435e-01 (2.0447e-01)
Epoch: [3][375/492]	Loss 9.2175e-02 (2.0145e-01)
Epoch: [3][400/492]	Loss 6.2150e-02 (1.9845e-01)
Epoch: [3][425/492]	Loss 3.2605e-01 (1.9801e-01)
Epoch: [3][450/492]	Loss 2.0652e-01 (1.9706e-01)
Epoch: [3][475/492]	Loss 9.9254e-02 (1.9762e-01)
Checkpoint ...
[33mEpoch 4/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [4][  0/492]	Loss 2.4954e-01 (2.4954e-01)
Epoch: [4][ 25/492]	Loss 1.3971e-01 (1.5567e-01)
Epoch: [4][ 50/492]	Loss 1.6066e-01 (1.6650e-01)
Epoch: [4][ 75/492]	Loss 3.0315e-01 (1.8702e-01)
Epoch: [4][100/492]	Loss 2.6776e-01 (1.7948e-01)
Epoch: [4][125/492]	Loss 1.0943e-01 (1.7711e-01)
Epoch: [4][150/492]	Loss 6.2016e-02 (1.7663e-01)
Epoch: [4][175/492]	Loss 4.5814e-01 (1.7553e-01)
Epoch: [4][200/492]	Loss 1.2818e-01 (1.7731e-01)
Epoch: [4][225/492]	Loss 1.1310e-01 (1.7301e-01)
Epoch: [4][250/492]	Loss 6.9812e-02 (1.7332e-01)
Epoch: [4][275/492]	Loss 1.1064e-01 (1.7226e-01)
Epoch: [4][300/492]	Loss 1.7459e-01 (1.6902e-01)
Epoch: [4][325/492]	Loss 2.6494e-01 (1.6929e-01)
Epoch: [4][350/492]	Loss 1.5432e-01 (1.6894e-01)
Epoch: [4][375/492]	Loss 6.9195e-02 (1.6807e-01)
Epoch: [4][400/492]	Loss 1.2587e-01 (1.6772e-01)
Epoch: [4][425/492]	Loss 1.1891e-01 (1.6579e-01)
Epoch: [4][450/492]	Loss 1.0631e-01 (1.6646e-01)
Epoch: [4][475/492]	Loss 2.4286e-01 (1.6556e-01)
Checkpoint ...
[33mEpoch 5/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00098
Train ...
Epoch: [5][  0/492]	Loss 1.8392e-01 (1.8392e-01)
Epoch: [5][ 25/492]	Loss 1.7366e-01 (1.8761e-01)
Epoch: [5][ 50/492]	Loss 7.1768e-02 (1.8414e-01)
Epoch: [5][ 75/492]	Loss 7.4228e-02 (1.7047e-01)
Epoch: [5][100/492]	Loss 2.5520e-01 (1.6892e-01)
Epoch: [5][125/492]	Loss 1.8581e-01 (1.7042e-01)
Epoch: [5][150/492]	Loss 6.5211e-02 (1.6552e-01)
Epoch: [5][175/492]	Loss 2.6356e-01 (1.6741e-01)
Epoch: [5][200/492]	Loss 3.2567e-01 (1.6966e-01)
Epoch: [5][225/492]	Loss 1.9455e-01 (1.7059e-01)
Epoch: [5][250/492]	Loss 1.2496e-01 (1.6662e-01)
Epoch: [5][275/492]	Loss 1.4642e-01 (1.6582e-01)
Epoch: [5][300/492]	Loss 6.2720e-02 (1.6262e-01)
Epoch: [5][325/492]	Loss 1.2274e-01 (1.6109e-01)
Epoch: [5][350/492]	Loss 1.0151e-01 (1.6002e-01)
Epoch: [5][375/492]	Loss 1.0173e-01 (1.6007e-01)
Epoch: [5][400/492]	Loss 2.0144e-01 (1.6189e-01)
Epoch: [5][425/492]	Loss 7.7672e-02 (1.6334e-01)
Epoch: [5][450/492]	Loss 1.0893e-01 (1.6224e-01)
Epoch: [5][475/492]	Loss 2.5220e-01 (1.6192e-01)
Checkpoint ...
[33mEpoch 6/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00096
Train ...
Epoch: [6][  0/492]	Loss 5.2880e-01 (5.2880e-01)
Epoch: [6][ 25/492]	Loss 4.7547e-02 (1.6378e-01)
Epoch: [6][ 50/492]	Loss 3.8557e-01 (1.5548e-01)
Epoch: [6][ 75/492]	Loss 1.7352e-01 (1.4842e-01)
Epoch: [6][100/492]	Loss 1.8368e-01 (1.4652e-01)
Epoch: [6][125/492]	Loss 7.3065e-02 (1.4781e-01)
Epoch: [6][150/492]	Loss 1.4640e-01 (1.4538e-01)
Epoch: [6][175/492]	Loss 1.0664e-01 (1.4258e-01)
Epoch: [6][200/492]	Loss 8.4778e-02 (1.4250e-01)
Epoch: [6][225/492]	Loss 4.7567e-01 (1.4581e-01)
Epoch: [6][250/492]	Loss 3.8863e-01 (1.4654e-01)
Epoch: [6][275/492]	Loss 1.4914e-01 (1.4674e-01)
Epoch: [6][300/492]	Loss 2.0934e-01 (1.4991e-01)
Epoch: [6][325/492]	Loss 5.2789e-01 (1.5119e-01)
Epoch: [6][350/492]	Loss 1.1250e-01 (1.5179e-01)
Epoch: [6][375/492]	Loss 1.6238e-01 (1.4921e-01)
Epoch: [6][400/492]	Loss 3.0350e-01 (1.4984e-01)
Epoch: [6][425/492]	Loss 8.6260e-02 (1.5151e-01)
Epoch: [6][450/492]	Loss 1.5700e-01 (1.5145e-01)
Epoch: [6][475/492]	Loss 2.4099e-01 (1.5157e-01)
Checkpoint ...
[33mEpoch 7/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00095
Train ...
Epoch: [7][  0/492]	Loss 2.8762e-01 (2.8762e-01)
Epoch: [7][ 25/492]	Loss 1.5914e-01 (1.3969e-01)
Epoch: [7][ 50/492]	Loss 6.2384e-02 (1.2584e-01)
Epoch: [7][ 75/492]	Loss 9.0912e-02 (1.3331e-01)
Epoch: [7][100/492]	Loss 2.1899e-01 (1.4287e-01)
Epoch: [7][125/492]	Loss 1.1490e-01 (1.4413e-01)
Epoch: [7][150/492]	Loss 3.5427e-01 (1.4388e-01)
Epoch: [7][175/492]	Loss 3.3129e-01 (1.4438e-01)
Epoch: [7][200/492]	Loss 6.5894e-02 (1.4361e-01)
Epoch: [7][225/492]	Loss 1.4055e-01 (1.4308e-01)
Epoch: [7][250/492]	Loss 6.6245e-02 (1.4404e-01)
Epoch: [7][275/492]	Loss 1.4063e-01 (1.4168e-01)
Epoch: [7][300/492]	Loss 2.0248e-01 (1.4182e-01)
Epoch: [7][325/492]	Loss 1.0663e-01 (1.4222e-01)
Epoch: [7][350/492]	Loss 7.7489e-02 (1.4355e-01)
Epoch: [7][375/492]	Loss 1.2405e-01 (1.4341e-01)
Epoch: [7][400/492]	Loss 5.9383e-02 (1.4122e-01)
Epoch: [7][425/492]	Loss 1.3158e-01 (1.4088e-01)
Epoch: [7][450/492]	Loss 7.5047e-02 (1.4102e-01)
Epoch: [7][475/492]	Loss 1.1294e-01 (1.4043e-01)
Checkpoint ...
[33mEpoch 8/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00094
Train ...
Epoch: [8][  0/492]	Loss 5.2909e-02 (5.2909e-02)
Epoch: [8][ 25/492]	Loss 4.5888e-02 (1.5134e-01)
Epoch: [8][ 50/492]	Loss 2.2422e-01 (1.3969e-01)
Epoch: [8][ 75/492]	Loss 8.9491e-02 (1.4188e-01)
Epoch: [8][100/492]	Loss 1.7543e-01 (1.4273e-01)
Epoch: [8][125/492]	Loss 5.5328e-02 (1.4143e-01)
Epoch: [8][150/492]	Loss 1.3219e-01 (1.4207e-01)
Epoch: [8][175/492]	Loss 1.9283e-01 (1.3759e-01)
Epoch: [8][200/492]	Loss 1.7798e-01 (1.3778e-01)
Epoch: [8][225/492]	Loss 1.2311e-01 (1.4056e-01)
Epoch: [8][250/492]	Loss 1.2386e-01 (1.4366e-01)
Epoch: [8][275/492]	Loss 1.8001e-01 (1.4395e-01)
Epoch: [8][300/492]	Loss 9.2040e-02 (1.4307e-01)
Epoch: [8][325/492]	Loss 1.2883e-01 (1.4605e-01)
Epoch: [8][350/492]	Loss 1.9945e-01 (1.4651e-01)
Epoch: [8][375/492]	Loss 5.4860e-02 (1.4419e-01)
Epoch: [8][400/492]	Loss 4.6626e-02 (1.4364e-01)
Epoch: [8][425/492]	Loss 6.5691e-02 (1.4235e-01)
Epoch: [8][450/492]	Loss 2.2650e-01 (1.4266e-01)
Epoch: [8][475/492]	Loss 6.3436e-02 (1.4153e-01)
Checkpoint ...
[33mEpoch 9/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00092
Train ...
Epoch: [9][  0/492]	Loss 7.9675e-02 (7.9675e-02)
Epoch: [9][ 25/492]	Loss 2.2818e-01 (1.5306e-01)
Epoch: [9][ 50/492]	Loss 8.5723e-02 (1.4403e-01)
Epoch: [9][ 75/492]	Loss 5.8971e-02 (1.4126e-01)
Epoch: [9][100/492]	Loss 2.4687e-01 (1.5027e-01)
Epoch: [9][125/492]	Loss 2.0999e-01 (1.4796e-01)
Epoch: [9][150/492]	Loss 1.6611e-01 (1.4933e-01)
Epoch: [9][175/492]	Loss 8.7230e-02 (1.4367e-01)
Epoch: [9][200/492]	Loss 8.0648e-02 (1.3971e-01)
Epoch: [9][225/492]	Loss 1.2848e-01 (1.3547e-01)
Epoch: [9][250/492]	Loss 5.3920e-02 (1.3581e-01)
Epoch: [9][275/492]	Loss 7.6504e-02 (1.3443e-01)
Epoch: [9][300/492]	Loss 8.5469e-02 (1.3366e-01)
Epoch: [9][325/492]	Loss 1.5901e-01 (1.3275e-01)
Epoch: [9][350/492]	Loss 1.1076e-01 (1.3351e-01)
Epoch: [9][375/492]	Loss 8.6999e-02 (1.3282e-01)
Epoch: [9][400/492]	Loss 2.0588e-01 (1.3537e-01)
Epoch: [9][425/492]	Loss 2.1156e-01 (1.3451e-01)
Epoch: [9][450/492]	Loss 1.2928e-01 (1.3312e-01)
Epoch: [9][475/492]	Loss 3.9135e-01 (1.3400e-01)
Checkpoint ...
[33mEpoch 10/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00090
Train ...
Epoch: [10][  0/492]	Loss 6.7795e-02 (6.7795e-02)
Epoch: [10][ 25/492]	Loss 5.4313e-02 (1.4850e-01)
Epoch: [10][ 50/492]	Loss 3.5476e-02 (1.3791e-01)
Epoch: [10][ 75/492]	Loss 6.0410e-02 (1.3264e-01)
Epoch: [10][100/492]	Loss 1.1968e-01 (1.3530e-01)
Epoch: [10][125/492]	Loss 2.6686e-01 (1.3430e-01)
Epoch: [10][150/492]	Loss 8.2280e-02 (1.3175e-01)
Epoch: [10][175/492]	Loss 1.9776e-01 (1.3255e-01)
Epoch: [10][200/492]	Loss 8.0319e-02 (1.3311e-01)
Epoch: [10][225/492]	Loss 8.0588e-02 (1.3433e-01)
Epoch: [10][250/492]	Loss 1.0022e-01 (1.3410e-01)
Epoch: [10][275/492]	Loss 3.6326e-01 (1.3596e-01)
Epoch: [10][300/492]	Loss 9.2614e-02 (1.3583e-01)
Epoch: [10][325/492]	Loss 1.1624e-01 (1.3486e-01)
Epoch: [10][350/492]	Loss 8.9932e-02 (1.3462e-01)
Epoch: [10][375/492]	Loss 2.3005e-01 (1.3454e-01)
Epoch: [10][400/492]	Loss 7.9368e-02 (1.3305e-01)
Epoch: [10][425/492]	Loss 4.4829e-02 (1.3255e-01)
Epoch: [10][450/492]	Loss 5.6603e-02 (1.3169e-01)
Epoch: [10][475/492]	Loss 8.2153e-02 (1.3112e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Evaluate ...
Result of kNN evaluation is 92.80
Checkpoint ...
[33mEpoch 11/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00089
Train ...
Epoch: [11][  0/492]	Loss 1.3823e-01 (1.3823e-01)
Epoch: [11][ 25/492]	Loss 8.5175e-02 (1.2442e-01)
Epoch: [11][ 50/492]	Loss 9.0608e-02 (1.1585e-01)
Epoch: [11][ 75/492]	Loss 5.1869e-02 (1.2174e-01)
Epoch: [11][100/492]	Loss 1.2394e-01 (1.2324e-01)
Epoch: [11][125/492]	Loss 5.6295e-02 (1.2063e-01)
Epoch: [11][150/492]	Loss 1.1171e-01 (1.2148e-01)
Epoch: [11][175/492]	Loss 8.2848e-02 (1.2082e-01)
Epoch: [11][200/492]	Loss 7.6965e-02 (1.2101e-01)
Epoch: [11][225/492]	Loss 4.0315e-02 (1.2200e-01)
Epoch: [11][250/492]	Loss 7.5468e-02 (1.2218e-01)
Epoch: [11][275/492]	Loss 9.7551e-02 (1.2364e-01)
Epoch: [11][300/492]	Loss 1.1680e-01 (1.2118e-01)
Epoch: [11][325/492]	Loss 2.8011e-01 (1.2129e-01)
Epoch: [11][350/492]	Loss 7.7840e-02 (1.2538e-01)
Epoch: [11][375/492]	Loss 2.8411e-01 (1.2675e-01)
Epoch: [11][400/492]	Loss 2.2367e-01 (1.2681e-01)
Epoch: [11][425/492]	Loss 1.8517e-01 (1.2635e-01)
Epoch: [11][450/492]	Loss 9.4199e-02 (1.2678e-01)
Epoch: [11][475/492]	Loss 1.0824e-01 (1.2782e-01)
Checkpoint ...
[33mEpoch 12/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00086
Train ...
Epoch: [12][  0/492]	Loss 7.7863e-02 (7.7863e-02)
Epoch: [12][ 25/492]	Loss 1.2318e-01 (1.3691e-01)
Epoch: [12][ 50/492]	Loss 2.8782e-01 (1.3730e-01)
Epoch: [12][ 75/492]	Loss 2.5077e-01 (1.3922e-01)
Epoch: [12][100/492]	Loss 6.5164e-02 (1.3486e-01)
Epoch: [12][125/492]	Loss 1.0906e-01 (1.3598e-01)
Epoch: [12][150/492]	Loss 2.8759e-01 (1.3500e-01)
Epoch: [12][175/492]	Loss 9.8007e-02 (1.3209e-01)
Epoch: [12][200/492]	Loss 3.6935e-02 (1.2965e-01)
Epoch: [12][225/492]	Loss 1.9808e-01 (1.2889e-01)
Epoch: [12][250/492]	Loss 9.3943e-02 (1.2718e-01)
Epoch: [12][275/492]	Loss 7.0217e-02 (1.2634e-01)
Epoch: [12][300/492]	Loss 5.0754e-02 (1.2657e-01)
Epoch: [12][325/492]	Loss 1.7178e-01 (1.2687e-01)
Epoch: [12][350/492]	Loss 8.3997e-02 (1.2510e-01)
Epoch: [12][375/492]	Loss 5.7898e-02 (1.2375e-01)
Epoch: [12][400/492]	Loss 1.4712e-01 (1.2376e-01)
Epoch: [12][425/492]	Loss 5.6010e-02 (1.2362e-01)
Epoch: [12][450/492]	Loss 1.6020e-01 (1.2266e-01)
Epoch: [12][475/492]	Loss 9.7226e-02 (1.2268e-01)
Checkpoint ...
[33mEpoch 13/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00084
Train ...
Epoch: [13][  0/492]	Loss 1.2396e-01 (1.2396e-01)
Epoch: [13][ 25/492]	Loss 9.2961e-02 (1.1275e-01)
Epoch: [13][ 50/492]	Loss 5.6546e-02 (1.1900e-01)
Epoch: [13][ 75/492]	Loss 1.1248e-01 (1.2553e-01)
Epoch: [13][100/492]	Loss 1.6721e-01 (1.1851e-01)
Epoch: [13][125/492]	Loss 9.3454e-02 (1.1867e-01)
Epoch: [13][150/492]	Loss 4.7767e-02 (1.1417e-01)
Epoch: [13][175/492]	Loss 1.1632e-01 (1.1641e-01)
Epoch: [13][200/492]	Loss 8.4463e-02 (1.1654e-01)
Epoch: [13][225/492]	Loss 2.2202e-01 (1.1850e-01)
Epoch: [13][250/492]	Loss 9.9753e-02 (1.2074e-01)
Epoch: [13][275/492]	Loss 7.3210e-02 (1.1989e-01)
Epoch: [13][300/492]	Loss 1.0766e-01 (1.2150e-01)
Epoch: [13][325/492]	Loss 1.4114e-01 (1.1920e-01)
Epoch: [13][350/492]	Loss 2.0748e-01 (1.1843e-01)
Epoch: [13][375/492]	Loss 1.1035e-01 (1.1868e-01)
Epoch: [13][400/492]	Loss 1.3539e-01 (1.1834e-01)
Epoch: [13][425/492]	Loss 1.0227e-01 (1.1940e-01)
Epoch: [13][450/492]	Loss 5.8859e-02 (1.2159e-01)
Epoch: [13][475/492]	Loss 5.4340e-02 (1.2128e-01)
Checkpoint ...
[33mEpoch 14/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00082
Train ...
Epoch: [14][  0/492]	Loss 7.1904e-02 (7.1904e-02)
Epoch: [14][ 25/492]	Loss 9.2002e-02 (1.0287e-01)
Epoch: [14][ 50/492]	Loss 8.0442e-02 (1.1824e-01)
Epoch: [14][ 75/492]	Loss 3.3460e-01 (1.1460e-01)
Epoch: [14][100/492]	Loss 7.2553e-02 (1.2375e-01)
Epoch: [14][125/492]	Loss 1.0229e-01 (1.2229e-01)
Epoch: [14][150/492]	Loss 1.0927e-01 (1.2128e-01)
Epoch: [14][175/492]	Loss 9.1576e-02 (1.2134e-01)
Epoch: [14][200/492]	Loss 1.9828e-01 (1.2282e-01)
Epoch: [14][225/492]	Loss 2.1659e-01 (1.2397e-01)
Epoch: [14][250/492]	Loss 1.1351e-01 (1.2468e-01)
Epoch: [14][275/492]	Loss 2.1763e-01 (1.2502e-01)
Epoch: [14][300/492]	Loss 8.3647e-02 (1.2452e-01)
Epoch: [14][325/492]	Loss 1.4828e-01 (1.2360e-01)
Epoch: [14][350/492]	Loss 8.9472e-02 (1.2225e-01)
Epoch: [14][375/492]	Loss 1.2344e-01 (1.2333e-01)
Epoch: [14][400/492]	Loss 1.2721e-01 (1.2213e-01)
Epoch: [14][425/492]	Loss 4.6466e-02 (1.2253e-01)
Epoch: [14][450/492]	Loss 4.8299e-02 (1.2179e-01)
Epoch: [14][475/492]	Loss 1.3618e-01 (1.2088e-01)
Checkpoint ...
[33mEpoch 15/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00079
Train ...
Epoch: [15][  0/492]	Loss 1.0324e-01 (1.0324e-01)
Epoch: [15][ 25/492]	Loss 1.3392e-01 (1.0982e-01)
Epoch: [15][ 50/492]	Loss 8.2753e-02 (1.3181e-01)
Epoch: [15][ 75/492]	Loss 8.2839e-02 (1.3427e-01)
Epoch: [15][100/492]	Loss 7.1752e-02 (1.2814e-01)
Epoch: [15][125/492]	Loss 6.0103e-02 (1.2215e-01)
Epoch: [15][150/492]	Loss 1.0446e-01 (1.2239e-01)
Epoch: [15][175/492]	Loss 1.9675e-01 (1.2281e-01)
Epoch: [15][200/492]	Loss 4.6827e-02 (1.2094e-01)
Epoch: [15][225/492]	Loss 1.2861e-01 (1.1975e-01)
Epoch: [15][250/492]	Loss 1.3024e-01 (1.2100e-01)
Epoch: [15][275/492]	Loss 6.7112e-02 (1.2014e-01)
Epoch: [15][300/492]	Loss 1.4135e-01 (1.1939e-01)
Epoch: [15][325/492]	Loss 1.7174e-01 (1.1961e-01)
Epoch: [15][350/492]	Loss 5.6165e-02 (1.1873e-01)
Epoch: [15][375/492]	Loss 1.8834e-01 (1.1773e-01)
Epoch: [15][400/492]	Loss 2.3841e-01 (1.1848e-01)
Epoch: [15][425/492]	Loss 8.1973e-02 (1.1781e-01)
Epoch: [15][450/492]	Loss 6.0587e-02 (1.1845e-01)
Epoch: [15][475/492]	Loss 6.2128e-02 (1.1952e-01)
Checkpoint ...
[33mEpoch 16/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00077
Train ...
Epoch: [16][  0/492]	Loss 5.9758e-02 (5.9758e-02)
Epoch: [16][ 25/492]	Loss 6.7095e-02 (1.1170e-01)
Epoch: [16][ 50/492]	Loss 1.0817e-01 (1.1830e-01)
Epoch: [16][ 75/492]	Loss 1.5866e-01 (1.1337e-01)
Epoch: [16][100/492]	Loss 9.5682e-02 (1.1510e-01)
Epoch: [16][125/492]	Loss 8.6265e-02 (1.1303e-01)
Epoch: [16][150/492]	Loss 2.2308e-01 (1.1307e-01)
Epoch: [16][175/492]	Loss 1.0719e-01 (1.1039e-01)
Epoch: [16][200/492]	Loss 8.8897e-02 (1.1512e-01)
Epoch: [16][225/492]	Loss 2.3107e-01 (1.1526e-01)
Epoch: [16][250/492]	Loss 8.0960e-02 (1.1552e-01)
Epoch: [16][275/492]	Loss 4.6102e-02 (1.1286e-01)
Epoch: [16][300/492]	Loss 1.5841e-01 (1.1341e-01)
Epoch: [16][325/492]	Loss 1.1637e-01 (1.1368e-01)
Epoch: [16][350/492]	Loss 8.1532e-02 (1.1480e-01)
Epoch: [16][375/492]	Loss 6.3016e-02 (1.1401e-01)
Epoch: [16][400/492]	Loss 1.8220e-01 (1.1401e-01)
Epoch: [16][425/492]	Loss 7.0826e-02 (1.1343e-01)
Epoch: [16][450/492]	Loss 1.0415e-01 (1.1252e-01)
Epoch: [16][475/492]	Loss 8.1781e-02 (1.1463e-01)
Checkpoint ...
[33mEpoch 17/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00074
Train ...
Epoch: [17][  0/492]	Loss 2.0779e-01 (2.0779e-01)
Epoch: [17][ 25/492]	Loss 3.7824e-02 (1.0925e-01)
Epoch: [17][ 50/492]	Loss 6.7106e-02 (9.9465e-02)
Epoch: [17][ 75/492]	Loss 6.0836e-02 (1.0168e-01)
Epoch: [17][100/492]	Loss 6.9997e-02 (1.0231e-01)
Epoch: [17][125/492]	Loss 1.8276e-01 (1.0551e-01)
Epoch: [17][150/492]	Loss 1.3678e-01 (1.0600e-01)
Epoch: [17][175/492]	Loss 3.7654e-01 (1.0748e-01)
Epoch: [17][200/492]	Loss 4.3303e-02 (1.0982e-01)
Epoch: [17][225/492]	Loss 6.3921e-02 (1.0917e-01)
Epoch: [17][250/492]	Loss 1.0665e-01 (1.0872e-01)
Epoch: [17][275/492]	Loss 6.3560e-02 (1.0588e-01)
Epoch: [17][300/492]	Loss 3.1905e-01 (1.0826e-01)
Epoch: [17][325/492]	Loss 1.0386e-01 (1.0690e-01)
Epoch: [17][350/492]	Loss 1.2263e-01 (1.0738e-01)
Epoch: [17][375/492]	Loss 8.5419e-02 (1.0802e-01)
Epoch: [17][400/492]	Loss 6.3760e-02 (1.0710e-01)
Epoch: [17][425/492]	Loss 1.2775e-01 (1.0653e-01)
Epoch: [17][450/492]	Loss 4.4180e-02 (1.0704e-01)
Epoch: [17][475/492]	Loss 9.1122e-02 (1.0726e-01)
Checkpoint ...
[33mEpoch 18/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00071
Train ...
Epoch: [18][  0/492]	Loss 9.7215e-02 (9.7215e-02)
Epoch: [18][ 25/492]	Loss 4.8449e-02 (1.0668e-01)
Epoch: [18][ 50/492]	Loss 6.7363e-02 (9.8365e-02)
Epoch: [18][ 75/492]	Loss 4.9032e-02 (1.0681e-01)
Epoch: [18][100/492]	Loss 7.2210e-02 (1.1122e-01)
Epoch: [18][125/492]	Loss 1.1626e-01 (1.0483e-01)
Epoch: [18][150/492]	Loss 1.2285e-01 (1.0301e-01)
Epoch: [18][175/492]	Loss 2.4958e-01 (1.0497e-01)
Epoch: [18][200/492]	Loss 5.3242e-02 (1.0900e-01)
Epoch: [18][225/492]	Loss 3.6993e-02 (1.1032e-01)
Epoch: [18][250/492]	Loss 3.5242e-02 (1.0910e-01)
Epoch: [18][275/492]	Loss 2.3120e-01 (1.0807e-01)
Epoch: [18][300/492]	Loss 8.0153e-02 (1.0749e-01)
Epoch: [18][325/492]	Loss 9.9373e-02 (1.0690e-01)
Epoch: [18][350/492]	Loss 1.3067e-01 (1.0738e-01)
Epoch: [18][375/492]	Loss 9.9405e-02 (1.0787e-01)
Epoch: [18][400/492]	Loss 4.5001e-02 (1.0631e-01)
Epoch: [18][425/492]	Loss 1.4396e-01 (1.0796e-01)
Epoch: [18][450/492]	Loss 1.1389e-01 (1.0888e-01)
Epoch: [18][475/492]	Loss 1.9843e-01 (1.0893e-01)
Checkpoint ...
[33mEpoch 19/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00068
Train ...
Epoch: [19][  0/492]	Loss 8.3801e-02 (8.3801e-02)
Epoch: [19][ 25/492]	Loss 4.1327e-02 (8.8249e-02)
Epoch: [19][ 50/492]	Loss 5.9250e-02 (9.2654e-02)
Epoch: [19][ 75/492]	Loss 1.0548e-01 (1.0564e-01)
Epoch: [19][100/492]	Loss 6.4489e-02 (1.0650e-01)
Epoch: [19][125/492]	Loss 1.0374e-01 (1.0423e-01)
Epoch: [19][150/492]	Loss 1.9614e-01 (1.0287e-01)
Epoch: [19][175/492]	Loss 8.2125e-02 (1.0353e-01)
Epoch: [19][200/492]	Loss 6.4825e-02 (1.0541e-01)
Epoch: [19][225/492]	Loss 8.7849e-02 (1.0563e-01)
Epoch: [19][250/492]	Loss 6.6384e-02 (1.0943e-01)
Epoch: [19][275/492]	Loss 5.7456e-02 (1.0685e-01)
Epoch: [19][300/492]	Loss 1.3342e-01 (1.0947e-01)
Epoch: [19][325/492]	Loss 9.2249e-02 (1.0762e-01)
Epoch: [19][350/492]	Loss 4.3742e-02 (1.0780e-01)
Epoch: [19][375/492]	Loss 5.1772e-02 (1.0869e-01)
Epoch: [19][400/492]	Loss 7.7923e-02 (1.0875e-01)
Epoch: [19][425/492]	Loss 2.1073e-01 (1.0761e-01)
Epoch: [19][450/492]	Loss 4.7189e-01 (1.0765e-01)
Epoch: [19][475/492]	Loss 1.1175e-01 (1.0744e-01)
Checkpoint ...
[33mEpoch 20/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00065
Train ...
Epoch: [20][  0/492]	Loss 6.5898e-02 (6.5898e-02)
Epoch: [20][ 25/492]	Loss 9.1574e-02 (1.0371e-01)
Epoch: [20][ 50/492]	Loss 1.6774e-01 (1.1390e-01)
Epoch: [20][ 75/492]	Loss 7.3498e-02 (1.0432e-01)
Epoch: [20][100/492]	Loss 2.1067e-01 (1.0621e-01)
Epoch: [20][125/492]	Loss 7.8506e-02 (1.0637e-01)
Epoch: [20][150/492]	Loss 1.1673e-01 (1.0631e-01)
Epoch: [20][175/492]	Loss 5.0866e-02 (1.0615e-01)
Epoch: [20][200/492]	Loss 1.0153e-01 (1.0975e-01)
Epoch: [20][225/492]	Loss 9.8213e-02 (1.1014e-01)
Epoch: [20][250/492]	Loss 4.6149e-02 (1.0863e-01)
Epoch: [20][275/492]	Loss 5.8498e-02 (1.0806e-01)
Epoch: [20][300/492]	Loss 2.1889e-01 (1.0846e-01)
Epoch: [20][325/492]	Loss 5.2149e-02 (1.0782e-01)
Epoch: [20][350/492]	Loss 7.4579e-02 (1.0731e-01)
Epoch: [20][375/492]	Loss 5.2647e-02 (1.0628e-01)
Epoch: [20][400/492]	Loss 5.9431e-02 (1.0729e-01)
Epoch: [20][425/492]	Loss 9.0168e-02 (1.0699e-01)
Epoch: [20][450/492]	Loss 9.7796e-02 (1.0836e-01)
Epoch: [20][475/492]	Loss 4.0314e-02 (1.0782e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Evaluate ...
Result of kNN evaluation is 95.75
Checkpoint ...
[33mEpoch 21/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00062
Train ...
Epoch: [21][  0/492]	Loss 1.7468e-01 (1.7468e-01)
Epoch: [21][ 25/492]	Loss 4.1172e-02 (1.1364e-01)
Epoch: [21][ 50/492]	Loss 8.9856e-02 (1.0307e-01)
Epoch: [21][ 75/492]	Loss 8.0489e-02 (1.0385e-01)
Epoch: [21][100/492]	Loss 6.6146e-02 (1.0296e-01)
Epoch: [21][125/492]	Loss 5.8697e-02 (1.0184e-01)
Epoch: [21][150/492]	Loss 6.0525e-02 (9.8646e-02)
Epoch: [21][175/492]	Loss 6.1753e-02 (9.9821e-02)
Epoch: [21][200/492]	Loss 4.5352e-01 (1.0259e-01)
Epoch: [21][225/492]	Loss 1.0774e-01 (1.0439e-01)
Epoch: [21][250/492]	Loss 1.0937e-01 (1.0212e-01)
Epoch: [21][275/492]	Loss 1.6722e-01 (1.0207e-01)
Epoch: [21][300/492]	Loss 7.1801e-02 (1.0074e-01)
Epoch: [21][325/492]	Loss 4.2403e-02 (1.0245e-01)
Epoch: [21][350/492]	Loss 4.7919e-02 (1.0234e-01)
Epoch: [21][375/492]	Loss 7.2739e-02 (1.0268e-01)
Epoch: [21][400/492]	Loss 1.4682e-01 (1.0280e-01)
Epoch: [21][425/492]	Loss 4.2123e-02 (1.0240e-01)
Epoch: [21][450/492]	Loss 5.7505e-02 (1.0308e-01)
Epoch: [21][475/492]	Loss 1.7728e-01 (1.0305e-01)
Checkpoint ...
[33mEpoch 22/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00059
Train ...
Epoch: [22][  0/492]	Loss 2.6028e-02 (2.6028e-02)
Epoch: [22][ 25/492]	Loss 1.3228e-01 (9.9563e-02)
Epoch: [22][ 50/492]	Loss 5.0463e-02 (1.0052e-01)
Epoch: [22][ 75/492]	Loss 1.0539e-01 (9.8543e-02)
Epoch: [22][100/492]	Loss 8.1041e-02 (9.2869e-02)
Epoch: [22][125/492]	Loss 1.1698e-01 (9.5142e-02)
Epoch: [22][150/492]	Loss 4.2121e-02 (9.3474e-02)
Epoch: [22][175/492]	Loss 1.1498e-01 (9.5293e-02)
Epoch: [22][200/492]	Loss 6.5564e-02 (9.7826e-02)
Epoch: [22][225/492]	Loss 4.9532e-02 (9.8332e-02)
Epoch: [22][250/492]	Loss 4.6366e-02 (1.0049e-01)
Epoch: [22][275/492]	Loss 3.9968e-02 (1.0036e-01)
Epoch: [22][300/492]	Loss 6.7782e-02 (9.9337e-02)
Epoch: [22][325/492]	Loss 5.5577e-02 (9.7882e-02)
Epoch: [22][350/492]	Loss 1.7721e-01 (9.9045e-02)
Epoch: [22][375/492]	Loss 1.0046e-01 (1.0119e-01)
Epoch: [22][400/492]	Loss 1.7439e-01 (1.0173e-01)
Epoch: [22][425/492]	Loss 8.1887e-02 (1.0062e-01)
Epoch: [22][450/492]	Loss 9.0817e-02 (1.0110e-01)
Epoch: [22][475/492]	Loss 3.3840e-02 (9.9932e-02)
Checkpoint ...
[33mEpoch 23/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00056
Train ...
Epoch: [23][  0/492]	Loss 9.1419e-02 (9.1419e-02)
Epoch: [23][ 25/492]	Loss 7.0273e-02 (9.4541e-02)
Epoch: [23][ 50/492]	Loss 8.5693e-02 (9.1205e-02)
Epoch: [23][ 75/492]	Loss 3.6311e-02 (1.0150e-01)
Epoch: [23][100/492]	Loss 2.3900e-01 (1.0546e-01)
Epoch: [23][125/492]	Loss 1.0372e-01 (1.0797e-01)
Epoch: [23][150/492]	Loss 6.2356e-02 (1.0721e-01)
Epoch: [23][175/492]	Loss 5.9072e-02 (1.0573e-01)
Epoch: [23][200/492]	Loss 6.1404e-02 (1.0726e-01)
Epoch: [23][225/492]	Loss 2.1398e-01 (1.0585e-01)
Epoch: [23][250/492]	Loss 1.5673e-01 (1.0377e-01)
Epoch: [23][275/492]	Loss 6.5170e-02 (1.0206e-01)
Epoch: [23][300/492]	Loss 1.6438e-01 (1.0128e-01)
Epoch: [23][325/492]	Loss 6.0638e-02 (1.0064e-01)
Epoch: [23][350/492]	Loss 6.7873e-02 (9.9378e-02)
Epoch: [23][375/492]	Loss 5.2064e-02 (9.8507e-02)
Epoch: [23][400/492]	Loss 3.1684e-02 (9.9546e-02)
Epoch: [23][425/492]	Loss 6.9568e-02 (9.9603e-02)
Epoch: [23][450/492]	Loss 2.5685e-01 (9.9469e-02)
Epoch: [23][475/492]	Loss 6.0140e-02 (9.9688e-02)
Checkpoint ...
[33mEpoch 24/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00053
Train ...
Epoch: [24][  0/492]	Loss 7.2734e-02 (7.2734e-02)
Epoch: [24][ 25/492]	Loss 2.0239e-01 (1.1463e-01)
Epoch: [24][ 50/492]	Loss 1.0343e-01 (1.0462e-01)
Epoch: [24][ 75/492]	Loss 8.0960e-02 (1.0320e-01)
Epoch: [24][100/492]	Loss 4.1990e-02 (1.0197e-01)
Epoch: [24][125/492]	Loss 6.0688e-02 (1.0287e-01)
Epoch: [24][150/492]	Loss 1.7479e-01 (1.0934e-01)
Epoch: [24][175/492]	Loss 1.2016e-01 (1.0821e-01)
Epoch: [24][200/492]	Loss 7.4865e-02 (1.0640e-01)
Epoch: [24][225/492]	Loss 3.8751e-02 (1.0406e-01)
Epoch: [24][250/492]	Loss 7.3721e-02 (1.0616e-01)
Epoch: [24][275/492]	Loss 8.9829e-02 (1.0599e-01)
Epoch: [24][300/492]	Loss 7.5862e-02 (1.0386e-01)
Epoch: [24][325/492]	Loss 5.1817e-02 (1.0249e-01)
Epoch: [24][350/492]	Loss 1.2303e-01 (1.0224e-01)
Epoch: [24][375/492]	Loss 7.0993e-02 (1.0151e-01)
Epoch: [24][400/492]	Loss 7.5669e-02 (1.0022e-01)
Epoch: [24][425/492]	Loss 7.2369e-02 (9.9936e-02)
Epoch: [24][450/492]	Loss 3.8847e-02 (1.0021e-01)
Epoch: [24][475/492]	Loss 5.3429e-02 (1.0036e-01)
Checkpoint ...
[33mEpoch 25/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00050
Train ...
Epoch: [25][  0/492]	Loss 8.1656e-02 (8.1656e-02)
Epoch: [25][ 25/492]	Loss 6.7215e-02 (1.0867e-01)
Epoch: [25][ 50/492]	Loss 5.1429e-02 (1.0187e-01)
Epoch: [25][ 75/492]	Loss 3.9083e-02 (1.0045e-01)
Epoch: [25][100/492]	Loss 1.0337e-01 (9.8877e-02)
Epoch: [25][125/492]	Loss 7.8600e-02 (1.0031e-01)
Epoch: [25][150/492]	Loss 1.7602e-01 (9.9324e-02)
Epoch: [25][175/492]	Loss 1.3409e-01 (9.8033e-02)
Epoch: [25][200/492]	Loss 7.4373e-02 (9.4860e-02)
Epoch: [25][225/492]	Loss 6.4581e-02 (9.6638e-02)
Epoch: [25][250/492]	Loss 8.7827e-02 (9.5363e-02)
Epoch: [25][275/492]	Loss 7.3005e-02 (9.5680e-02)
Epoch: [25][300/492]	Loss 7.6296e-02 (9.5999e-02)
Epoch: [25][325/492]	Loss 4.9244e-02 (9.6450e-02)
Epoch: [25][350/492]	Loss 1.1855e-01 (9.6099e-02)
Epoch: [25][375/492]	Loss 5.7202e-02 (9.5394e-02)
Epoch: [25][400/492]	Loss 4.0198e-02 (9.4083e-02)
Epoch: [25][425/492]	Loss 1.6025e-01 (9.4920e-02)
Epoch: [25][450/492]	Loss 6.6466e-02 (9.5085e-02)
Epoch: [25][475/492]	Loss 9.6863e-02 (9.4208e-02)
Checkpoint ...
[33mEpoch 26/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00047
Train ...
Epoch: [26][  0/492]	Loss 2.7746e-01 (2.7746e-01)
Epoch: [26][ 25/492]	Loss 6.4510e-02 (1.2141e-01)
Epoch: [26][ 50/492]	Loss 2.6205e-01 (1.0703e-01)
Epoch: [26][ 75/492]	Loss 6.2566e-02 (1.0469e-01)
Epoch: [26][100/492]	Loss 7.2158e-02 (9.8622e-02)
Epoch: [26][125/492]	Loss 3.9467e-02 (1.0118e-01)
Epoch: [26][150/492]	Loss 1.4099e-01 (9.9091e-02)
Epoch: [26][175/492]	Loss 7.1709e-02 (9.7653e-02)
Epoch: [26][200/492]	Loss 2.8537e-02 (9.5367e-02)
Epoch: [26][225/492]	Loss 5.3703e-02 (9.3491e-02)
Epoch: [26][250/492]	Loss 7.4032e-02 (9.5579e-02)
Epoch: [26][275/492]	Loss 1.0832e-01 (9.4903e-02)
Epoch: [26][300/492]	Loss 5.1070e-02 (9.6761e-02)
Epoch: [26][325/492]	Loss 2.4683e-02 (9.7058e-02)
Epoch: [26][350/492]	Loss 4.9348e-02 (9.7774e-02)
Epoch: [26][375/492]	Loss 8.6796e-02 (9.6821e-02)
Epoch: [26][400/492]	Loss 4.4840e-02 (9.5749e-02)
Epoch: [26][425/492]	Loss 7.4145e-02 (9.6584e-02)
Epoch: [26][450/492]	Loss 1.4927e-01 (9.6270e-02)
Epoch: [26][475/492]	Loss 6.9550e-02 (9.5316e-02)
Checkpoint ...
[33mEpoch 27/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00044
Train ...
Epoch: [27][  0/492]	Loss 1.7875e-01 (1.7875e-01)
Epoch: [27][ 25/492]	Loss 9.3221e-02 (8.0564e-02)
Epoch: [27][ 50/492]	Loss 2.9276e-02 (9.2634e-02)
Epoch: [27][ 75/492]	Loss 6.8707e-02 (1.0163e-01)
Epoch: [27][100/492]	Loss 8.8079e-02 (1.0094e-01)
Epoch: [27][125/492]	Loss 8.7982e-02 (9.9850e-02)
Epoch: [27][150/492]	Loss 8.5201e-02 (9.8973e-02)
Epoch: [27][175/492]	Loss 8.0798e-02 (1.0283e-01)
Epoch: [27][200/492]	Loss 6.6307e-02 (1.0528e-01)
Epoch: [27][225/492]	Loss 1.3291e-01 (1.0594e-01)
Epoch: [27][250/492]	Loss 1.1473e-01 (1.0561e-01)
Epoch: [27][275/492]	Loss 5.9248e-02 (1.0664e-01)
Epoch: [27][300/492]	Loss 1.0170e-01 (1.0581e-01)
Epoch: [27][325/492]	Loss 2.5304e-01 (1.0380e-01)
Epoch: [27][350/492]	Loss 4.9630e-02 (1.0238e-01)
Epoch: [27][375/492]	Loss 6.7638e-02 (1.0133e-01)
Epoch: [27][400/492]	Loss 5.1250e-02 (1.0002e-01)
Epoch: [27][425/492]	Loss 1.6611e-01 (1.0095e-01)
Epoch: [27][450/492]	Loss 5.2576e-02 (1.0114e-01)
Epoch: [27][475/492]	Loss 4.7838e-02 (1.0122e-01)
Checkpoint ...
[33mEpoch 28/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00041
Train ...
Epoch: [28][  0/492]	Loss 4.5686e-02 (4.5686e-02)
Epoch: [28][ 25/492]	Loss 1.2754e-01 (1.0026e-01)
Epoch: [28][ 50/492]	Loss 3.6037e-01 (1.0264e-01)
Epoch: [28][ 75/492]	Loss 4.5558e-02 (1.0836e-01)
Epoch: [28][100/492]	Loss 6.6855e-02 (1.0387e-01)
Epoch: [28][125/492]	Loss 5.1803e-02 (1.0057e-01)
Epoch: [28][150/492]	Loss 4.6491e-02 (9.8524e-02)
Epoch: [28][175/492]	Loss 7.1567e-02 (9.9858e-02)
Epoch: [28][200/492]	Loss 5.2478e-02 (1.0147e-01)
Epoch: [28][225/492]	Loss 5.5222e-02 (1.0050e-01)
Epoch: [28][250/492]	Loss 1.1147e-01 (1.0176e-01)
Epoch: [28][275/492]	Loss 1.3201e-01 (1.0119e-01)
Epoch: [28][300/492]	Loss 4.3179e-02 (1.0140e-01)
Epoch: [28][325/492]	Loss 4.0251e-02 (1.0274e-01)
Epoch: [28][350/492]	Loss 4.9502e-02 (1.0076e-01)
Epoch: [28][375/492]	Loss 5.3170e-02 (1.0104e-01)
Epoch: [28][400/492]	Loss 8.0013e-02 (1.0102e-01)
Epoch: [28][425/492]	Loss 4.6600e-02 (1.0086e-01)
Epoch: [28][450/492]	Loss 9.3108e-02 (1.0031e-01)
Epoch: [28][475/492]	Loss 1.4630e-01 (1.0087e-01)
Checkpoint ...
[33mEpoch 29/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00038
Train ...
Epoch: [29][  0/492]	Loss 8.5860e-02 (8.5860e-02)
Epoch: [29][ 25/492]	Loss 2.5837e-01 (1.1904e-01)
Epoch: [29][ 50/492]	Loss 1.3192e-01 (1.0817e-01)
Epoch: [29][ 75/492]	Loss 4.6800e-02 (9.7927e-02)
Epoch: [29][100/492]	Loss 5.5062e-02 (9.9837e-02)
Epoch: [29][125/492]	Loss 4.4296e-02 (9.9786e-02)
Epoch: [29][150/492]	Loss 1.5641e-01 (9.6721e-02)
Epoch: [29][175/492]	Loss 2.7528e-02 (9.5103e-02)
Epoch: [29][200/492]	Loss 3.7113e-02 (9.4450e-02)
Epoch: [29][225/492]	Loss 5.6462e-02 (9.4751e-02)
Epoch: [29][250/492]	Loss 1.1849e-01 (9.5507e-02)
Epoch: [29][275/492]	Loss 2.7889e-01 (9.4073e-02)
Epoch: [29][300/492]	Loss 2.2567e-01 (9.6215e-02)
Epoch: [29][325/492]	Loss 5.1182e-02 (9.7322e-02)
Epoch: [29][350/492]	Loss 4.7299e-02 (9.5014e-02)
Epoch: [29][375/492]	Loss 8.4366e-02 (9.5329e-02)
Epoch: [29][400/492]	Loss 7.2980e-02 (9.6273e-02)
Epoch: [29][425/492]	Loss 7.1388e-02 (9.5445e-02)
Epoch: [29][450/492]	Loss 6.7658e-02 (9.4476e-02)
Epoch: [29][475/492]	Loss 4.1120e-02 (9.3257e-02)
Checkpoint ...
[33mEpoch 30/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00035
Train ...
Epoch: [30][  0/492]	Loss 7.8054e-02 (7.8054e-02)
Epoch: [30][ 25/492]	Loss 6.7405e-02 (9.6947e-02)
Epoch: [30][ 50/492]	Loss 1.2274e-01 (9.3128e-02)
Epoch: [30][ 75/492]	Loss 6.5369e-02 (9.6153e-02)
Epoch: [30][100/492]	Loss 1.7693e-01 (1.0574e-01)
Epoch: [30][125/492]	Loss 7.4277e-02 (1.0690e-01)
Epoch: [30][150/492]	Loss 1.2215e-01 (1.0232e-01)
Epoch: [30][175/492]	Loss 1.3413e-01 (1.0213e-01)
Epoch: [30][200/492]	Loss 6.9946e-02 (9.9753e-02)
Epoch: [30][225/492]	Loss 1.7461e-01 (9.7002e-02)
Epoch: [30][250/492]	Loss 3.8320e-02 (9.4890e-02)
Epoch: [30][275/492]	Loss 4.6045e-02 (9.3373e-02)
Epoch: [30][300/492]	Loss 1.9739e-01 (9.2431e-02)
Epoch: [30][325/492]	Loss 6.3584e-02 (9.2847e-02)
Epoch: [30][350/492]	Loss 4.7445e-02 (9.1061e-02)
Epoch: [30][375/492]	Loss 3.5134e-02 (9.0967e-02)
Epoch: [30][400/492]	Loss 5.6970e-02 (9.1477e-02)
Epoch: [30][425/492]	Loss 1.5822e-01 (9.2527e-02)
Epoch: [30][450/492]	Loss 6.4242e-02 (9.2239e-02)
Epoch: [30][475/492]	Loss 3.0016e-02 (9.1616e-02)
Fill memory bank for kNN...
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Evaluate ...
Result of kNN evaluation is 95.18
Checkpoint ...
[33mEpoch 31/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00032
Train ...
Epoch: [31][  0/492]	Loss 4.1812e-02 (4.1812e-02)
Epoch: [31][ 25/492]	Loss 1.7033e-01 (1.1097e-01)
Epoch: [31][ 50/492]	Loss 7.4122e-02 (9.8831e-02)
Epoch: [31][ 75/492]	Loss 5.4842e-02 (1.0074e-01)
Epoch: [31][100/492]	Loss 4.8504e-02 (9.3880e-02)
Epoch: [31][125/492]	Loss 6.0152e-02 (9.2348e-02)
Epoch: [31][150/492]	Loss 5.4815e-02 (9.2563e-02)
Epoch: [31][175/492]	Loss 5.9391e-02 (9.2049e-02)
Epoch: [31][200/492]	Loss 1.5249e-01 (9.4308e-02)
Epoch: [31][225/492]	Loss 2.7322e-02 (9.2896e-02)
Epoch: [31][250/492]	Loss 6.5235e-02 (9.1684e-02)
Epoch: [31][275/492]	Loss 3.6041e-02 (9.2253e-02)
Epoch: [31][300/492]	Loss 8.3736e-02 (9.3195e-02)
Epoch: [31][325/492]	Loss 1.5922e-01 (9.2971e-02)
Epoch: [31][350/492]	Loss 2.6613e-02 (9.2516e-02)
Epoch: [31][375/492]	Loss 6.7144e-02 (9.2328e-02)
Epoch: [31][400/492]	Loss 6.8604e-02 (9.2707e-02)
Epoch: [31][425/492]	Loss 5.3250e-02 (9.1570e-02)
Epoch: [31][450/492]	Loss 2.7219e-01 (9.1341e-02)
Epoch: [31][475/492]	Loss 4.7799e-02 (9.1179e-02)
Checkpoint ...
[33mEpoch 32/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00029
Train ...
Epoch: [32][  0/492]	Loss 7.3552e-02 (7.3552e-02)
Epoch: [32][ 25/492]	Loss 9.3974e-02 (8.6817e-02)
Epoch: [32][ 50/492]	Loss 9.2820e-02 (9.4501e-02)
Epoch: [32][ 75/492]	Loss 4.1962e-02 (8.9805e-02)
Epoch: [32][100/492]	Loss 3.5365e-02 (9.3754e-02)
Epoch: [32][125/492]	Loss 1.4927e-01 (9.5078e-02)
Epoch: [32][150/492]	Loss 9.9967e-02 (9.2915e-02)
Epoch: [32][175/492]	Loss 4.4331e-02 (9.1850e-02)
Epoch: [32][200/492]	Loss 1.3190e-01 (9.0885e-02)
Epoch: [32][225/492]	Loss 3.6221e-02 (9.0137e-02)
Epoch: [32][250/492]	Loss 6.3354e-02 (9.5823e-02)
Epoch: [32][275/492]	Loss 7.0739e-02 (9.6074e-02)
Epoch: [32][300/492]	Loss 6.8629e-02 (9.9787e-02)
Epoch: [32][325/492]	Loss 5.2915e-02 (9.8540e-02)
Epoch: [32][350/492]	Loss 3.7786e-02 (9.7706e-02)
Epoch: [32][375/492]	Loss 4.1146e-02 (9.8003e-02)
Epoch: [32][400/492]	Loss 4.4289e-02 (9.8899e-02)
Epoch: [32][425/492]	Loss 4.7579e-02 (9.7327e-02)
Epoch: [32][450/492]	Loss 3.3594e-02 (9.6344e-02)
Epoch: [32][475/492]	Loss 6.0837e-02 (9.5365e-02)
Checkpoint ...
[33mEpoch 33/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00026
Train ...
Epoch: [33][  0/492]	Loss 4.7370e-02 (4.7370e-02)
Epoch: [33][ 25/492]	Loss 6.9765e-02 (8.6358e-02)
Epoch: [33][ 50/492]	Loss 2.0689e-01 (7.9661e-02)
Epoch: [33][ 75/492]	Loss 4.5015e-02 (7.7996e-02)
Epoch: [33][100/492]	Loss 9.9490e-02 (8.2016e-02)
Epoch: [33][125/492]	Loss 3.2042e-02 (8.5508e-02)
Epoch: [33][150/492]	Loss 7.7840e-02 (8.3951e-02)
Epoch: [33][175/492]	Loss 6.8149e-02 (8.1500e-02)
Epoch: [33][200/492]	Loss 4.7411e-02 (8.0952e-02)
Epoch: [33][225/492]	Loss 3.7016e-02 (8.2087e-02)
Epoch: [33][250/492]	Loss 2.1762e-01 (8.2788e-02)
Epoch: [33][275/492]	Loss 1.7463e-01 (8.3426e-02)
Epoch: [33][300/492]	Loss 5.0207e-02 (8.4033e-02)
Epoch: [33][325/492]	Loss 8.7148e-02 (8.3846e-02)
Epoch: [33][350/492]	Loss 4.8497e-02 (8.3847e-02)
Epoch: [33][375/492]	Loss 6.8275e-02 (8.4301e-02)
Epoch: [33][400/492]	Loss 6.8639e-02 (8.6558e-02)
Epoch: [33][425/492]	Loss 1.8614e-01 (8.6493e-02)
Epoch: [33][450/492]	Loss 3.9152e-02 (8.6198e-02)
Epoch: [33][475/492]	Loss 4.5019e-02 (8.5464e-02)
Checkpoint ...
[33mEpoch 34/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00023
Train ...
Epoch: [34][  0/492]	Loss 2.7098e-02 (2.7098e-02)
Epoch: [34][ 25/492]	Loss 1.6552e-01 (9.1096e-02)
Epoch: [34][ 50/492]	Loss 3.3673e-02 (9.7801e-02)
Epoch: [34][ 75/492]	Loss 6.7485e-02 (9.5300e-02)
Epoch: [34][100/492]	Loss 7.0117e-02 (8.9672e-02)
Epoch: [34][125/492]	Loss 5.4989e-02 (9.4280e-02)
Epoch: [34][150/492]	Loss 1.0589e-01 (9.2717e-02)
Epoch: [34][175/492]	Loss 5.1913e-02 (9.3391e-02)
Epoch: [34][200/492]	Loss 1.4315e-01 (9.4136e-02)
Epoch: [34][225/492]	Loss 1.0798e-01 (9.6123e-02)
Epoch: [34][250/492]	Loss 3.6817e-02 (9.3647e-02)
Epoch: [34][275/492]	Loss 1.7698e-01 (9.3252e-02)
Epoch: [34][300/492]	Loss 1.6756e-01 (9.3163e-02)
Epoch: [34][325/492]	Loss 1.2843e-01 (9.2287e-02)
Epoch: [34][350/492]	Loss 1.5895e-01 (9.2599e-02)
Epoch: [34][375/492]	Loss 4.2821e-02 (9.3571e-02)
Epoch: [34][400/492]	Loss 8.3193e-02 (9.3211e-02)
Epoch: [34][425/492]	Loss 4.8025e-02 (9.2631e-02)
Epoch: [34][450/492]	Loss 6.0734e-02 (9.2746e-02)
Epoch: [34][475/492]	Loss 5.3481e-02 (9.2755e-02)
Checkpoint ...
[33mEpoch 35/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00021
Train ...
Epoch: [35][  0/492]	Loss 3.1490e-02 (3.1490e-02)
Epoch: [35][ 25/492]	Loss 4.6620e-02 (7.9350e-02)
Epoch: [35][ 50/492]	Loss 4.4353e-02 (8.8489e-02)
Epoch: [35][ 75/492]	Loss 6.6207e-02 (9.7289e-02)
Epoch: [35][100/492]	Loss 5.2793e-02 (9.8365e-02)
Epoch: [35][125/492]	Loss 5.4273e-02 (9.9121e-02)
Epoch: [35][150/492]	Loss 3.3303e-02 (9.5532e-02)
Epoch: [35][175/492]	Loss 3.2923e-01 (9.6535e-02)
Epoch: [35][200/492]	Loss 5.8252e-01 (9.9215e-02)
Epoch: [35][225/492]	Loss 3.3527e-02 (9.6287e-02)
Epoch: [35][250/492]	Loss 3.9137e-02 (9.8390e-02)
Epoch: [35][275/492]	Loss 2.8988e-02 (9.8362e-02)
Epoch: [35][300/492]	Loss 6.9814e-02 (9.6762e-02)
Epoch: [35][325/492]	Loss 1.1925e-01 (9.7700e-02)
Epoch: [35][350/492]	Loss 1.1689e-01 (9.7377e-02)
Epoch: [35][375/492]	Loss 9.5157e-02 (9.7552e-02)
Epoch: [35][400/492]	Loss 3.6367e-02 (9.6915e-02)
Epoch: [35][425/492]	Loss 5.6811e-02 (9.6208e-02)
Epoch: [35][450/492]	Loss 6.2555e-02 (9.6408e-02)
Epoch: [35][475/492]	Loss 5.3177e-02 (9.6906e-02)
Checkpoint ...
[33mEpoch 36/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00018
Train ...
Epoch: [36][  0/492]	Loss 5.0574e-02 (5.0574e-02)
Epoch: [36][ 25/492]	Loss 9.4031e-02 (7.8857e-02)
Epoch: [36][ 50/492]	Loss 4.6875e-02 (8.1423e-02)
Epoch: [36][ 75/492]	Loss 4.4182e-02 (7.9967e-02)
Epoch: [36][100/492]	Loss 4.6737e-02 (8.1012e-02)
Epoch: [36][125/492]	Loss 1.0420e-01 (7.8715e-02)
Epoch: [36][150/492]	Loss 9.1250e-02 (7.8157e-02)
Epoch: [36][175/492]	Loss 1.5147e-01 (8.1228e-02)
Epoch: [36][200/492]	Loss 4.2953e-02 (8.3214e-02)
Epoch: [36][225/492]	Loss 1.1828e-01 (8.2934e-02)
Epoch: [36][250/492]	Loss 6.9530e-02 (8.3119e-02)
Epoch: [36][275/492]	Loss 3.3739e-02 (8.2056e-02)
Epoch: [36][300/492]	Loss 6.4490e-02 (8.3620e-02)
Epoch: [36][325/492]	Loss 1.5128e-01 (8.3661e-02)
Epoch: [36][350/492]	Loss 7.3955e-02 (8.3375e-02)
Epoch: [36][375/492]	Loss 5.1406e-02 (8.3370e-02)
Epoch: [36][400/492]	Loss 1.7212e-01 (8.4964e-02)
Epoch: [36][425/492]	Loss 9.9475e-02 (8.5832e-02)
Epoch: [36][450/492]	Loss 9.7029e-02 (8.6124e-02)
Epoch: [36][475/492]	Loss 1.4285e-01 (8.7821e-02)
Checkpoint ...
[33mEpoch 37/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00016
Train ...
Epoch: [37][  0/492]	Loss 8.3906e-02 (8.3906e-02)
Epoch: [37][ 25/492]	Loss 3.3098e-02 (7.2865e-02)
Epoch: [37][ 50/492]	Loss 9.9075e-02 (8.5719e-02)
Epoch: [37][ 75/492]	Loss 6.8268e-02 (8.4016e-02)
Epoch: [37][100/492]	Loss 4.4117e-02 (8.4930e-02)
Epoch: [37][125/492]	Loss 5.6566e-02 (8.2281e-02)
Epoch: [37][150/492]	Loss 1.9132e-01 (8.4888e-02)
Epoch: [37][175/492]	Loss 4.2976e-02 (8.5442e-02)
Epoch: [37][200/492]	Loss 5.1884e-02 (8.5859e-02)
Epoch: [37][225/492]	Loss 3.7755e-02 (8.5201e-02)
Epoch: [37][250/492]	Loss 1.8655e-01 (8.4129e-02)
Epoch: [37][275/492]	Loss 5.2592e-02 (8.4771e-02)
Epoch: [37][300/492]	Loss 1.6901e-01 (8.4601e-02)
Epoch: [37][325/492]	Loss 1.2519e-01 (8.4497e-02)
Epoch: [37][350/492]	Loss 5.9019e-02 (8.4345e-02)
Epoch: [37][375/492]	Loss 6.2654e-02 (8.4211e-02)
Epoch: [37][400/492]	Loss 4.2015e-02 (8.4221e-02)
Epoch: [37][425/492]	Loss 6.2390e-02 (8.3974e-02)
Epoch: [37][450/492]	Loss 3.1940e-02 (8.4294e-02)
Epoch: [37][475/492]	Loss 1.1984e-01 (8.4579e-02)
Checkpoint ...
[33mEpoch 38/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00014
Train ...
Epoch: [38][  0/492]	Loss 1.1884e-01 (1.1884e-01)
Epoch: [38][ 25/492]	Loss 5.9156e-02 (1.0210e-01)
Epoch: [38][ 50/492]	Loss 5.5690e-02 (9.3887e-02)
Epoch: [38][ 75/492]	Loss 4.8387e-02 (9.2218e-02)
Epoch: [38][100/492]	Loss 6.6232e-02 (9.6708e-02)
Epoch: [38][125/492]	Loss 5.6239e-02 (9.1172e-02)
Epoch: [38][150/492]	Loss 6.3912e-02 (8.9473e-02)
Epoch: [38][175/492]	Loss 1.4360e-01 (8.8482e-02)
Epoch: [38][200/492]	Loss 1.0406e-01 (8.8862e-02)
Epoch: [38][225/492]	Loss 9.6596e-02 (8.7349e-02)
Epoch: [38][250/492]	Loss 5.5511e-02 (9.1652e-02)
Epoch: [38][275/492]	Loss 1.0032e-01 (9.1791e-02)
Epoch: [38][300/492]	Loss 3.2990e-02 (9.0587e-02)
Epoch: [38][325/492]	Loss 1.6327e-01 (8.9462e-02)
Epoch: [38][350/492]	Loss 4.4045e-02 (8.8296e-02)
Epoch: [38][375/492]	Loss 3.1395e-02 (8.8709e-02)
Epoch: [38][400/492]	Loss 8.3635e-02 (8.8140e-02)
Epoch: [38][425/492]	Loss 3.7439e-02 (8.7958e-02)
Epoch: [38][450/492]	Loss 8.7066e-02 (8.7919e-02)
Epoch: [38][475/492]	Loss 6.0819e-02 (8.7151e-02)
Checkpoint ...
[33mEpoch 39/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00012
Train ...
Epoch: [39][  0/492]	Loss 5.6798e-02 (5.6798e-02)
Epoch: [39][ 25/492]	Loss 2.0527e-01 (8.6582e-02)
Epoch: [39][ 50/492]	Loss 4.3616e-02 (7.8543e-02)
Epoch: [39][ 75/492]	Loss 9.4763e-02 (7.9097e-02)
Epoch: [39][100/492]	Loss 1.2706e-01 (8.2784e-02)
Epoch: [39][125/492]	Loss 3.5989e-02 (8.1794e-02)
Epoch: [39][150/492]	Loss 4.7984e-02 (8.2066e-02)
Epoch: [39][175/492]	Loss 3.8677e-01 (8.5532e-02)
Epoch: [39][200/492]	Loss 3.8051e-02 (8.6311e-02)
Epoch: [39][225/492]	Loss 1.9330e-01 (8.5843e-02)
Epoch: [39][250/492]	Loss 6.9101e-02 (8.6752e-02)
Epoch: [39][275/492]	Loss 7.7598e-02 (8.5470e-02)
Epoch: [39][300/492]	Loss 5.4426e-02 (8.6464e-02)
Epoch: [39][325/492]	Loss 6.4585e-02 (8.6282e-02)
Epoch: [39][350/492]	Loss 9.6920e-02 (8.7911e-02)
Epoch: [39][375/492]	Loss 6.5583e-02 (8.6858e-02)
Epoch: [39][400/492]	Loss 6.6328e-02 (8.6233e-02)
Epoch: [39][425/492]	Loss 7.3445e-02 (8.6311e-02)
Epoch: [39][450/492]	Loss 1.3811e-01 (8.5625e-02)
Epoch: [39][475/492]	Loss 4.4307e-02 (8.6329e-02)
Checkpoint ...
[33mEpoch 40/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00010
Train ...
Epoch: [40][  0/492]	Loss 9.5375e-02 (9.5375e-02)
Epoch: [40][ 25/492]	Loss 6.2657e-02 (7.4082e-02)
Epoch: [40][ 50/492]	Loss 9.3858e-02 (7.6165e-02)
Epoch: [40][ 75/492]	Loss 4.1659e-02 (7.5029e-02)
Epoch: [40][100/492]	Loss 4.1412e-02 (7.8854e-02)
Epoch: [40][125/492]	Loss 6.7264e-02 (7.9744e-02)
Epoch: [40][150/492]	Loss 5.3711e-02 (8.0295e-02)
Epoch: [40][175/492]	Loss 4.8909e-02 (7.8532e-02)
Epoch: [40][200/492]	Loss 1.1426e-01 (8.0280e-02)
Epoch: [40][225/492]	Loss 1.7674e-01 (8.2845e-02)
Epoch: [40][250/492]	Loss 2.7956e-01 (8.4159e-02)
Epoch: [40][275/492]	Loss 7.0093e-02 (8.3712e-02)
Epoch: [40][300/492]	Loss 1.4893e-01 (8.4684e-02)
Epoch: [40][325/492]	Loss 4.5258e-02 (8.5689e-02)
Epoch: [40][350/492]	Loss 4.3437e-02 (8.4409e-02)
Epoch: [40][375/492]	Loss 6.0464e-02 (8.3979e-02)
Epoch: [40][400/492]	Loss 6.1640e-02 (8.3575e-02)
Epoch: [40][425/492]	Loss 5.8127e-02 (8.3111e-02)
Epoch: [40][450/492]	Loss 4.6026e-02 (8.3227e-02)
Epoch: [40][475/492]	Loss 6.3131e-02 (8.2907e-02)
Fill memory bank for kNN...
Fill Memory Bank [0/493]
Fill Memory Bank [100/493]
Fill Memory Bank [200/493]
Fill Memory Bank [300/493]
Fill Memory Bank [400/493]
Evaluate ...
Result of kNN evaluation is 96.29
Checkpoint ...
[33mEpoch 41/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00008
Train ...
Epoch: [41][  0/492]	Loss 1.3348e-01 (1.3348e-01)
Epoch: [41][ 25/492]	Loss 1.6387e-01 (8.6452e-02)
Epoch: [41][ 50/492]	Loss 3.6584e-01 (9.3126e-02)
Epoch: [41][ 75/492]	Loss 7.0312e-02 (9.1395e-02)
Epoch: [41][100/492]	Loss 6.0160e-02 (9.0842e-02)
Epoch: [41][125/492]	Loss 1.4793e-01 (8.9341e-02)
Epoch: [41][150/492]	Loss 1.8250e-01 (8.8823e-02)
Epoch: [41][175/492]	Loss 6.4364e-02 (9.0662e-02)
Epoch: [41][200/492]	Loss 4.4645e-02 (8.9610e-02)
Epoch: [41][225/492]	Loss 6.5127e-02 (8.8812e-02)
Epoch: [41][250/492]	Loss 8.4986e-02 (8.8824e-02)
Epoch: [41][275/492]	Loss 6.8898e-02 (8.8155e-02)
Epoch: [41][300/492]	Loss 2.4476e-01 (8.8309e-02)
Epoch: [41][325/492]	Loss 3.2778e-02 (8.6676e-02)
Epoch: [41][350/492]	Loss 8.2992e-02 (8.6330e-02)
Epoch: [41][375/492]	Loss 3.3924e-02 (8.5469e-02)
Epoch: [41][400/492]	Loss 1.6342e-01 (8.4121e-02)
Epoch: [41][425/492]	Loss 3.5995e-02 (8.3122e-02)
Epoch: [41][450/492]	Loss 4.8950e-02 (8.2967e-02)
Epoch: [41][475/492]	Loss 8.5281e-02 (8.3971e-02)
Checkpoint ...
[33mEpoch 42/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00006
Train ...
Epoch: [42][  0/492]	Loss 4.9590e-02 (4.9590e-02)
Epoch: [42][ 25/492]	Loss 3.7262e-02 (7.6583e-02)
Epoch: [42][ 50/492]	Loss 3.1007e-02 (7.5981e-02)
Epoch: [42][ 75/492]	Loss 9.9513e-02 (7.5090e-02)
Epoch: [42][100/492]	Loss 3.3988e-02 (7.2201e-02)
Epoch: [42][125/492]	Loss 2.5771e-01 (7.4327e-02)
Epoch: [42][150/492]	Loss 2.2410e-01 (7.5747e-02)
Epoch: [42][175/492]	Loss 1.0480e-01 (7.9070e-02)
Epoch: [42][200/492]	Loss 1.2156e-01 (8.3104e-02)
Epoch: [42][225/492]	Loss 6.8821e-02 (8.2118e-02)
Epoch: [42][250/492]	Loss 1.2280e-01 (8.3122e-02)
Epoch: [42][275/492]	Loss 6.8047e-02 (8.3857e-02)
Epoch: [42][300/492]	Loss 3.5939e-02 (8.5217e-02)
Epoch: [42][325/492]	Loss 1.7159e-01 (8.5286e-02)
Epoch: [42][350/492]	Loss 4.7760e-02 (8.5077e-02)
Epoch: [42][375/492]	Loss 3.4134e-02 (8.3200e-02)
Epoch: [42][400/492]	Loss 4.2135e-02 (8.2721e-02)
Epoch: [42][425/492]	Loss 2.7190e-02 (8.3386e-02)
Epoch: [42][450/492]	Loss 1.2372e-01 (8.4082e-02)
Epoch: [42][475/492]	Loss 7.8141e-02 (8.4412e-02)
Checkpoint ...
[33mEpoch 43/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00005
Train ...
Epoch: [43][  0/492]	Loss 4.4153e-02 (4.4153e-02)
Epoch: [43][ 25/492]	Loss 3.5834e-02 (7.8222e-02)
Epoch: [43][ 50/492]	Loss 1.2645e-01 (7.7991e-02)
Epoch: [43][ 75/492]	Loss 2.2578e-02 (7.5680e-02)
Epoch: [43][100/492]	Loss 3.3604e-02 (7.7398e-02)
Epoch: [43][125/492]	Loss 5.2077e-02 (7.7654e-02)
Epoch: [43][150/492]	Loss 1.1131e-01 (7.8471e-02)
Epoch: [43][175/492]	Loss 4.9740e-02 (8.3429e-02)
Epoch: [43][200/492]	Loss 7.3630e-02 (8.5840e-02)
Epoch: [43][225/492]	Loss 8.8078e-02 (8.8027e-02)
Epoch: [43][250/492]	Loss 5.2533e-02 (8.8785e-02)
Epoch: [43][275/492]	Loss 9.8420e-02 (8.7874e-02)
Epoch: [43][300/492]	Loss 6.5968e-02 (8.6600e-02)
Epoch: [43][325/492]	Loss 5.0905e-02 (8.6012e-02)
Epoch: [43][350/492]	Loss 7.3654e-02 (8.6102e-02)
Epoch: [43][375/492]	Loss 2.1163e-01 (8.8233e-02)
Epoch: [43][400/492]	Loss 9.4644e-02 (8.9402e-02)
Epoch: [43][425/492]	Loss 4.1457e-02 (8.7337e-02)
Epoch: [43][450/492]	Loss 3.0248e-02 (8.7482e-02)
Epoch: [43][475/492]	Loss 5.0409e-02 (8.6630e-02)
Checkpoint ...
[33mEpoch 44/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00004
Train ...
Epoch: [44][  0/492]	Loss 4.2490e-02 (4.2490e-02)
Epoch: [44][ 25/492]	Loss 3.2836e-02 (9.7334e-02)
Epoch: [44][ 50/492]	Loss 4.4869e-02 (8.7035e-02)
Epoch: [44][ 75/492]	Loss 1.2118e-01 (8.8046e-02)
Epoch: [44][100/492]	Loss 1.0062e-01 (8.5856e-02)
Epoch: [44][125/492]	Loss 7.1430e-02 (8.7825e-02)
Epoch: [44][150/492]	Loss 5.2139e-02 (8.3820e-02)
Epoch: [44][175/492]	Loss 3.2676e-02 (8.4619e-02)
Epoch: [44][200/492]	Loss 6.2067e-02 (8.7334e-02)
Epoch: [44][225/492]	Loss 9.0702e-02 (8.7172e-02)
Epoch: [44][250/492]	Loss 2.9994e-02 (8.7155e-02)
Epoch: [44][275/492]	Loss 7.2794e-02 (8.6933e-02)
Epoch: [44][300/492]	Loss 9.3777e-02 (8.6371e-02)
Epoch: [44][325/492]	Loss 8.3277e-02 (8.5367e-02)
Epoch: [44][350/492]	Loss 3.6666e-02 (8.3821e-02)
Epoch: [44][375/492]	Loss 6.3776e-02 (8.4340e-02)
Epoch: [44][400/492]	Loss 6.5844e-02 (8.4528e-02)
Epoch: [44][425/492]	Loss 1.0234e-01 (8.5346e-02)
Epoch: [44][450/492]	Loss 5.8107e-02 (8.5108e-02)
Epoch: [44][475/492]	Loss 7.1808e-02 (8.3711e-02)
Checkpoint ...
[33mEpoch 45/50[0m
[33m---------------[0m
Adjusted learning rate to 0.00003
Train ...
slurmstepd: error: *** STEP 734787.0 ON vulcan03 CANCELLED AT 2021-03-04T14:04:09 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 734787 ON vulcan03 CANCELLED AT 2021-03-04T14:04:09 DUE TO TIME LIMIT ***
